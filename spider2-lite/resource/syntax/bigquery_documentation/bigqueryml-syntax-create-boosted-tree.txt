[ ![Google Cloud](https://www.gstatic.com/devrel-
devsite/prod/vc851b65627ca98cc752c9ae13e5f506cd6dbb7ed1bb4c8df6090c5f9130ed83c/cloud/images/cloud-
logo.svg) ](/)

*

[ Documentation ](https://cloud.google.com/docs) [ Technology areas
](https://cloud.google.com/docs/tech-area-overviews)

close

* [ AI solutions, generative AI, and ML  ](https://cloud.google.com/docs/ai-ml)
* [ Application development  ](https://cloud.google.com/docs/application-development)
* [ Application hosting  ](https://cloud.google.com/docs/application-hosting)
* [ Compute  ](https://cloud.google.com/docs/compute-area)
* [ Data analytics and pipelines  ](https://cloud.google.com/docs/data)
* [ Databases  ](https://cloud.google.com/docs/databases)
* [ Distributed, hybrid, and multicloud  ](https://cloud.google.com/docs/dhm-cloud)
* [ Industry solutions  ](https://cloud.google.com/docs/industry)
* [ Networking  ](https://cloud.google.com/docs/networking)
* [ Observability and monitoring  ](https://cloud.google.com/docs/observability)
* [ Security  ](https://cloud.google.com/docs/security)
* [ Storage  ](https://cloud.google.com/docs/storage)

[ Cross-product tools ](https://cloud.google.com/docs/cross-product-overviews)

close

* [ Access and resources management  ](https://cloud.google.com/docs/access-resources)
* [ Cloud SDK, languages, frameworks, and tools  ](https://cloud.google.com/docs/devtools)
* [ Costs and usage management  ](https://cloud.google.com/docs/costs-usage)
* [ Infrastructure as code  ](https://cloud.google.com/docs/iac)
* [ Migration  ](https://cloud.google.com/docs/migration)

[ Related sites ](https://cloud.google.com/)

close

* [ Google Cloud Home  ](https://cloud.google.com/)
* [ Free Trial and Free Tier  ](https://cloud.google.com/free)
* [ Architecture Center  ](https://cloud.google.com/architecture)
* [ Blog  ](https://cloud.google.com/blog)
* [ Contact Sales  ](https://cloud.google.com/contact)
* [ Google Cloud Developer Center  ](https://cloud.google.com/developers)
* [ Google Developer Center  ](https://developers.google.com/)
* [ Google Cloud Marketplace (in console)  ](https://console.cloud.google.com/marketplace)
* [ Google Cloud Marketplace Documentation  ](https://cloud.google.com/marketplace/docs)
* [ Google Cloud Skills Boost  ](https://www.cloudskillsboost.google/paths)
* [ Google Cloud Solution Center  ](https://cloud.google.com/solutions)
* [ Google Cloud Support  ](https://cloud.google.com/support-hub)
* [ Google Cloud Tech Youtube Channel  ](https://www.youtube.com/@googlecloudtech)

* English
* Deutsch
* Español – América Latina
* Français
* Português – Brasil
* 中文 – 简体
* 日本語
* 한국어

Sign in

* [ BigQuery ](https://cloud.google.com/bigquery)

[ Guides ](https://cloud.google.com/bigquery/docs/introduction) [ Reference
](https://cloud.google.com/bigquery/quotas) [ Samples
](https://cloud.google.com/bigquery/docs/samples) [ Resources
](https://cloud.google.com/bigquery/docs/release-notes)

[ Contact Us ](https://cloud.google.com/contact) [ Start free
](//console.cloud.google.com/freetrial)

[ ![Google Cloud](https://www.gstatic.com/devrel-
devsite/prod/vc851b65627ca98cc752c9ae13e5f506cd6dbb7ed1bb4c8df6090c5f9130ed83c/cloud/images/cloud-
logo.svg) ](/)

*

* [ Documentation  ](/docs)
* [ Guides  ](/bigquery/docs/introduction)
* [ Reference  ](/bigquery/quotas)
* [ Samples  ](/bigquery/docs/samples)
* [ Resources  ](/bigquery/docs/release-notes)
* [ Technology areas  ](/docs/tech-area-overviews)
* More
* [ Cross-product tools  ](/docs/cross-product-overviews)
* More
* [ Related sites  ](/)
* More
* [ Console  ](//console.cloud.google.com/)
* [ Contact Us  ](/contact)
* [ Start free  ](//console.cloud.google.com/freetrial)

* Quotas and limits

* [ Quotas and limits reference  ](/bigquery/quotas)
* [ Troubleshoot quota errors  ](/bigquery/docs/troubleshoot-quotas)
* BigQuery command-line tool

* [ bq command-line tool reference  ](/bigquery/docs/reference/bq-cli-reference)
* SQL in BigQuery

* GoogleSQL reference

* [ Query syntax  ](/bigquery/docs/reference/standard-sql/query-syntax)
* General reference

* [ Data types  ](/bigquery/docs/reference/standard-sql/data-types)
* [ Lexical structure and syntax  ](/bigquery/docs/reference/standard-sql/lexical)
* [ Conversion rules  ](/bigquery/docs/reference/standard-sql/conversion_rules)
* [ Format elements  ](/bigquery/docs/reference/standard-sql/format-elements)
* [ Collation  ](/bigquery/docs/reference/standard-sql/collation-concepts)
* [ Text analysis  ](/bigquery/docs/reference/standard-sql/text-analysis)
* [ BI Engine optimized functions  ](/bigquery/docs/bi-engine-optimized-sql)

* Expressions

* [ Function calls  ](/bigquery/docs/reference/standard-sql/functions-reference)
* [ Aggregate function calls  ](/bigquery/docs/reference/standard-sql/aggregate-function-calls)
* [ Window function calls  ](/bigquery/docs/reference/standard-sql/window-function-calls)
* [ Operators  ](/bigquery/docs/reference/standard-sql/operators)
* [ Conditional expressions  ](/bigquery/docs/reference/standard-sql/conditional_expressions)
* [ Subqueries  ](/bigquery/docs/reference/standard-sql/subqueries)

* Functions

* [ All functions and operators  ](/bigquery/docs/reference/standard-sql/functions-and-operators)
* [ AEAD encryption functions  ](/bigquery/docs/reference/standard-sql/aead_encryption_functions)
* [ Aggregate functions  ](/bigquery/docs/reference/standard-sql/aggregate_functions)
* [ Approximate aggregate functions  ](/bigquery/docs/reference/standard-sql/approximate_aggregate_functions)
* [ Array functions  ](/bigquery/docs/reference/standard-sql/array_functions)
* [ Bit functions  ](/bigquery/docs/reference/standard-sql/bit_functions)
* [ Conversion functions  ](/bigquery/docs/reference/standard-sql/conversion_functions)
* [ Date functions  ](/bigquery/docs/reference/standard-sql/date_functions)
* [ Datetime functions  ](/bigquery/docs/reference/standard-sql/datetime_functions)
* [ Debugging functions  ](/bigquery/docs/reference/standard-sql/debugging_functions)
* [ Differentially private aggregate functions  ](/bigquery/docs/reference/standard-sql/aggregate-dp-functions)
* [ Federated query functions  ](/bigquery/docs/reference/standard-sql/federated_query_functions)
* [ DLP encryption functions  ](/bigquery/docs/reference/standard-sql/dlp_functions)
* [ Geography functions  ](/bigquery/docs/reference/standard-sql/geography_functions)
* [ Hash functions  ](/bigquery/docs/reference/standard-sql/hash_functions)
* [ HyperLogLog++ functions  ](/bigquery/docs/reference/standard-sql/hll_functions)
* [ Interval functions  ](/bigquery/docs/reference/standard-sql/interval_functions)
* [ JSON functions  ](/bigquery/docs/reference/standard-sql/json_functions)
* [ Mathematical functions  ](/bigquery/docs/reference/standard-sql/mathematical_functions)
* [ Navigation functions  ](/bigquery/docs/reference/standard-sql/navigation_functions)
* [ Net functions  ](/bigquery/docs/reference/standard-sql/net_functions)
* [ Numbering functions  ](/bigquery/docs/reference/standard-sql/numbering_functions)
* [ Range functions  ](/bigquery/docs/reference/standard-sql/range-functions)
* [ Search functions  ](/bigquery/docs/reference/standard-sql/search_functions)
* [ Security functions  ](/bigquery/docs/reference/standard-sql/security_functions)
* [ Statistical aggregate functions  ](/bigquery/docs/reference/standard-sql/statistical_aggregate_functions)
* [ String functions  ](/bigquery/docs/reference/standard-sql/string_functions)
* [ Table functions (built-in)  ](/bigquery/docs/reference/standard-sql/table-functions-built-in)
* [ Text analysis functions  ](/bigquery/docs/reference/standard-sql/text-analysis-functions)
* [ Time functions  ](/bigquery/docs/reference/standard-sql/time_functions)
* [ Time series functions  ](/bigquery/docs/reference/standard-sql/time-series-functions)
* [ Timestamp functions  ](/bigquery/docs/reference/standard-sql/timestamp_functions)
* [ Utility functions  ](/bigquery/docs/reference/standard-sql/utility-functions)

* Statements

* [ Data definition language (DDL)  ](/bigquery/docs/reference/standard-sql/data-definition-language)
* [ Data manipulation language (DML)  ](/bigquery/docs/reference/standard-sql/dml-syntax)
* [ Data control language (DCL)  ](/bigquery/docs/reference/standard-sql/data-control-language)
* [ Procedural language  ](/bigquery/docs/reference/standard-sql/procedural-language)
* [ Export and load statements  ](/bigquery/docs/reference/standard-sql/other-statements)
* [ Debugging statements  ](/bigquery/docs/reference/standard-sql/debugging-statements)

* BigQuery ML SQL reference

* Creating and training models

* [ CREATE MODEL statement overview  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create)
* Regression and classification

* [ Linear and logistic regression  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-glm)
* [ Boosted trees  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-boosted-tree)
* [ Random forest  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-random-forest)
* [ Deep neural networks  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-dnn-models)
* [ Wide & Deep networks  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-wnd-models)
* [ AutoML models  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-automl)

* Clustering

* [ K-means  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-kmeans)

* Dimensionality reduction

* [ Principal component analysis  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-pca)
* [ Autoencoder  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-autoencoder)

* Collaborative filtering

* [ Matrix factorization  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-matrix-factorization)

* Time series forecasting

* [ Univariate forecasting with ARIMA_PLUS models  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-time-series)
* [ Multivariate forecasting with ARIMA_PLUS_XREG models  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-multivariate-time-series)

* Importing models

* [ Open Neural Network Exchange (ONNX)  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-onnx)
* [ TensorFlow  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-tensorflow)
* [ TensorFlow Lite  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-tflite)
* [ XGBoost  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-xgboost)

* Remote models

* [ LLMs  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-remote-model)
* [ Cloud AI services  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-remote-model-service)
* [ Vertex AI hosted models  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-remote-model-https)

* Feature engineering

* [ Feature transformation  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-transform)
* [ ML.TRANSFORM  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-transform)
* [ ML.FEATURE_INFO  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-feature)
* General functions

* [ ML.IMPUTER  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-imputer)

* Numerical functions

* [ ML.BUCKETIZE  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-bucketize)
* [ ML.MAX_ABS_SCALER  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-max-abs-scaler)
* [ ML.MIN_MAX_SCALER  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-min-max-scaler)
* [ ML.NORMALIZER  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-normalizer)
* [ ML.POLYNOMIAL_EXPAND  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-polynomial-expand)
* [ ML.QUANTILE_BUCKETIZE  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-quantile-bucketize)
* [ ML.ROBUST_SCALER  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-robust-scaler)
* [ ML.STANDARD_SCALER  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-standard-scaler)

* Categorical functions

* [ ML.FEATURE_CROSS  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-feature-cross)
* [ ML.HASH_BUCKETIZE  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-hash-bucketize)
* [ ML.LABEL_ENCODER  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-label-encoder)
* [ ML.MULTI_HOT_ENCODER  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-multi-hot-encoder)
* [ ML.ONE_HOT_ENCODER  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-one-hot-encoder)

* Text functions

* [ ML.NGRAMS  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-ngrams)
* [ ML.BAG_OF_WORDS  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-bag-of-words)
* [ ML.TF_IDF  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-tf-idf)

* Image functions

* [ ML.CONVERT_COLOR_SPACE  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-convert-color-space)
* [ ML.CONVERT_IMAGE_TYPE  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-convert-image-type)
* [ ML.DECODE_IMAGE  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-decode-image)
* [ ML.RESIZE_IMAGE  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-resize-image)

* Point-in-time lookup functions

* [ ML.FEATURES_AT_TIME  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-feature-time)
* [ ML.ENTITY_FEATURES_AT_TIME  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-entity-feature-time)

* Hyperparameter tuning functions

* [ ML.TRIAL_INFO  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-trial-info)

* Evaluation functions

* [ ML.EVALUATE  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-evaluate)
* [ ML.ROC_CURVE  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-roc)
* [ ML.CONFUSION_MATRIX  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-confusion)
* [ ML.ARIMA_EVALUATE  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-arima-evaluate)
* [ ML.TRAINING_INFO  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-train)
* [ ML.RECONSTRUCTION_LOSS  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-reconstruction-loss)
* [ ML.HOLIDAY_INFO  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-holiday-info)

* Inference functions

* [ ML.PREDICT  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-predict)
* [ ML.FORECAST  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-forecast)
* [ ML.RECOMMEND  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-recommend)
* [ ML.DETECT_ANOMALIES  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-detect-anomalies)

* Generative AI functions

* [ ML.GENERATE_TEXT  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-generate-text)
* [ ML.GENERATE_EMBEDDING  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-generate-embedding)

* AI functions

* [ ML.UNDERSTAND_TEXT  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-understand-text)
* [ ML.TRANSLATE  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-translate)
* [ ML.PROCESS_DOCUMENT  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-process-document)
* [ ML.TRANSCRIBE  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-transcribe)
* [ ML.ANNOTATE_IMAGE  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-annotate-image)

* AI Explanation functions

* [ ML.EXPLAIN_PREDICT  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-explain-predict)
* [ ML.EXPLAIN_FORECAST  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-explain-forecast)
* [ ML.GLOBAL_EXPLAIN  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-global-explain)
* [ ML.FEATURE_IMPORTANCE  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-importance)
* [ ML.ADVANCED_WEIGHTS  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-advanced-weights)

* Model weights functions

* [ ML.WEIGHTS  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-weights)
* [ ML.CENTROIDS  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-centroids)
* [ ML.PRINCIPAL_COMPONENTS  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-principal-components)
* [ ML.PRINCIPAL_COMPONENT_INFO  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-principal-component-info)
* [ ML.ARIMA_COEFFICIENTS  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-arima-coefficients)

* Model monitoring functions

* [ ML.DESCRIBE_DATA  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-describe-data)
* [ ML.VALIDATE_DATA_DRIFT  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-validate-data-drift)
* [ ML.VALIDATE_DATA_SKEW  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-validate-data-skew)
* [ ML.TFDV_DESCRIBE  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-tfdv-describe)
* [ ML.TFDV_VALIDATE  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-tfdv-validate)

* Math utility functions

* [ ML.DISTANCE  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-distance)
* [ ML.LP_NORM  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-lp-norm)

* Model management statements

* [ EXPORT MODEL statement  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-export-model)
* [ ALTER MODEL statement  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-alter-model)
* [ DROP MODEL statement  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-drop-model)

* INFORMATION SCHEMA views

* [ Introduction  ](/bigquery/docs/information-schema-intro)
* Access control

* [ OBJECT_PRIVILEGES view  ](/bigquery/docs/information-schema-object-privileges)

* BI Engine

* [ BI_CAPACITIES  ](/bigquery/docs/information-schema-bi-capacities)
* [ BI_CAPACITY_CHANGES  ](/bigquery/docs/information-schema-bi-capacity-changes)

* Configurations

* [ EFFECTIVE_PROJECT_OPTIONS view  ](/bigquery/docs/information-schema-effective-project-options)
* [ ORGANIZATION_OPTIONS view  ](/bigquery/docs/information-schema-organization-options)
* [ ORGANIZATION_OPTIONS_CHANGES view  ](/bigquery/docs/information-schema-organization-options-changes)
* [ PROJECT_OPTIONS view  ](/bigquery/docs/information-schema-project-options)
* [ PROJECT_OPTIONS_CHANGES view  ](/bigquery/docs/information-schema-project-options-changes)

* Datasets

* [ SCHEMATA view  ](/bigquery/docs/information-schema-datasets-schemata)
* [ SCHEMATA_LINKS view  ](/bigquery/docs/information-schema-datasets-schemata-links)
* [ SCHEMATA_OPTIONS view  ](/bigquery/docs/information-schema-datasets-schemata-options)
* [ SHARED_DATASET_USAGE view  ](/bigquery/docs/information-schema-shared-dataset-usage)
* [ SCHEMATA_REPLICAS view  ](/bigquery/docs/information-schema-schemata-replicas)

* Jobs

* [ JOBS view  ](/bigquery/docs/information-schema-jobs)
* [ JOBS_BY_USER view  ](/bigquery/docs/information-schema-jobs-by-user)
* [ JOBS_BY_FOLDER view  ](/bigquery/docs/information-schema-jobs-by-folder)
* [ JOBS_BY_ORGANIZATION view  ](/bigquery/docs/information-schema-jobs-by-organization)

* Jobs by timeslice

* [ JOBS_TIMELINE view  ](/bigquery/docs/information-schema-jobs-timeline)
* [ JOBS_TIMELINE_BY_USER view  ](/bigquery/docs/information-schema-jobs-timeline-by-user)
* [ JOBS_TIMELINE_BY_FOLDER view  ](/bigquery/docs/information-schema-jobs-timeline-by-folder)
* [ JOBS_TIMELINE_BY_ORGANIZATION view  ](/bigquery/docs/information-schema-jobs-timeline-by-organization)

* Reservations

* [ ASSIGNMENTS view  ](/bigquery/docs/information-schema-assignments)
* [ ASSIGNMENT_CHANGES view  ](/bigquery/docs/information-schema-assignments-changes)
* [ CAPACITY_COMMITMENTS view  ](/bigquery/docs/information-schema-capacity-commitments)
* [ CAPACITY_COMMITMENT_CHANGES view  ](/bigquery/docs/information-schema-capacity-commitment-changes)
* [ RESERVATIONS view  ](/bigquery/docs/information-schema-reservations)
* [ RESERVATION_CHANGES view  ](/bigquery/docs/information-schema-reservation-changes)
* [ RESERVATIONS_TIMELINE view  ](/bigquery/docs/information-schema-reservation-timeline)

* Routines

* [ PARAMETERS view  ](/bigquery/docs/information-schema-parameters)
* [ ROUTINES view  ](/bigquery/docs/information-schema-routines)
* [ ROUTINE_OPTIONS view  ](/bigquery/docs/information-schema-routine-options)

* Search indexes

* [ SEARCH_INDEXES view  ](/bigquery/docs/information-schema-indexes)
* [ SEARCH_INDEX_COLUMNS view  ](/bigquery/docs/information-schema-index-columns)

* Sessions

* [ SESSIONS_BY_PROJECT view  ](/bigquery/docs/information-schema-sessions-by-project)
* [ SESSIONS_BY_USER view  ](/bigquery/docs/information-schema-sessions-by-user)

* Streaming

* [ STREAMING_TIMELINE view  ](/bigquery/docs/information-schema-streaming)
* [ STREAMING_TIMELINE_BY_FOLDER view  ](/bigquery/docs/information-schema-streaming-by-folder)
* [ STREAMING_TIMELINE_BY_ORGANIZATION view  ](/bigquery/docs/information-schema-streaming-by-organization)

* Tables

* [ COLUMNS view  ](/bigquery/docs/information-schema-columns)
* [ COLUMN_FIELD_PATHS view  ](/bigquery/docs/information-schema-column-field-paths)
* [ CONSTRAINT_COLUMN_USAGE view  ](/bigquery/docs/information-schema-constraint-column-usage)
* [ KEY_COLUMN_USAGE view  ](/bigquery/docs/information-schema-key-column-usage)
* [ PARTITIONS view  ](/bigquery/docs/information-schema-partitions)
* [ TABLES view  ](/bigquery/docs/information-schema-tables)
* [ TABLE_OPTIONS view  ](/bigquery/docs/information-schema-table-options)
* [ TABLE_CONSTRAINTS view  ](/bigquery/docs/information-schema-table-constraints)
* [ TABLE_SNAPSHOTS view  ](/bigquery/docs/information-schema-snapshots)
* [ TABLE_STORAGE view  ](/bigquery/docs/information-schema-table-storage)
* [ TABLE_STORAGE_BY_ORGANIZATION view  ](/bigquery/docs/information-schema-table-storage-by-organization)
* [ TABLE_STORAGE_USAGE_TIMELINE view  ](/bigquery/docs/information-schema-table-storage-usage)
* [ TABLE_STORAGE_USAGE_TIMELINE_BY_ORGANIZATION view  ](/bigquery/docs/information-schema-table-storage-usage-by-organization)

* Vector indexes

* [ VECTOR_INDEXES view  ](/bigquery/docs/information-schema-vector-indexes)
* [ VECTOR_INDEX_COLUMNS view  ](/bigquery/docs/information-schema-vector-index-columns)
* [ VECTOR_INDEX_OPTIONS view  ](/bigquery/docs/information-schema-vector-index-options)

* Views

* [ VIEWS view  ](/bigquery/docs/information-schema-views)
* [ MATERIALIZED_VIEWS view  ](/bigquery/docs/information-schema-materialized-views)

* Write API

* [ WRITE_API_TIMELINE view  ](/bigquery/docs/information-schema-write-api)
* [ WRITE_API_TIMELINE_BY_FOLDER view  ](/bigquery/docs/information-schema-write-api-by-folder)
* [ WRITE_API_TIMELINE_BY_ORGANIZATION view  ](/bigquery/docs/information-schema-write-api-by-organization)

* Legacy SQL reference

* [ Migrating to GoogleSQL  ](/bigquery/docs/reference/standard-sql/migrating-from-legacy-sql)
* [ Functions and operators  ](/bigquery/docs/reference/legacy-sql)
* [ Data types  ](/bigquery/docs/data-types)
* [ Querying nested and repeated fields  ](/bigquery/docs/legacy-nested-repeated)
* [ User-defined functions  ](/bigquery/docs/user-defined-functions-legacy)
* [ Table decorators  ](/bigquery/docs/table-decorators)

* BigQuery DataFrames Python API

* [ BigQuery DataFrames  ](/bigquery/docs/reference/bigquery-dataframes)
* BigQuery APIs

* BigQuery API reference

* [ BigQuery APIs and libraries overview  ](/bigquery/docs/reference/libraries-overview)
* BigQuery API reference

* [ BigQuery client libraries  ](/bigquery/docs/reference/libraries)
* [ BigQuery REST API  ](/bigquery/docs/reference/rest)
* REST reference (v2)

* REST Resources

* datasets

* [ Overview  ](/bigquery/docs/reference/rest/v2/datasets)
* [ delete  ](/bigquery/docs/reference/rest/v2/datasets/delete)
* [ get  ](/bigquery/docs/reference/rest/v2/datasets/get)
* [ insert  ](/bigquery/docs/reference/rest/v2/datasets/insert)
* [ list  ](/bigquery/docs/reference/rest/v2/datasets/list)
* [ patch  ](/bigquery/docs/reference/rest/v2/datasets/patch)
* [ undelete  ](/bigquery/docs/reference/rest/v2/datasets/undelete)
* [ update  ](/bigquery/docs/reference/rest/v2/datasets/update)

* jobs

* [ Overview  ](/bigquery/docs/reference/rest/v2/jobs)
* [ cancel  ](/bigquery/docs/reference/rest/v2/jobs/cancel)
* [ delete  ](/bigquery/docs/reference/rest/v2/jobs/delete)
* [ get  ](/bigquery/docs/reference/rest/v2/jobs/get)
* [ getQueryResults  ](/bigquery/docs/reference/rest/v2/jobs/getQueryResults)
* [ insert  ](/bigquery/docs/reference/rest/v2/jobs/insert)
* [ list  ](/bigquery/docs/reference/rest/v2/jobs/list)
* [ query  ](/bigquery/docs/reference/rest/v2/jobs/query)

* models

* [ Overview  ](/bigquery/docs/reference/rest/v2/models)
* [ delete  ](/bigquery/docs/reference/rest/v2/models/delete)
* [ get  ](/bigquery/docs/reference/rest/v2/models/get)
* [ list  ](/bigquery/docs/reference/rest/v2/models/list)
* [ patch  ](/bigquery/docs/reference/rest/v2/models/patch)

* projects

* [ Overview  ](/bigquery/docs/reference/rest/v2/projects)
* [ getServiceAccount  ](/bigquery/docs/reference/rest/v2/projects/getServiceAccount)
* [ list  ](/bigquery/docs/reference/rest/v2/projects/list)

* routines

* [ Overview  ](/bigquery/docs/reference/rest/v2/routines)
* [ delete  ](/bigquery/docs/reference/rest/v2/routines/delete)
* [ get  ](/bigquery/docs/reference/rest/v2/routines/get)
* [ insert  ](/bigquery/docs/reference/rest/v2/routines/insert)
* [ list  ](/bigquery/docs/reference/rest/v2/routines/list)
* [ update  ](/bigquery/docs/reference/rest/v2/routines/update)

* rowAccessPolicies

* [ Overview  ](/bigquery/docs/reference/rest/v2/rowAccessPolicies)
* [ getIamPolicy  ](/bigquery/docs/reference/rest/v2/rowAccessPolicies/getIamPolicy)
* [ list  ](/bigquery/docs/reference/rest/v2/rowAccessPolicies/list)
* [ testIamPermissions  ](/bigquery/docs/reference/rest/v2/rowAccessPolicies/testIamPermissions)

* tabledata

* [ Overview  ](/bigquery/docs/reference/rest/v2/tabledata)
* [ insertAll  ](/bigquery/docs/reference/rest/v2/tabledata/insertAll)
* [ list  ](/bigquery/docs/reference/rest/v2/tabledata/list)

* tables

* [ Overview  ](/bigquery/docs/reference/rest/v2/tables)
* [ delete  ](/bigquery/docs/reference/rest/v2/tables/delete)
* [ get  ](/bigquery/docs/reference/rest/v2/tables/get)
* [ getIamPolicy  ](/bigquery/docs/reference/rest/v2/tables/getIamPolicy)
* [ insert  ](/bigquery/docs/reference/rest/v2/tables/insert)
* [ list  ](/bigquery/docs/reference/rest/v2/tables/list)
* [ patch  ](/bigquery/docs/reference/rest/v2/tables/patch)
* [ setIamPolicy  ](/bigquery/docs/reference/rest/v2/tables/setIamPolicy)
* [ testIamPermissions  ](/bigquery/docs/reference/rest/v2/tables/testIamPermissions)
* [ update  ](/bigquery/docs/reference/rest/v2/tables/update)

* Types

* [ ConnectionProperty  ](/bigquery/docs/reference/rest/v2/ConnectionProperty)
* [ DataFormatOptions  ](/bigquery/docs/reference/rest/v2/DataFormatOptions)
* [ DatasetAccessEntry  ](/bigquery/docs/reference/rest/v2/DatasetAccessEntry)
* [ DmlStats  ](/bigquery/docs/reference/rest/v2/DmlStats)
* [ EncryptionConfiguration  ](/bigquery/docs/reference/rest/v2/EncryptionConfiguration)
* [ GetPolicyOptions  ](/bigquery/docs/reference/rest/v2/GetPolicyOptions)
* [ Job  ](/bigquery/docs/reference/rest/v2/Job)
* [ JobReference  ](/bigquery/docs/reference/rest/v2/JobReference)
* [ Policy  ](/bigquery/docs/reference/rest/v2/Policy)
* [ ProjectReference  ](/bigquery/docs/reference/rest/v2/ProjectReference)
* [ QueryParameter  ](/bigquery/docs/reference/rest/v2/QueryParameter)
* [ RoundingMode  ](/bigquery/docs/reference/rest/v2/RoundingMode)
* [ RowAccessPolicyReference  ](/bigquery/docs/reference/rest/v2/RowAccessPolicyReference)
* [ SessionInfo  ](/bigquery/docs/reference/rest/v2/SessionInfo)
* [ StandardSqlDataType  ](/bigquery/docs/reference/rest/v2/StandardSqlDataType)
* [ StandardSqlField  ](/bigquery/docs/reference/rest/v2/StandardSqlField)
* [ TableReference  ](/bigquery/docs/reference/rest/v2/TableReference)
* [ TargetType  ](/bigquery/docs/reference/rest/v2/TargetType)
* [ TestIamPermissionsResponse  ](/bigquery/docs/reference/rest/v2/TestIamPermissionsResponse)

* [ API uploads  ](/bigquery/docs/reference/api-uploads)

* BigQuery Data Policy API reference

* [ Data Policy REST reference  ](/bigquery/docs/reference/bigquerydatapolicy/rest)
* v1

* REST Resources

* projects.locations.dataPolicies

* [ Overview  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1/projects.locations.dataPolicies)
* [ create  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1/projects.locations.dataPolicies/create)
* [ delete  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1/projects.locations.dataPolicies/delete)
* [ get  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1/projects.locations.dataPolicies/get)
* [ getIamPolicy  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1/projects.locations.dataPolicies/getIamPolicy)
* [ list  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1/projects.locations.dataPolicies/list)
* [ patch  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1/projects.locations.dataPolicies/patch)
* [ rename  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1/projects.locations.dataPolicies/rename)
* [ setIamPolicy  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1/projects.locations.dataPolicies/setIamPolicy)
* [ testIamPermissions  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1/projects.locations.dataPolicies/testIamPermissions)

* v1beta1

* REST Resources

* projects.locations.dataPolicies

* [ Overview  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1beta1/projects.locations.dataPolicies)
* [ create  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1beta1/projects.locations.dataPolicies/create)
* [ delete  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1beta1/projects.locations.dataPolicies/delete)
* [ get  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1beta1/projects.locations.dataPolicies/get)
* [ getIamPolicy  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1beta1/projects.locations.dataPolicies/getIamPolicy)
* [ list  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1beta1/projects.locations.dataPolicies/list)
* [ patch  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1beta1/projects.locations.dataPolicies/patch)
* [ setIamPolicy  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1beta1/projects.locations.dataPolicies/setIamPolicy)
* [ testIamPermissions  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1beta1/projects.locations.dataPolicies/testIamPermissions)

* BigQuery Connections API reference

* [ BigQuery Connection client libraries  ](/bigquery/docs/reference/bigqueryconnection)
* [ BigQuery Connection REST API  ](/bigquery/docs/reference/bigqueryconnection/rest)
* RPC reference

* [ Overview  ](/bigquery/docs/reference/bigqueryconnection/rpc)
* [ google.cloud.bigquery.connection.v1  ](/bigquery/docs/reference/bigqueryconnection/rpc/google.cloud.bigquery.connection.v1)
* [ google.cloud.bigquery.connection.v1beta1  ](/bigquery/docs/reference/bigqueryconnection/rpc/google.cloud.bigquery.connection.v1beta1)
* [ google.iam.v1  ](/bigquery/docs/reference/bigqueryconnection/rpc/google.iam.v1)
* [ google.type  ](/bigquery/docs/reference/bigqueryconnection/rpc/google.type)

* REST reference (v1)

* REST Resources

* projects.locations.connections

* [ Overview  ](/bigquery/docs/reference/bigqueryconnection/rest/v1/projects.locations.connections)
* [ create  ](/bigquery/docs/reference/bigqueryconnection/rest/v1/projects.locations.connections/create)
* [ delete  ](/bigquery/docs/reference/bigqueryconnection/rest/v1/projects.locations.connections/delete)
* [ get  ](/bigquery/docs/reference/bigqueryconnection/rest/v1/projects.locations.connections/get)
* [ getIamPolicy  ](/bigquery/docs/reference/bigqueryconnection/rest/v1/projects.locations.connections/getIamPolicy)
* [ list  ](/bigquery/docs/reference/bigqueryconnection/rest/v1/projects.locations.connections/list)
* [ patch  ](/bigquery/docs/reference/bigqueryconnection/rest/v1/projects.locations.connections/patch)
* [ setIamPolicy  ](/bigquery/docs/reference/bigqueryconnection/rest/v1/projects.locations.connections/setIamPolicy)
* [ testIamPermissions  ](/bigquery/docs/reference/bigqueryconnection/rest/v1/projects.locations.connections/testIamPermissions)

* REST reference (v1beta1)

* REST Resources

* projects.locations.connections

* [ Overview  ](/bigquery/docs/reference/bigqueryconnection/rest/v1beta1/projects.locations.connections)
* [ create  ](/bigquery/docs/reference/bigqueryconnection/rest/v1beta1/projects.locations.connections/create)
* [ delete  ](/bigquery/docs/reference/bigqueryconnection/rest/v1beta1/projects.locations.connections/delete)
* [ get  ](/bigquery/docs/reference/bigqueryconnection/rest/v1beta1/projects.locations.connections/get)
* [ getIamPolicy  ](/bigquery/docs/reference/bigqueryconnection/rest/v1beta1/projects.locations.connections/getIamPolicy)
* [ list  ](/bigquery/docs/reference/bigqueryconnection/rest/v1beta1/projects.locations.connections/list)
* [ patch  ](/bigquery/docs/reference/bigqueryconnection/rest/v1beta1/projects.locations.connections/patch)
* [ setIamPolicy  ](/bigquery/docs/reference/bigqueryconnection/rest/v1beta1/projects.locations.connections/setIamPolicy)
* [ testIamPermissions  ](/bigquery/docs/reference/bigqueryconnection/rest/v1beta1/projects.locations.connections/testIamPermissions)
* [ updateCredential  ](/bigquery/docs/reference/bigqueryconnection/rest/v1beta1/projects.locations.connections/updateCredential)

* BigQuery Migration API reference

* [ BigQuery Migration client libraries  ](/bigquery/docs/reference/migration)
* [ BigQuery Migration REST API  ](/bigquery/docs/reference/migration/rest)
* REST reference (v2)

* REST Resources

* projects.locations.workflows

* [ Overview  ](/bigquery/docs/reference/migration/rest/v2/projects.locations.workflows)
* [ create  ](/bigquery/docs/reference/migration/rest/v2/projects.locations.workflows/create)
* [ delete  ](/bigquery/docs/reference/migration/rest/v2/projects.locations.workflows/delete)
* [ get  ](/bigquery/docs/reference/migration/rest/v2/projects.locations.workflows/get)
* [ list  ](/bigquery/docs/reference/migration/rest/v2/projects.locations.workflows/list)
* [ start  ](/bigquery/docs/reference/migration/rest/v2/projects.locations.workflows/start)

* projects.locations.workflows.subtasks

* [ Overview  ](/bigquery/docs/reference/migration/rest/v2/projects.locations.workflows.subtasks)
* [ get  ](/bigquery/docs/reference/migration/rest/v2/projects.locations.workflows.subtasks/get)
* [ list  ](/bigquery/docs/reference/migration/rest/v2/projects.locations.workflows.subtasks/list)

* Types

* [ Distribution  ](/bigquery/docs/reference/migration/rest/Shared.Types/Distribution)
* [ ErrorInfo  ](/bigquery/docs/reference/migration/rest/Shared.Types/ErrorInfo)
* [ MetricKind  ](/bigquery/docs/reference/migration/rest/Shared.Types/MetricKind)
* [ ResourceInfo  ](/bigquery/docs/reference/migration/rest/Shared.Types/ResourceInfo)
* [ ValueType  ](/bigquery/docs/reference/migration/rest/Shared.Types/ValueType)

* REST reference (v2alpha)

* REST Resources

* projects.locations.workflows

* [ Overview  ](/bigquery/docs/reference/migration/rest/v2alpha/projects.locations.workflows)
* [ create  ](/bigquery/docs/reference/migration/rest/v2alpha/projects.locations.workflows/create)
* [ delete  ](/bigquery/docs/reference/migration/rest/v2alpha/projects.locations.workflows/delete)
* [ get  ](/bigquery/docs/reference/migration/rest/v2alpha/projects.locations.workflows/get)
* [ list  ](/bigquery/docs/reference/migration/rest/v2alpha/projects.locations.workflows/list)
* [ start  ](/bigquery/docs/reference/migration/rest/v2alpha/projects.locations.workflows/start)

* projects.locations.workflows.subtasks

* [ Overview  ](/bigquery/docs/reference/migration/rest/v2alpha/projects.locations.workflows.subtasks)
* [ get  ](/bigquery/docs/reference/migration/rest/v2alpha/projects.locations.workflows.subtasks/get)
* [ list  ](/bigquery/docs/reference/migration/rest/v2alpha/projects.locations.workflows.subtasks/list)

* RPC reference

* [ Overview  ](/bigquery/docs/reference/migration/rpc)
* [ google.api  ](/bigquery/docs/reference/migration/rpc/google.api)
* [ google.cloud.bigquery.migration.tasks.assessment.v2alpha  ](/bigquery/docs/reference/migration/rpc/google.cloud.bigquery.migration.tasks.assessment.v2alpha)
* [ google.cloud.bigquery.migration.tasks.translation.v2alpha  ](/bigquery/docs/reference/migration/rpc/google.cloud.bigquery.migration.tasks.translation.v2alpha)
* [ google.cloud.bigquery.migration.v2  ](/bigquery/docs/reference/migration/rpc/google.cloud.bigquery.migration.v2)
* [ google.cloud.bigquery.migration.v2alpha  ](/bigquery/docs/reference/migration/rpc/google.cloud.bigquery.migration.v2alpha)
* [ google.rpc  ](/bigquery/docs/reference/migration/rpc/google.rpc)

* BigQuery Storage API reference

* [ Storage API client libraries  ](/bigquery/docs/reference/storage/libraries)
* RPC reference

* [ Overview  ](/bigquery/docs/reference/storage/rpc)
* [ google.cloud.bigquery.storage.v1  ](/bigquery/docs/reference/storage/rpc/google.cloud.bigquery.storage.v1)
* [ google.cloud.bigquery.storage.v1beta1  ](/bigquery/docs/reference/storage/rpc/google.cloud.bigquery.storage.v1beta1)
* [ google.cloud.bigquery.storage.v1beta2  ](/bigquery/docs/reference/storage/rpc/google.cloud.bigquery.storage.v1beta2)
* [ google.rpc  ](/bigquery/docs/reference/storage/rpc/google.rpc)

* BigQuery Reservation API reference

* [ BigQuery Reservation API client libraries  ](/bigquery/docs/reference/reservations)
* [ BigQuery Reservation REST API  ](/bigquery/docs/reference/reservations/rest)
* RPC reference

* [ Overview  ](/bigquery/docs/reference/reservations/rpc)
* [ google.cloud.bigquery.reservation.v1  ](/bigquery/docs/reference/reservations/rpc/google.cloud.bigquery.reservation.v1)
* [ google.rpc  ](/bigquery/docs/reference/reservations/rpc/google.rpc)

* REST reference (v1)

* REST Resources

* projects.locations

* [ Overview  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations)
* [ getBiReservation  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations/getBiReservation)
* [ searchAllAssignments  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations/searchAllAssignments)
* [ searchAssignments  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations/searchAssignments)
* [ updateBiReservation  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations/updateBiReservation)

* projects.locations.capacityCommitments

* [ Overview  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.capacityCommitments)
* [ create  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.capacityCommitments/create)
* [ delete  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.capacityCommitments/delete)
* [ get  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.capacityCommitments/get)
* [ list  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.capacityCommitments/list)
* [ merge  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.capacityCommitments/merge)
* [ patch  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.capacityCommitments/patch)
* [ split  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.capacityCommitments/split)

* projects.locations.reservations

* [ Overview  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.reservations)
* [ create  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.reservations/create)
* [ delete  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.reservations/delete)
* [ get  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.reservations/get)
* [ list  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.reservations/list)
* [ patch  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.reservations/patch)

* projects.locations.reservations.assignments

* [ Overview  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.reservations.assignments)
* [ create  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.reservations.assignments/create)
* [ delete  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.reservations.assignments/delete)
* [ list  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.reservations.assignments/list)
* [ move  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.reservations.assignments/move)
* [ patch  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.reservations.assignments/patch)

* Types

* [ BiReservation  ](/bigquery/docs/reference/reservations/rest/v1/BiReservation)
* [ Edition  ](/bigquery/docs/reference/reservations/rest/v1/Edition)

* BigQuery Analytics Hub API reference

* [ Analytics Hub client libraries  ](/bigquery/docs/reference/analytics-hub)
* [ Analytics Hub REST API  ](/bigquery/docs/reference/analytics-hub/rest)
* REST reference (v1)

* REST Resources

* organizations.locations.dataExchanges

* [ Overview  ](/bigquery/docs/reference/analytics-hub/rest/v1/organizations.locations.dataExchanges)
* [ list  ](/bigquery/docs/reference/analytics-hub/rest/v1/organizations.locations.dataExchanges/list)

* projects.locations.dataExchanges

* [ Overview  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges)
* [ create  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges/create)
* [ delete  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges/delete)
* [ get  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges/get)
* [ getIamPolicy  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges/getIamPolicy)
* [ list  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges/list)
* [ listSubscriptions  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges/listSubscriptions)
* [ patch  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges/patch)
* [ setIamPolicy  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges/setIamPolicy)
* [ subscribe  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges/subscribe)
* [ testIamPermissions  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges/testIamPermissions)

* projects.locations.dataExchanges.listings

* [ Overview  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges.listings)
* [ create  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges.listings/create)
* [ delete  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges.listings/delete)
* [ get  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges.listings/get)
* [ getIamPolicy  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges.listings/getIamPolicy)
* [ list  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges.listings/list)
* [ listSubscriptions  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges.listings/listSubscriptions)
* [ patch  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges.listings/patch)
* [ setIamPolicy  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges.listings/setIamPolicy)
* [ subscribe  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges.listings/subscribe)
* [ testIamPermissions  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges.listings/testIamPermissions)

* projects.locations.subscriptions

* [ Overview  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.subscriptions)
* [ delete  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.subscriptions/delete)
* [ get  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.subscriptions/get)
* [ list  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.subscriptions/list)
* [ refresh  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.subscriptions/refresh)
* [ revoke  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.subscriptions/revoke)

* Types

* [ ListSharedResourceSubscriptionsResponse  ](/bigquery/docs/reference/analytics-hub/rest/v1/ListSharedResourceSubscriptionsResponse)
* [ Operation  ](/bigquery/docs/reference/analytics-hub/rest/v1/Operation)

* REST reference (v1beta1)

* REST Resources

* organizations.locations.dataExchanges

* [ Overview  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/organizations.locations.dataExchanges)
* [ list  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/organizations.locations.dataExchanges/list)

* projects.locations.dataExchanges

* [ Overview  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges)
* [ create  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges/create)
* [ delete  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges/delete)
* [ get  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges/get)
* [ getIamPolicy  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges/getIamPolicy)
* [ list  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges/list)
* [ patch  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges/patch)
* [ setIamPolicy  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges/setIamPolicy)
* [ testIamPermissions  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges/testIamPermissions)

* projects.locations.dataExchanges.listings

* [ Overview  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges.listings)
* [ create  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges.listings/create)
* [ delete  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges.listings/delete)
* [ get  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges.listings/get)
* [ getIamPolicy  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges.listings/getIamPolicy)
* [ list  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges.listings/list)
* [ patch  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges.listings/patch)
* [ setIamPolicy  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges.listings/setIamPolicy)
* [ subscribe  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges.listings/subscribe)
* [ testIamPermissions  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges.listings/testIamPermissions)

* BigQuery Data Transfer Service API reference

* [ BigQuery Data Transfer Service client libraries  ](/bigquery/docs/reference/datatransfer/libraries)
* [ BigQuery Data Transfer Service REST API  ](/bigquery/docs/reference/datatransfer/rest)
* REST reference

* REST Resources

* projects

* [ Overview  ](/bigquery/docs/reference/datatransfer/rest/v1/projects)
* [ enrollDataSources  ](/bigquery/docs/reference/datatransfer/rest/v1/projects/enrollDataSources)

* projects.dataSources

* [ Overview  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.dataSources)
* [ checkValidCreds  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.dataSources/checkValidCreds)
* [ get  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.dataSources/get)
* [ list  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.dataSources/list)

* projects.locations

* [ Overview  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations)
* [ enrollDataSources  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations/enrollDataSources)
* [ get  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations/get)
* [ list  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations/list)
* [ unenrollDataSources  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations/unenrollDataSources)

* projects.locations.dataSources

* [ Overview  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.dataSources)
* [ checkValidCreds  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.dataSources/checkValidCreds)
* [ get  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.dataSources/get)
* [ list  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.dataSources/list)

* projects.locations.transferConfigs

* [ Overview  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.transferConfigs)
* [ create  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.transferConfigs/create)
* [ delete  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.transferConfigs/delete)
* [ get  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.transferConfigs/get)
* [ list  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.transferConfigs/list)
* [ patch  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.transferConfigs/patch)
* [ scheduleRuns  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.transferConfigs/scheduleRuns)
* [ startManualRuns  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.transferConfigs/startManualRuns)

* projects.locations.transferConfigs.runs

* [ Overview  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.transferConfigs.runs)
* [ delete  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.transferConfigs.runs/delete)
* [ get  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.transferConfigs.runs/get)
* [ list  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.transferConfigs.runs/list)

* projects.locations.transferConfigs.runs.transferLogs

* [ Overview  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.transferConfigs.runs.transferLogs)
* [ list  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.transferConfigs.runs.transferLogs/list)

* projects.transferConfigs

* [ Overview  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.transferConfigs)
* [ create  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.transferConfigs/create)
* [ delete  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.transferConfigs/delete)
* [ get  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.transferConfigs/get)
* [ list  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.transferConfigs/list)
* [ patch  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.transferConfigs/patch)
* [ scheduleRuns  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.transferConfigs/scheduleRuns)
* [ startManualRuns  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.transferConfigs/startManualRuns)

* projects.transferConfigs.runs

* [ Overview  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.transferConfigs.runs)
* [ delete  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.transferConfigs.runs/delete)
* [ get  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.transferConfigs.runs/get)
* [ list  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.transferConfigs.runs/list)

* projects.transferConfigs.runs.transferLogs

* [ Overview  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.transferConfigs.runs.transferLogs)
* [ list  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.transferConfigs.runs.transferLogs/list)

* Types

* [ CheckValidCredsResponse  ](/bigquery/docs/reference/datatransfer/rest/v1/CheckValidCredsResponse)
* [ Code  ](/bigquery/docs/reference/datatransfer/rest/v1/Code)
* [ EmailPreferences  ](/bigquery/docs/reference/datatransfer/rest/v1/EmailPreferences)
* [ ListDataSourcesResponse  ](/bigquery/docs/reference/datatransfer/rest/v1/ListDataSourcesResponse)
* [ ListTransferConfigsResponse  ](/bigquery/docs/reference/datatransfer/rest/v1/ListTransferConfigsResponse)
* [ ListTransferLogsResponse  ](/bigquery/docs/reference/datatransfer/rest/v1/ListTransferLogsResponse)
* [ ListTransferRunsResponse  ](/bigquery/docs/reference/datatransfer/rest/v1/ListTransferRunsResponse)
* [ RunAttempt  ](/bigquery/docs/reference/datatransfer/rest/v1/RunAttempt)
* [ ScheduleTransferRunsResponse  ](/bigquery/docs/reference/datatransfer/rest/v1/ScheduleTransferRunsResponse)
* [ StartManualTransferRunsResponse  ](/bigquery/docs/reference/datatransfer/rest/v1/StartManualTransferRunsResponse)
* [ TimeRange  ](/bigquery/docs/reference/datatransfer/rest/v1/TimeRange)
* [ TransferState  ](/bigquery/docs/reference/datatransfer/rest/v1/TransferState)

* RPC reference

* [ Overview  ](/bigquery/docs/reference/datatransfer/rpc)
* [ google.cloud.bigquery.datatransfer.v1  ](/bigquery/docs/reference/datatransfer/rpc/google.cloud.bigquery.datatransfer.v1)
* [ google.cloud.location  ](/bigquery/docs/reference/datatransfer/rpc/google.cloud.location)
* [ google.rpc  ](/bigquery/docs/reference/datatransfer/rpc/google.rpc)

* BigQuery BigLake API reference

* [ BigLake REST API  ](/bigquery/docs/reference/biglake/rest)
* REST reference (v1)

* REST Resources

* projects.locations.catalogs

* [ Overview  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs)
* [ create  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs/create)
* [ delete  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs/delete)
* [ get  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs/get)
* [ list  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs/list)

* projects.locations.catalogs.databases

* [ Overview  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs.databases)
* [ create  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs.databases/create)
* [ delete  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs.databases/delete)
* [ get  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs.databases/get)
* [ list  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs.databases/list)
* [ patch  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs.databases/patch)

* projects.locations.catalogs.databases.tables

* [ Overview  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs.databases.tables)
* [ create  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs.databases.tables/create)
* [ delete  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs.databases.tables/delete)
* [ get  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs.databases.tables/get)
* [ list  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs.databases.tables/list)
* [ patch  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs.databases.tables/patch)
* [ rename  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs.databases.tables/rename)

* REST reference (v1alpha1)

* REST Resources

* projects.locations.catalogs

* [ Overview  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs)
* [ create  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs/create)
* [ delete  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs/delete)
* [ get  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs/get)
* [ list  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs/list)

* projects.locations.catalogs.databases

* [ Overview  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases)
* [ create  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases/create)
* [ delete  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases/delete)
* [ get  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases/get)
* [ list  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases/list)
* [ patch  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases/patch)

* projects.locations.catalogs.databases.locks

* [ Overview  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases.locks)
* [ check  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases.locks/check)
* [ create  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases.locks/create)
* [ delete  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases.locks/delete)
* [ list  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases.locks/list)

* projects.locations.catalogs.databases.tables

* [ Overview  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases.tables)
* [ create  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases.tables/create)
* [ delete  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases.tables/delete)
* [ get  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases.tables/get)
* [ list  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases.tables/list)
* [ patch  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases.tables/patch)
* [ rename  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases.tables/rename)

* BigQuery routines

* [ System procedures reference  ](/bigquery/docs/reference/system-procedures)
* [ System variables reference  ](/bigquery/docs/reference/system-variables)
* BigQuery audit logging

* BigQuery audit logging reference

* [ Overview  ](/bigquery/docs/reference/auditlogs)
* Types

* [ AuditData  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/AuditData)
* [ AuditLogConfig.LogType  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/AuditLogConfig.LogType)
* [ BigQueryAuditMetadata  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata)
* [ BigQueryAuditMetadata.AccessChange.Action  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.AccessChange.Action)
* [ BigQueryAuditMetadata.ConnectionChange.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.ConnectionChange.Reason)
* [ BigQueryAuditMetadata.CreateDisposition  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.CreateDisposition)
* [ BigQueryAuditMetadata.DatasetChange.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.DatasetChange.Reason)
* [ BigQueryAuditMetadata.DatasetCreation.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.DatasetCreation.Reason)
* [ BigQueryAuditMetadata.DatasetDeletion.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.DatasetDeletion.Reason)
* [ BigQueryAuditMetadata.JobConfig.Query.Priority  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.JobConfig.Query.Priority)
* [ BigQueryAuditMetadata.JobConfig.Type  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.JobConfig.Type)
* [ BigQueryAuditMetadata.JobDeletion.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.JobDeletion.Reason)
* [ BigQueryAuditMetadata.JobInsertion.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.JobInsertion.Reason)
* [ BigQueryAuditMetadata.JobState  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.JobState)
* [ BigQueryAuditMetadata.ModelCreation.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.ModelCreation.Reason)
* [ BigQueryAuditMetadata.ModelDataChange.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.ModelDataChange.Reason)
* [ BigQueryAuditMetadata.ModelDataRead.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.ModelDataRead.Reason)
* [ BigQueryAuditMetadata.ModelDeletion.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.ModelDeletion.Reason)
* [ BigQueryAuditMetadata.ModelMetadataChange.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.ModelMetadataChange.Reason)
* [ BigQueryAuditMetadata.OperationType  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.OperationType)
* [ BigQueryAuditMetadata.QueryStatementType  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.QueryStatementType)
* [ BigQueryAuditMetadata.RoutineChange.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.RoutineChange.Reason)
* [ BigQueryAuditMetadata.RoutineCreation.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.RoutineCreation.Reason)
* [ BigQueryAuditMetadata.RoutineDeletion.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.RoutineDeletion.Reason)
* [ BigQueryAuditMetadata.SearchIndexCreation.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.SearchIndexCreation.Reason)
* [ BigQueryAuditMetadata.SearchIndexDeletion.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.SearchIndexDeletion.Reason)
* [ BigQueryAuditMetadata.TableChange.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.TableChange.Reason)
* [ BigQueryAuditMetadata.TableCreation.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.TableCreation.Reason)
* [ BigQueryAuditMetadata.TableDataChange.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.TableDataChange.Reason)
* [ BigQueryAuditMetadata.TableDataRead.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.TableDataRead.Reason)
* [ BigQueryAuditMetadata.TableDeletion.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.TableDeletion.Reason)
* [ BigQueryAuditMetadata.UnlinkDataset.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.UnlinkDataset.Reason)
* [ BigQueryAuditMetadata.WriteDisposition  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.WriteDisposition)
* [ BindingDelta.Action  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BindingDelta.Action)
* [ DatasetAccessEntry  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/DatasetAccessEntry)
* [ DatasetAccessEntry.TargetType  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/DatasetAccessEntry.TargetType)
* [ Expr  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/Expr)
* [ JoinRestrictionPolicy.JoinCondition  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/JoinRestrictionPolicy.JoinCondition)
* [ Policy  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/Policy)
* [ RoutineReference  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/RoutineReference)
* [ Status  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/Status)
* [ TableReference  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/TableReference)

* [ AI solutions, generative AI, and ML  ](/docs/ai-ml)
* [ Application development  ](/docs/application-development)
* [ Application hosting  ](/docs/application-hosting)
* [ Compute  ](/docs/compute-area)
* [ Data analytics and pipelines  ](/docs/data)
* [ Databases  ](/docs/databases)
* [ Distributed, hybrid, and multicloud  ](/docs/dhm-cloud)
* [ Industry solutions  ](/docs/industry)
* [ Networking  ](/docs/networking)
* [ Observability and monitoring  ](/docs/observability)
* [ Security  ](/docs/security)
* [ Storage  ](/docs/storage)

* [ Access and resources management  ](/docs/access-resources)
* [ Cloud SDK, languages, frameworks, and tools  ](/docs/devtools)
* [ Costs and usage management  ](/docs/costs-usage)
* [ Infrastructure as code  ](/docs/iac)
* [ Migration  ](/docs/migration)

* [ Google Cloud Home  ](/)
* [ Free Trial and Free Tier  ](/free)
* [ Architecture Center  ](/architecture)
* [ Blog  ](https://cloud.google.com/blog)
* [ Contact Sales  ](/contact)
* [ Google Cloud Developer Center  ](/developers)
* [ Google Developer Center  ](https://developers.google.com/)
* [ Google Cloud Marketplace (in console)  ](https://console.cloud.google.com/marketplace)
* [ Google Cloud Marketplace Documentation  ](/marketplace/docs)
* [ Google Cloud Skills Boost  ](https://www.cloudskillsboost.google/paths)
* [ Google Cloud Solution Center  ](/solutions)
* [ Google Cloud Support  ](/support-hub)
* [ Google Cloud Tech Youtube Channel  ](https://www.youtube.com/@googlecloudtech)

* [ Home ](https://cloud.google.com/)
* [ BigQuery ](https://cloud.google.com/bigquery)
* [ Documentation ](https://cloud.google.com/bigquery/docs)
* [ Reference ](https://cloud.google.com/bigquery/quotas)

Send feedback  Stay organized with collections  Save and categorize content
based on your preferences.

#  The CREATE MODEL statement for boosted tree models using XGBoost

This document describes the ` CREATE MODEL ` statement for creating [ boosted
tree ](https://xgboost.readthedocs.io/en/stable/tutorials/model.html) models
in BigQuery. Boosted tree models are trained using the [ XGBoost library
](https://xgboost.readthedocs.io/en/latest/) . Boosted tree models support [
hyperparameter tuning ](/bigquery/docs/hp-tuning-overview) .

For information about the supported SQL statements and functions for each
model type, see [ End-to-end user journey for each model
](/bigquery/docs/e2e-journey) .

##  ` CREATE MODEL ` syntax



{CREATE MODEL | CREATE MODEL IF NOT EXISTS | CREATE OR REPLACE MODEL} model_name
OPTIONS(model_option_list)
AS query_statement

model_option_list:
MODEL_TYPE = { 'BOOSTED_TREE_CLASSIFIER' | 'BOOSTED_TREE_REGRESSOR' }
[, APPROX_GLOBAL_FEATURE_CONTRIB = { TRUE | FALSE } ]
[, CATEGORY_ENCODING_METHOD = { 'ONE_HOT_ENCODING` | 'TARGET_ENCODING' | 'LABEL_ENCODING' } ]
[, BOOSTER_TYPE = { {'GBTREE' | 'DART'} | HPARAM_CANDIDATES([candidates]) } ]
[, DART_NORMALIZE_TYPE = { {'TREE' | 'FOREST'} | HPARAM_CANDIDATES([candidates]) } ]
[, NUM_PARALLEL_TREE = { int64_value | HPARAM_RANGE(range) | HPARAM_CANDIDATES([candidates]) } ]
[, MAX_TREE_DEPTH = { int64_value | HPARAM_RANGE(range) | HPARAM_CANDIDATES([candidates]) } ]
[, DROPOUT = { float64_value | HPARAM_RANGE(range) | HPARAM_CANDIDATES([candidates]) } ]
[, L1_REG = { float64_value | HPARAM_RANGE(range) | HPARAM_CANDIDATES([candidates]) } ]
[, L2_REG = { float64_value | HPARAM_RANGE(range) | HPARAM_CANDIDATES([candidates]) } ]
[, LEARN_RATE = { float64_value | HPARAM_RANGE(range) | HPARAM_CANDIDATES([candidates]) } ]
[, TREE_METHOD = { {'AUTO' | 'EXACT' | 'APPROX' | 'HIST'} | HPARAM_CANDIDATES([candidates]) } ]
[, MIN_TREE_CHILD_WEIGHT = { int64_value | HPARAM_RANGE(range) | HPARAM_CANDIDATES([candidates]) } ]
[, COLSAMPLE_BYTREE = { float64_value | HPARAM_RANGE(range) | HPARAM_CANDIDATES([candidates]) } ]
[, COLSAMPLE_BYLEVEL = { float64_value | HPARAM_RANGE(range) | HPARAM_CANDIDATES([candidates]) } ]
[, COLSAMPLE_BYNODE = { float64_value | HPARAM_RANGE(range) | HPARAM_CANDIDATES([candidates]) } ]
[, MIN_SPLIT_LOSS = { float64_value | HPARAM_RANGE(range) | HPARAM_CANDIDATES([candidates]) } ]
[, SUBSAMPLE = { float64_value | HPARAM_RANGE(range) | HPARAM_CANDIDATES([candidates]) } ]
[, INSTANCE_WEIGHT_COL = string_value ]
[, XGBOOST_VERSION = { '0.9' | '1.1' } ]
[, AUTO_CLASS_WEIGHTS = { TRUE | FALSE } ]
[, CLASS_WEIGHTS = struct_array ]
[, ENABLE_GLOBAL_EXPLAIN = { TRUE | FALSE } ]
[, EARLY_STOP = { TRUE | FALSE } ]
[, MIN_REL_PROGRESS = float64_value ]
[, INPUT_LABEL_COLS = string_array ]
[, MAX_ITERATIONS = int64_value ]
[, DATA_SPLIT_METHOD = { 'AUTO_SPLIT' | 'RANDOM' | 'CUSTOM' | 'SEQ' | 'NO_SPLIT' } ]
[, DATA_SPLIT_EVAL_FRACTION = float64_value ]
[, DATA_SPLIT_TEST_FRACTION = float64_value ]
[, DATA_SPLIT_COL = string_value ]
[, NUM_TRIALS = int64_value ]
[, MAX_PARALLEL_TRIALS = int64_value ]
[, HPARAM_TUNING_ALGORITHM = { 'VIZIER_DEFAULT' | 'RANDOM_SEARCH' | 'GRID_SEARCH' } ]
[, HPARAM_TUNING_OBJECTIVES = { 'ROC_AUC' | 'R2_SCORE' | ... } ]


###  ` CREATE MODEL `

Creates and trains a new model in the specified dataset. If the model name
exists, ` CREATE MODEL ` returns an error.

###  ` CREATE MODEL IF NOT EXISTS `

Creates and trains a new model only if the model doesn't exist in the
specified dataset.

###  ` CREATE OR REPLACE MODEL `

Creates and trains a model and replaces an existing model with the same name
in the specified dataset.

###  ` model_name `

The name of the model you're creating or replacing. The model name must be
unique in the dataset: no other model or table can have the same name. The
model name must follow the same naming rules as a BigQuery table. A model name
can:

* Contain up to 1,024 characters
* Contain letters (upper or lower case), numbers, and underscores

` model_name ` is not case-sensitive.

If you don't have a default project configured, then you must prepend the
project ID to the model name in the following format, including backticks:

`[PROJECT_ID].[DATASET].[MODEL]`

For example, `myproject.mydataset.mymodel`.

###  ` MODEL_TYPE `

**Syntax**



MODEL_TYPE = { 'BOOSTED_TREE_CLASSIFIER' | 'BOOSTED_TREE_REGRESSOR' }


**Description**

Specifies the model type. This option is required.

###  ` APPROX_GLOBAL_FEATURE_CONTRIB `

**Syntax**



APPROX_GLOBAL_FEATURE_CONTRIB = { TRUE | FALSE }


**Description**

Enables fast approximation for feature contributions. This functionality is
provided by the XGBoost library; BigQuery ML only passes this option through
to it. For more information, see [ Package 'xgboost'
](https://cran.r-project.org/web/packages/xgboost/xgboost.pdf) and search for
` approxcontrib ` .

In order to use the fast approximation for feature contribution computations,
you need to set both ` ENABLE_GLOBAL_EXPLAIN ` and `
APPROX_GLOBAL_FEATURE_CONTRIB ` to ` TRUE ` .

**Arguments**

A ` BOOL ` value. The default value is ` TRUE ` when ` ENABLE_GLOBAL_EXPLAIN `
is ` TRUE ` and ` NUM_PARALLEL_TREE >= 10 ` , otherwise it is ` FALSE ` .

###  ` CATEGORY_ENCODING_METHOD `

**Syntax**



CATEGORY_ENCODING_METHOD = { 'ONE_HOT_ENCODING' | 'TARGET_ENCODING' | 'LABEL_ENCODING' }


**Description**

Specifies which encoding method to use on non-numeric features. For more
information about supported encoding methods, see [ BigQuery ML auto
preprocessing ](/bigquery/docs/auto-preprocessing) .

**Arguments**

This option accepts the following values:

* ` LABEL_ENCODING ` . This is the default.
* ` TARGET_ENCODING `
* ` ONE_HOT_ENCODING `

###  ` BOOSTER_TYPE `

**Syntax**

` BOOSTER_TYPE = { { 'GBTREE' | 'DART'} | HPARAM_CANDIDATES(  [candidates]  )
} `

**Description**

Specifies the booster type to use.

**Arguments**

This option accepts the following values:

* ` GBTREE ` : [ tree-booster ](https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-tree-booster) . This is the default.
* ` DART ` : [ dart-booster ](https://xgboost.readthedocs.io/en/latest/tutorials/dart.html)

If you are running hyperparameter training, you can provide more than one
value for this option by using ` HPARAM_CANDIDATES ` and specifying an array.
For example, ` BOOSTER_TYPE = HPARAM_CANDIDATES(['GBTREE', 'DART']) ` .

###  ` DART_NORMALIZE_TYPE `

**Syntax**

` DART_NORMALIZE_TYPE = { { 'TREE' | 'FOREST'} | HPARAM_CANDIDATES(
[candidates]  ) } `

**Description**

The type of normalization algorithm to use if you are using the ` DART `
booster.

**Arguments**

This option accepts the following values:

* ` TREE ` : New trees have the same weight of each of the dropped trees. This is the default value.
* ` FOREST ` : New trees have the same weight of the sum of the dropped trees (forest).

If you are running hyperparameter training, you can provide more than one
value for this option by using ` HPARAM_CANDIDATES ` and specifying an array.
For example, ` DART_NORMALIZE_TYPE = HPARAM_CANDIDATES(['TREE', 'FOREST']) ` .

###  ` NUM_PARALLEL_TREE `

**Syntax**

` NUM_PARALLEL_TREE = { int64_value | HPARAM_RANGE(  range  ) |
HPARAM_CANDIDATES(  [candidates]  ) } `

**Description**

The number of parallel trees constructed during each iteration. To train a
boosted random forest model, set this value to larger than ` 1 ` .

**Arguments**

If you aren't running hyperparameter tuning, then you can specify an ` INT64 `
value. The default value is ` 1 ` .

If you are running hyperparameter tuning, use one of the following options:

* The ` HPARAM_RANGE ` keyword and two ` INT64 ` values that define the range of the hyperparameter. For example, ` NUM_PARALLEL_TREE = HPARAM_RANGE(1, 5) ` .
* The ` HPARAM_CANDIDATES ` keyword and an array of ` INT64 ` values that provide discrete values to use for the hyperparameter. For example, ` NUM_PARALLEL_TREE = HPARAM_CANDIDATES([0, 1, 3, 5]) ` .

When running hyperparameter tuning, the valid range is ` (1, ∞] ` , there is
no default range, and the scale type is ` LINEAR ` .

###  ` MAX_TREE_DEPTH `

**Syntax**

` MAX_TREE_DEPTH = { int64_value | HPARAM_RANGE(  range  ) |
HPARAM_CANDIDATES(  [candidates]  ) } `

**Description**

The maximum depth of a tree.

**Arguments**

If you aren't running hyperparameter tuning, then you can specify an ` INT64 `
value. The default value is ` 6 ` .

If you are running hyperparameter tuning, use one of the following options:

* The ` HPARAM_RANGE ` keyword and two ` INT64 ` values that define the range of the hyperparameter. For example, ` MAX_TREE_DEPTH = HPARAM_RANGE(0, 4) ` .
* The ` HPARAM_CANDIDATES ` keyword and an array of ` INT64 ` values that provide discrete values to use for the hyperparameter. For example, ` MAX_TREE_DEPTH = HPARAM_CANDIDATES([1, 5, 10, 15]) ` .

When running hyperparameter tuning, the valid range is ` (1, 20] ` , the
default range is ` (1, 10] ` , and the scale type is ` LINEAR ` .

###  ` DROPOUT `

**Syntax**

` DROPOUT = { float64_value | HPARAM_RANGE(  range  ) | HPARAM_CANDIDATES(
[candidates]  ) } `

**Description**

Specifies the dropout rate, which is the fraction of previous trees to drop
during the dropout.

**Arguments**

If you aren't running hyperparameter tuning, then you can specify a ` FLOAT64
` value between ` 0 ` and ` 1.0 ` . The default value is ` 0 ` .

If you are running hyperparameter tuning, use one of the following options:

* The ` HPARAM_RANGE ` keyword and two ` FLOAT64 ` values that define the range of the hyperparameter. For example, ` DROPOUT = HPARAM_RANGE(0, 0.6) ` .
* The ` HPARAM_CANDIDATES ` keyword and an array of ` FLOAT64 ` values that provide discrete values to use for the hyperparameter. For example, ` DROPOUT = HPARAM_CANDIDATES([0, 0.1, 0.2, 0.6]) ` .

When running hyperparameter tuning, the valid range is ` (0, 1.0] ` , there is
no default range, and the scale type is ` LINEAR ` .

###  ` L1_REG `

**Syntax**

` L1_REG = {  float64_value  | HPARAM_RANGE(  range  ) | HPARAM_CANDIDATES(
[candidates]  ) } `

**Description**

The amount of [ L1 regularization ](https://developers.google.com/machine-
learning/glossary/#L1_regularization) applied.

**Arguments**

If you aren't running hyperparameter tuning, then you can specify a ` FLOAT64
` value. The default value is ` 0 ` .

If you are running hyperparameter tuning, then you can use one of the
following options:

* The ` HPARAM_RANGE ` keyword and two ` FLOAT64 ` values that define the range to use for the hyperparameter. For example, ` L1_REG = HPARAM_RANGE(0, 5.0) ` .
* The ` HPARAM_CANDIDATES ` keyword and an array of ` FLOAT64 ` values that provide discrete values to use for the hyperparameter. For example, ` L1_REG = HPARAM_CANDIDATES([0, 1.0, 3.0, 5.0]) ` .

When running hyperparameter tuning, the valid range is ` (0, ∞) ` , the
default range is ` (0, 10.0] ` , and the scale type is ` LOG ` .

###  ` L2_REG `

**Syntax**

` L2_REG = {  float64_value  | HPARAM_RANGE(  range  ) | HPARAM_CANDIDATES(
[candidates]  ) } `

**Description**

The amount of [ L2 regularization ](https://developers.google.com/machine-
learning/glossary/#L2_regularization) applied.

**Arguments**

If you aren't running hyperparameter tuning, then you can specify a ` FLOAT64
` value. The default value is ` 1.0 ` .

If you are running hyperparameter tuning, then you can use one of the
following options:

* The ` HPARAM_RANGE ` keyword and two ` FLOAT64 ` values that define the range to use for the hyperparameter. For example, ` L2_REG = HPARAM_RANGE(1.5, 5.0) ` .
* The ` HPARAM_CANDIDATES ` keyword and an array of ` FLOAT64 ` values that provide discrete values to use for the hyperparameter. For example, ` L2_REG = HPARAM_CANDIDATES([0, 1.0, 3.0, 5.0]) ` .

When running hyperparameter tuning, the valid range is ` (0, ∞) ` , the
default range is ` (0, 10.0] ` , and the scale type is ` LOG ` .

###  ` LEARN_RATE `

**Syntax**

` LEARN_RATE = {  float64_value  | HPARAM_RANGE(  range  ) |
HPARAM_CANDIDATES(  [candidates]  ) } `

**Description**

` LEARN_RATE ` is the step size shrinkage used in updates to prevents
overfitting. After each boosting step, ` LEARN_RATE ` shrinks the feature
weights to make the boosting process more conservative.

**Arguments**

If you aren't running hyperparameter tuning, then you can specify a ` FLOAT64
` value. The default value is ` 0.3 ` .

If you are running hyperparameter tuning, use one of the following options:

* The ` HPARAM_RANGE ` keyword and two ` FLOAT64 ` values that define the range of the hyperparameter. For example, ` LEARN_RATE = HPARAM_RANGE(0, 0.5) ` .
* The ` HPARAM_CANDIDATES ` keyword and an array of ` FLOAT64 ` values that provide discrete values to use for the hyperparameter. For example, ` LEARN_RATE = HPARAM_CANDIDATES([0, 0.1, 0.3, 0.5]) ` .

When running hyperparameter tuning, the valid range is ` (0, ∞] ` , the
default range is ` (0, 1.0] ` , and the scale type is ` LINEAR ` .

###  ` TREE_METHOD `

**Syntax**

` TREE_METHOD = { { 'AUTO' | 'EXACT' | 'APPROX' | 'HIST'} | HPARAM_CANDIDATES(
[candidates]  ) } `

**Description**

The type of tree construction algorithm.

` HIST ` is recommended for large datasets in order to increase training speed
and reduce resource consumption. For more information, see [ tree booster
](https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-tree-
booster) .

**Arguments**

This option accepts the following values:

* ` AUTO ` : Faster histogram optimized approximate greedy algorithm. This is the default.
* ` EXACT ` : Exact greedy algorithm. Enumerates all split candidates.
* ` APPROX ` : Approximate greedy algorithm using quantile sketch and gradient histogram.
* ` HIST ` : Same as the ` AUTO ` tree method..

If you are running hyperparameter training, you can provide more than one
value for this option by using ` HPARAM_CANDIDATES ` and specifying an array.
For example, ` TREE_METHOD = HPARAM_CANDIDATES(['APPROX', 'HIST']) ` .

###  ` MIN_TREE_CHILD_WEIGHT `

**Syntax**

` MIN_TREE_CHILD_WEIGHT = { int64_value | HPARAM_RANGE(  range  ) |
HPARAM_CANDIDATES(  [candidates]  ) } `

**Description**

The minimum sum of instance weight needed in a child for further partitioning.
If the tree partition step results in a leaf node whose sum of instance weight
is less than ` MIN_TREE_CHILD_WEIGHT ` , then the building process stops
partitioning. The larger the ` MIN_TREE_CHILD_WEIGHT ` value is, the more
conservative the algorithm is.

**Arguments**

If you aren't running hyperparameter tuning, then you can specify an ` INT64 `
value greater than or equal to ` 0 ` . The default value is ` 1 ` .

If you are running hyperparameter tuning, use one of the following options:

* The ` HPARAM_RANGE ` keyword and two ` INT64 ` values that define the range of the hyperparameter. For example, ` MIN_TREE_CHILD_WEIGHT = HPARAM_RANGE(0, 5) ` .
* The ` HPARAM_CANDIDATES ` keyword and an array of ` INT64 ` values that provide discrete values to use for the hyperparameter. For example, ` MIN_TREE_CHILD_WEIGHT = HPARAM_CANDIDATES([0, 1, 3, 5]) ` .

When running hyperparameter tuning, the valid range is ` [0, ∞) ` , there is
no default range, and the scale type is ` LINEAR ` .

###  ` COLSAMPLE_BYTREE `

**Syntax**

` COLSAMPLE_BYTREE = { float64_value | HPARAM_RANGE(  range  ) |
HPARAM_CANDIDATES(  [candidates]  ) } `

**Description**

The subsample ratio of columns when constructing each tree. Subsampling occurs
once for every tree constructed.

**Arguments**

If you aren't running hyperparameter tuning, then you can specify a ` FLOAT64
` value between ` 0 ` and ` 1.0 ` . The default value is ` 1.0 ` .

If you are running hyperparameter tuning, use one of the following options:

* The ` HPARAM_RANGE ` keyword and two ` FLOAT64 ` values that define the range of the hyperparameter. For example, ` COLSAMPLE_BYTREE = HPARAM_RANGE(0, 0.3)) ` .
* The ` HPARAM_CANDIDATES ` keyword and an array of ` FLOAT64 ` values that provide discrete values to use for the hyperparameter. For example, ` COLSAMPLE_BYTREE = HPARAM_CANDIDATES([0, 0.1, 0.3, 0.5]) ` .

When running hyperparameter tuning, the valid range is ` [0, 1.0] ` , there is
no default range, and the scale type is ` LINEAR ` .

###  ` COLSAMPLE_BYLEVEL `

**Syntax**

` COLSAMPLE_BYLEVEL = { float64_value | HPARAM_RANGE(  range  ) |
HPARAM_CANDIDATES(  [candidates]  ) } `

**Description**

The subsample ratio of columns for each level. Subsampling occurs once for
every new depth level reached in a tree. Columns are subsampled from the set
of columns chosen for the current tree.

**Arguments**

If you aren't running hyperparameter tuning, then you can specify a ` FLOAT64
` value between ` 0 ` and ` 1.0 ` . The default value is ` 1.0 ` .

If you are running hyperparameter tuning, use one of the following options:

* The ` HPARAM_RANGE ` keyword and two ` FLOAT64 ` values that define the range of the hyperparameter. For example, ` COLSAMPLE_BYLEVEL = HPARAM_RANGE(0, 0.3)) ` .
* The ` HPARAM_CANDIDATES ` keyword and an array of ` FLOAT64 ` values that provide discrete values to use for the hyperparameter. For example, ` COLSAMPLE_BYLEVEL = HPARAM_CANDIDATES([0, 0.1, 0.3, 0.5]) ` .

When running hyperparameter tuning, the valid range is ` [0, 1.0] ` , there is
no default range, and the scale type is ` LINEAR ` .

###  ` COLSAMPLE_BYNODE `

**Syntax**

` COLSAMPLE_BYNODE = { float64_value | HPARAM_RANGE(  range  ) |
HPARAM_CANDIDATES(  [candidates]  ) } `

**Description**

The subsample ratio of columns for each node (split). Subsampling occurs once
for every time a new split is evaluated. Columns are subsampled from the set
of columns chosen for the current level.

**Arguments**

If you aren't running hyperparameter tuning, then you can specify a ` FLOAT64
` value between ` 0 ` and ` 1.0 ` . The default value is ` 1.0 ` .

If you are running hyperparameter tuning, use one of the following options:

* The ` HPARAM_RANGE ` keyword and two ` FLOAT64 ` values that define the range of the hyperparameter. For example, ` COLSAMPLE_BYNODE = HPARAM_RANGE(0, 0.3)) ` .
* The ` HPARAM_CANDIDATES ` keyword and an array of ` FLOAT64 ` values that provide discrete values to use for the hyperparameter. For example, ` COLSAMPLE_BYNODE = HPARAM_CANDIDATES([0, 0.1, 0.3, 0.5]) ` .

When running hyperparameter tuning, the valid range is ` [0, 1.0] ` , there is
no default range, and the scale type is ` LINEAR ` .

###  ` MIN_SPLIT_LOSS `

**Syntax**

` MIN_SPLIT_LOSS = { float64_value | HPARAM_RANGE(  range  ) |
HPARAM_CANDIDATES(  [candidates]  ) } `

**Description**

The minimum loss reduction required to make a further partition on a leaf node
of the tree. The larger the ` MIN_SPLIT_LOSS ` value is, the more conservative
the algorithm is.

**Arguments**

If you aren't running hyperparameter tuning, then you can specify a ` FLOAT64
` value. The default value is ` 0 ` .

If you are running hyperparameter tuning, use one of the following options:

* The ` HPARAM_RANGE ` keyword and two ` FLOAT64 ` values that define the range of the hyperparameter. For example, ` MIN_SPLIT_LOSS = HPARAM_RANGE(0, 5.5) ` .
* The ` HPARAM_CANDIDATES ` keyword and an array of ` FLOAT64 ` values that provide discrete values to use for the hyperparameter. For example, ` MIN_SPLIT_LOSS = HPARAM_CANDIDATES([0, 0.5, 1.5, 2.5]) ` .

When running hyperparameter tuning, the valid range is ` [0, ∞) ` , there is
no default range, and the scale type is ` LINEAR ` .

###  ` SUBSAMPLE `

**Syntax**

` SUBSAMPLE = { float64_value | HPARAM_RANGE(  range  ) | HPARAM_CANDIDATES(
[candidates]  ) } `

**Description**

The subsample ratio of the training instances. Setting this value to ` 0.5 `
means that training randomly samples half of the training data prior to
growing trees, which prevents overfitting. Subsampling occurs once in every
boosting iteration. This is independent of the training-test data split used
in the training options. The test data is not used in any iteration
irrespective of the ` SUBSAMPLE ` value; subsampling is only applied to the
training data.

**Arguments**

If you aren't running hyperparameter tuning, then you can specify a ` FLOAT64
` value between ` 0 ` and ` 1.0 ` . The default value is ` 1.0 ` , which
specifies that subsampling uses all of the training data in each iteration.

If you are running hyperparameter tuning, use one of the following options:

* The ` HPARAM_RANGE ` keyword and two ` FLOAT64 ` values that define the range of the hyperparameter. For example, ` SUBSAMPLE = HPARAM_RANGE(0, 0.6) ` .
* The ` HPARAM_CANDIDATES ` keyword and an array of ` FLOAT64 ` values that provide discrete values to use for the hyperparameter. For example, ` SUBSAMPLE = HPARAM_CANDIDATES([0, 0.1, 0.2, 0.6]) ` .

When running hyperparameter tuning, the valid range is ` (0, 1.0] ` , the
default range is ` (0, 1.0] ` , and the scale type is ` LINEAR ` .

###  ` INSTANCE_WEIGHT_COL `

**Syntax**

` INSTANCE_WEIGHT_COL =  string_value  `

**Description**

The column used to specify the weights for each data point in the training
dataset. The column you specify must be a numerical column. You can't use this
column as a feature or label, and it is excluded from features automatically.
You can't specify this option if ` AUTO_CLASS_WEIGHTS ` is ` TRUE ` or if `
CLASS_WEIGHTS ` is set.

The ` INSTANCE_WEIGHT_COL ` option is only supported for non-array features.

**Arguments**

A ` STRING ` value.

###  ` XGBOOST_VERSION `

**Syntax**



XGBOOST_VERSION = { '0.9' | '1.1' }


**Description**

The XGBoost version for model training.

**Arguments**

This option accepts the following values:

* ` 0.9 ` . This is the default.
* ` 1.1 `

###  ` AUTO_CLASS_WEIGHTS `

**Syntax**



AUTO_CLASS_WEIGHTS = { TRUE | FALSE }


**Description**

Determines whether to balance class labels by using weights for each class in
inverse proportion to the frequency of that class.

Only use this option with classifier models.

By default, the training data used to create the model is unweighted. If the
labels in the training data are imbalanced, the model might learn to predict
the most popular class of labels more heavily, which you might not want.

To balance every class, set this option to ` TRUE ` . Balance is accomplished
using the following formula:



total_input_rows / (input_rows_for_class_n * number_of_unique_classes)


**Arguments**

A ` BOOL ` value. The default value is ` FALSE ` .

###  ` CLASS_WEIGHTS `

**Syntax**

` CLASS_WEIGHTS =  struct_array  `

**Description**

The weights to use for each class label. You can't specify this option if  `
AUTO_CLASS_WEIGHTS ` is ` TRUE ` .

**Arguments**

An ` ARRAY ` of ` STRUCT ` values. Each ` STRUCT ` contains a ` STRING ` value
that specifies the class label and a ` FLOAT64 ` value that specifies the
weight for that class label. A weight must be present for every class label.
The weights are not required to add up to 1.

A ` CLASS_WEIGHTS ` value might look like the following example:



CLASS_WEIGHTS = [STRUCT('example_label', .2)]


###  ` ENABLE_GLOBAL_EXPLAIN `

**Syntax**



ENABLE_GLOBAL_EXPLAIN = { TRUE | FALSE }


**Description**

Determines whether to compute global explanations by using [ explainable AI
](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-xai-overview) to
evaluate the importance of global features to the model.

Global explanations are computed when you create the model. This option must
be ` TRUE ` if you want to use the [ ` ML.GLOBAL_EXPLAIN ` function
](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-global-explain) to
retrieve the global explanations after the model is created.

**Arguments**

A ` BOOL ` value. The default value is ` FALSE ` .

###  ` EARLY_STOP `

**Syntax**



EARLY_STOP = { TRUE | FALSE }


**Description**

Determines whether training should stop after the first iteration in which the
relative loss improvement is less than the value specified for  `
MIN_REL_PROGRESS ` .

**Arguments**

A ` BOOL ` value. The default value is ` TRUE ` .

###  ` MIN_REL_PROGRESS `

**Syntax**

` MIN_REL_PROGRESS =  float64_value  `

**Description**

The minimum relative loss improvement that is necessary to continue training
when  ` EARLY_STOP ` is set to ` TRUE ` . For example, a value of ` 0.01 `
specifies that each iteration must reduce the loss by 1% for training to
continue.

**Arguments**

A ` FLOAT64 ` value. The default value is ` 0.01 ` .

###  ` INPUT_LABEL_COLS `

**Syntax**

` INPUT_LABEL_COLS =  string_array  `

**Description**

The name of the label column in the training data.

**Arguments**

A one-element ` ARRAY ` of string values. Defaults to ` label ` .

###  ` MAX_ITERATIONS `

**Syntax**

` MAX_ITERATIONS =  int64_value  `

**Description**

The maximum number of rounds for boosting.

**Arguments**

An ` INT64 ` value. The default value is ` 20 ` .

###  ` DATA_SPLIT_METHOD `

**Syntax**



DATA_SPLIT_METHOD = { 'AUTO_SPLIT' | 'RANDOM' | 'CUSTOM' | 'SEQ' | 'NO_SPLIT' }


**Description**

The method used to split input data into training, evaluation, and, if you are
running hyperparameter tuning, test data sets. Training data is used to train
the model. Evaluation data is used to avoid [ overfitting
](https://developers.google.com/machine-learning/glossary/#overfitting) by
using early stopping. Test data is used to test the hyperparameter tuning
trial and record its metrics in the model.

The percentage sizes of the data sets produced by the various arguments for
this option are approximate. Larger input data sets come closer to the
percentages described than smaller input data sets do.

You can see the model's data split information in the following ways:

* The data split method and percentage are shown in the **Training Options** section of the model's **Details** page on the BigQuery page of the Google Cloud console.
* Links to temporary tables that contain the split data are available in the **Model Details** section of the model's **Details** page on the BigQuery of the Google Cloud console. You can also return this information from the [ ` DataSplitResult ` field ](/bigquery/docs/reference/rest/v2/models#datasplitresult) in the BigQuery API. These tables are saved for 48 hours. If you need this information for more than 48 hours, then you should export this data or copy it to permanent tables.

**Arguments**

This option accepts the following values:

* ` AUTO_SPLIT ` : This is the default value. This option splits the data as follows:

* If there are fewer than 500 rows in the input data, then all rows are used as training data.
* If you aren't running hyperparameter tuning, then data is randomized and split as follows:

* If there are between 500 and 50,000 rows in the input data, then 20% of the data is used as evaluation data and 80% is used as training data.
* If there are more than 50,000 rows, then 10,000 rows are used as evaluation data and the remaining rows are used as training data.
* If you are running hyperparameter tuning and there are more than 500 rows in the input data, then the data is randomized and split as follows:

* 10% of the data is used as evaluation data
* 10% is used as test data
* 80% is used as training data

For more information, see [ Data split ](/bigquery/docs/hp-tuning-
overview#data_split) .

* ` RANDOM ` : Data is randomized before being split into sets. You can use this option with the  ` DATA_SPLIT_EVAL_FRACTION ` and  ` DATA_SPLIT_TEST_FRACTION ` options to customize the data split. If you don't specify either of those options, data is split in the same way as for the ` AUTO_SPLIT ` option.

A random split is deterministic: different training runs produce the same
split results if the same underlying training data is used.

**Note:** A random split is based on the [ FARM_FINGERPRINT
](/bigquery/docs/reference/standard-sql/hash_functions#farm_fingerprint) of
the data (including the column name and schema), so tables with the same
content but different column names and schemas may get different splitting and
different evaluation metrics.

* ` CUSTOM ` : Split data using the value in a specified column:

* If you aren't running hyperparameter tuning, then you must provide the name of a column of type ` BOOL ` . Rows with a value of ` TRUE ` or ` NULL ` are used as evaluation data, rows with a value of ` FALSE ` are used as training data.
* If you are running hyperparameter tuning, then you must provide the name of a column of type ` STRING ` . Rows with a value of ` TRAIN ` are used as training data, rows with a value of ` EVAL ` are used as evaluation data, and rows with a value of ` TEST ` are used as test data.

Use the  ` DATA_SPLIT_COL ` option  to identify the column that contains the
data split information.

* ` SEQ ` : Split data sequentially by using the value in a specified column of one of the following types:

* ` NUMERIC `
* ` BIGNUMERIC `
* ` STRING `
* ` TIMESTAMP `

The data is sorted smallest to largest based on the specified column.

When you aren't running hyperparameter tuning, the first  n  rows are used as
evaluation data, where  n  is the value specified for `
DATA_SPLIT_EVAL_FRACTION ` . The remaining rows are used as training data.

When you are running hyperparameter tuning, the first  n  rows are used as
evaluation data, where  n  is the value specified for `
DATA_SPLIT_EVAL_FRACTION ` . The next  m  rows are used as test data, where  m
is the value specified for ` DATA_SPLIT_TEST_FRACTION ` . The remaining rows
are used as training data.

All rows with split values smaller than the threshold are used as training
data. The remaining rows, including ` NULLs ` , are used as evaluation data.

Use the ` DATA_SPLIT_COL ` option to identify the column that contains the
data split information.

* ` NO_SPLIT ` : No data split; all input data is used as training data.

###  ` DATA_SPLIT_EVAL_FRACTION `

**Syntax**

` DATA_SPLIT_EVAL_FRACTION =  float64_value  `

**Description**

The fraction of the data to use as evaluation data. Use when you are
specifying ` RANDOM ` or ` SEQ ` as the value for the  ` DATA_SPLIT_COL `
option  .

If you are running hyperparameter tuning and you specify a value for this
option, you must also specify a value for  ` DATA_SPLIT_TEST_FRACTION ` . In
this case, the training dataset is ` 1 -  eval_fraction  -  test_fraction  ` .
For example, if you specify ` 20.00 ` for ` DATA_SPLIT_EVAL_FRACTION ` and `
8.0 ` for ` DATA_SPLIT_TEST_FRACTION ` , your training dataset is 72% of the
input data.

**Arguments**

A ` FLOAT64 ` value. The default is ` 0.2 ` . The service maintains the
accuracy of the input value to two decimal places.

###  ` DATA_SPLIT_TEST_FRACTION `

**Syntax**

` DATA_SPLIT_TEST_FRACTION =  float64_value  `

**Description**

The fraction of the data to use as test data. Use this option when you are
running hyperparameter tuning and specifying either ` RANDOM ` or ` SEQ ` as
value for the  ` DATA_SPLIT_COL ` option  .

If you specify a value for this option, you must also specify a value for  `
DATA_SPLIT_EVAL_FRACTION ` . In this case, the training dataset is ` 1 -
eval_fraction  -  test_fraction  ` . For example, if you specify ` 20.00 ` for
` DATA_SPLIT_EVAL_FRACTION ` and ` 8.0 ` for ` DATA_SPLIT_TEST_FRACTION ` ,
your training dataset is 72% of the input data.

**Arguments**

A ` FLOAT64 ` value. The default is ` 0 ` . The service maintains the accuracy
of the input value to two decimal places.

###  ` DATA_SPLIT_COL `

**Syntax**

` DATA_SPLIT_COL =  string_value  `

**Description**

The name of the column to use to sort input data into the training,
evaluation, or test set. Use when you are specifying ` CUSTOM ` or ` SEQ ` as
the value for ` DATA_SPLIT_COL ` :

* If you aren't running hyperparameter tuning and you are specifying ` SEQ ` as the value for ` DATA_SPLIT_METHOD ` , then the data is first sorted smallest to largest based on the specified column. The last  n  rows are used as evaluation data, where  n  is the value specified for  ` DATA_SPLIT_EVAL_FRACTION ` . The remaining rows are used as training data.
* If you aren't running hyperparameter tuning and you are specifying ` CUSTOM ` as the value for ` DATA_SPLIT_COL ` , then you must provide the name of a column of type ` BOOL ` . Rows with a value of ` TRUE ` or ` NULL ` are used as evaluation data, rows with a value of ` FALSE ` are used as training data.
* If you are running hyperparameter tuning and you are specifying ` SEQ ` as the value for ` DATA_SPLIT_METHOD ` , then the data is first sorted smallest to largest based on the specified column. The last  n  rows are used as evaluation data, where  n  is the value specified for  ` DATA_SPLIT_EVAL_FRACTION ` . The next  m  rows are used as test data, where  m  is the value specified for  ` DATA_SPLIT_TEST_FRACTION ` . The remaining rows are used as training data.
* If you are running hyperparameter tuning and you are specifying ` CUSTOM ` as the value for ` DATA_SPLIT_METHOD ` , then you must provide the name of a column of type ` STRING ` . Rows with a value of ` TRAIN ` are used as training data, rows with a value of ` EVAL ` are used as evaluation data, and rows with a value of ` TEST ` are used as test data.

The column you specify for ` DATA_SPLIT_COL ` can't be used as a feature or
label, and is excluded from features automatically.

**Arguments**

A ` STRING ` value.

###  ` NUM_TRIALS `

**Syntax**

` NUM_TRIALS =  int64_value  `

**Description**

The maximum number of submodels to train. The tuning stops when ` NUM_TRIALS `
submodels are trained, or when the hyperparameter search space is exhausted.
You must specify this option in order to use hyperparameter tuning.

**Arguments**

An ` INT64 ` value between ` 1 ` and ` 100 ` , inclusive.

**Note:** We recommend using at least ` (  number_of_hyperparameters  * 10) `
trials to tune a model.

###  ` MAX_PARALLEL_TRIALS `

**Syntax**

` MAX_PARALLEL_TRIALS =  int64_value  `

**Description**

The maximum number of trials to run at the same time. If you specify a value
for this option, you must also specify a value for  ` NUM_TRIALS ` .

**Arguments**

An ` INT64 ` value between ` 1 ` and ` 5 ` , inclusive. The default value is `
1 ` .

**Note:** Although specifying larger ` MAX_PARALLEL_TRIALS ` values can
accelerate the hyperparameter tuning process, acceleration can undermine the
final model quality when you specify ` VIZIER_DEFAULT ` as the  `
HPARAM_TUNING_ALGORITHM ` value. This is because the parallel trials can't
benefit from concurrent training results.

###  ` HPARAM_TUNING_ALGORITHM `

**Syntax**



HPARAM_TUNING_ALGORITHM = { 'VIZIER_DEFAULT' | 'RANDOM_SEARCH' | 'GRID_SEARCH' }


**Description**

The algorithm used to tune the hyperparameters. If you specify a value for
this option, you must also specify a value for  ` NUM_TRIALS ` .

**Arguments**

Specify one of the following values:

* ` VIZIER_DEFAULT ` : Use the default algorithm in Vertex AI Vizier to tune hyperparameters. This algorithm is the most powerful algorithm of those offered. It performs a mixture of advanced search algorithms, including [ Bayesian optimization ](https://en.wikipedia.org/wiki/Bayesian_optimization) with [ Gaussian processes ](https://en.wikipedia.org/wiki/Gaussian_process) . It also uses [ transfer learning ](/bigquery/docs/reference/standard-sql/bigqueryml-hyperparameter-tuning#transfer_learning) to take advantage of previously tuned models. This is the default, and also the recommended approach.

* ` RANDOM_SEARCH ` : Use [ random search ](https://en.wikipedia.org/wiki/Hyperparameter_optimization#Random_search) to explore the search space.

* ` GRID_SEARCH ` : Use [ grid search ](https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search) to explore the search space. You can only use this algorithm when every hyperparameter's search space is discrete.

###  ` HPARAM_TUNING_OBJECTIVES `

**Syntax**

For ` BOOSTED_TREE_CLASSIFIER ` models:



HPARAM_TUNING_OBJECTIVES = { 'PRECISION' | 'RECALL' | 'ACCURACY' | 'F1_SCORE' | 'LOG_LOSS' | 'ROC_AUC' }


For ` BOOSTED_TREE_REGRESSOR ` models:



HPARAM_TUNING_OBJECTIVES = { 'MEAN_ABSOLUTE_ERROR' | 'MEAN_SQUARED_ERROR' | 'MEAN_SQUARED_LOG_ERROR' | 'MEDIAN_ABSOLUTE_ERROR' | 'R2_SCORE' | 'EXPLAINED_VARIANCE' }


**Description**

The hyperparameter tuning objective for the model; only one objective is
supported. If you specify a value for this option, you must also specify a
value for  ` NUM_TRIALS ` .

**Arguments**

The possible objectives are a subset of the [ model evaluation metrics
](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-
evaluate#mlevaluate_output) for the model type. If you aren't running
hyperparameter tuning, or if you are and you don't specify an objective, the
default objective is used. For ` BOOSTED_TREE_CLASSIFIER ` models, the default
is ` ROC_AUC ` . For ` BOOSTED_TREE_REGRESSOR ` models, the default is `
R2_SCORE ` .

###  Internal parameter defaults

BigQuery ML uses the following default values when building models:



loss_reduction = losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE

batch_norm = False


##  Example

The following example trains a boosted tree classifier model against `
'mytable' ` with ` 'mylabel' ` as the label column.



CREATE MODEL `project_id.mydataset.mymodel`
OPTIONS(MODEL_TYPE='BOOSTED_TREE_CLASSIFIER',
BOOSTER_TYPE = 'GBTREE',
NUM_PARALLEL_TREE = 1,
MAX_ITERATIONS = 50,
TREE_METHOD = 'HIST',
EARLY_STOP = FALSE,
SUBSAMPLE = 0.85,
INPUT_LABEL_COLS = ['mylabel'])
AS SELECT * FROM `project_id.mydataset.mytable`;


##  Supported regions

Training boosted tree models is not supported in all BigQuery ML regions. For
a complete list of supported regions and multi-regions, see [ BigQuery ML
locations ](/bigquery/docs/locations#bqml-loc) .

Send feedback

Except as otherwise noted, the content of this page is licensed under the [
Creative Commons Attribution 4.0 License
](https://creativecommons.org/licenses/by/4.0/) , and code samples are
licensed under the [ Apache 2.0 License
](https://www.apache.org/licenses/LICENSE-2.0) . For details, see the [ Google
Developers Site Policies ](https://developers.google.com/site-policies) . Java
is a registered trademark of Oracle and/or its affiliates.

Last updated 2024-04-29 UTC.

[{ "type": "thumb-down", "id": "hardToUnderstand", "label":"Hard to
understand" },{ "type": "thumb-down", "id":
"incorrectInformationOrSampleCode", "label":"Incorrect information or sample
code" },{ "type": "thumb-down", "id": "missingTheInformationSamplesINeed",
"label":"Missing the information/samples I need" },{ "type": "thumb-down",
"id": "otherDown", "label":"Other" }]  [{ "type": "thumb-up", "id":
"easyToUnderstand", "label":"Easy to understand" },{ "type": "thumb-up", "id":
"solvedMyProblem", "label":"Solved my problem" },{ "type": "thumb-up", "id":
"otherUp", "label":"Other" }]  Need to tell us more?

* ###  Why Google

* [ Choosing Google Cloud ](/why-google-cloud/)
* [ Trust and security ](/trust-center/)
* [ Open cloud ](/open-cloud/)
* [ Multicloud ](/multicloud/)
* [ Global infrastructure ](/infrastructure/)
* [ Customers and case studies ](/customers/)
* [ Analyst reports ](/analyst-reports/)
* [ Whitepapers ](/whitepapers/)
* [ Blog ](//cloud.google.com/blog/)
* ###  Products and pricing

* [ Google Cloud pricing ](/pricing/)
* [ Google Workspace pricing ](//workspace.google.com/pricing.html)
* [ See all products ](/products/)
* ###  Solutions

* [ Infrastructure modernization ](/solutions/infrastructure-modernization/)
* [ Databases ](/solutions/databases/)
* [ Application modernization ](/solutions/application-modernization/)
* [ Smart analytics ](/solutions/smart-analytics/)
* [ Artificial Intelligence ](/solutions/ai/)
* [ Security ](/solutions/security/)
* [ Productivity & work transformation ](https://workspace.google.com/enterprise/)
* [ Industry solutions ](/solutions/#industry-solutions)
* [ DevOps solutions ](/solutions/devops/)
* [ Small business solutions ](/solutions/#section-14)
* [ See all solutions ](/solutions/)
* ###  Resources

* [ Google Cloud documentation ](/docs/)
* [ Google Cloud quickstarts ](/docs/get-started/)
* [ Google Cloud Marketplace ](/marketplace/)
* [ Learn about cloud computing ](/discover/)
* [ Support ](/support-hub/)
* [ Code samples ](/docs/samples)
* [ Cloud Architecture Center ](/architecture/)
* [ Training ](/learn/training/)
* [ Certifications ](/learn/certification/)
* [ Google for Developers ](//developers.google.com)
* [ Google Cloud for Startups ](/startup/)
* [ System status ](//status.cloud.google.com)
* [ Release Notes ](/release-notes)
* ###  Engage

* [ Contact sales ](/contact/)
* [ Find a Partner ](//cloud.google.com/find-a-partner)
* [ Become a Partner ](/partners/become-a-partner/)
* [ Events ](/events/)
* [ Podcasts ](/podcasts/)
* [ Developer Center ](/developers/)
* [ Press Corner ](https://www.googlecloudpresscorner.com/)
* [ Google Cloud on YouTube ](//www.youtube.com/googlecloud)
* [ Google Cloud Tech on YouTube ](//www.youtube.com/googlecloudplatform)
* [ Follow on X ](//x.com/googlecloud)
* [ Join User Research ](//userresearch.google.com/?reserved=1&utm_source=website&Q_Language=en&utm_medium=own_srch&utm_campaign=CloudWebFooter&utm_term=0&utm_content=0&productTag=clou&campaignDate=jul19&pType=devel&referral_code=jk212693)
* [ We're hiring. Join Google Cloud! ](//careers.google.com/cloud)
* [ Google Cloud Community ](https://www.googlecloudcommunity.com/)

* [ About Google ](//about.google/)
* [ Privacy ](//policies.google.com/privacy)
* [ Site terms ](//www.google.com/intl/en/policies/terms/regional.html)
* [ Google Cloud terms ](/product-terms/)
* Manage cookies
* [ Our third decade of climate action: join us ](/sustainability)
* Sign up for the Google Cloud newsletter  [ Subscribe ](/newsletter/)

* English
* Deutsch
* Español – América Latina
* Français
* Português – Brasil
* 中文 – 简体
* 日本語
* 한국어

