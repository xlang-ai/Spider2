[ ![Google Cloud](https://www.gstatic.com/devrel-
devsite/prod/vc851b65627ca98cc752c9ae13e5f506cd6dbb7ed1bb4c8df6090c5f9130ed83c/cloud/images/cloud-
logo.svg) ](/)

*

[ Documentation ](https://cloud.google.com/docs) [ Technology areas
](https://cloud.google.com/docs/tech-area-overviews)

close

* [ AI solutions, generative AI, and ML  ](https://cloud.google.com/docs/ai-ml)
* [ Application development  ](https://cloud.google.com/docs/application-development)
* [ Application hosting  ](https://cloud.google.com/docs/application-hosting)
* [ Compute  ](https://cloud.google.com/docs/compute-area)
* [ Data analytics and pipelines  ](https://cloud.google.com/docs/data)
* [ Databases  ](https://cloud.google.com/docs/databases)
* [ Distributed, hybrid, and multicloud  ](https://cloud.google.com/docs/dhm-cloud)
* [ Industry solutions  ](https://cloud.google.com/docs/industry)
* [ Networking  ](https://cloud.google.com/docs/networking)
* [ Observability and monitoring  ](https://cloud.google.com/docs/observability)
* [ Security  ](https://cloud.google.com/docs/security)
* [ Storage  ](https://cloud.google.com/docs/storage)

[ Cross-product tools ](https://cloud.google.com/docs/cross-product-overviews)

close

* [ Access and resources management  ](https://cloud.google.com/docs/access-resources)
* [ Cloud SDK, languages, frameworks, and tools  ](https://cloud.google.com/docs/devtools)
* [ Costs and usage management  ](https://cloud.google.com/docs/costs-usage)
* [ Infrastructure as code  ](https://cloud.google.com/docs/iac)
* [ Migration  ](https://cloud.google.com/docs/migration)

[ Related sites ](https://cloud.google.com/)

close

* [ Google Cloud Home  ](https://cloud.google.com/)
* [ Free Trial and Free Tier  ](https://cloud.google.com/free)
* [ Architecture Center  ](https://cloud.google.com/architecture)
* [ Blog  ](https://cloud.google.com/blog)
* [ Contact Sales  ](https://cloud.google.com/contact)
* [ Google Cloud Developer Center  ](https://cloud.google.com/developers)
* [ Google Developer Center  ](https://developers.google.com/)
* [ Google Cloud Marketplace (in console)  ](https://console.cloud.google.com/marketplace)
* [ Google Cloud Marketplace Documentation  ](https://cloud.google.com/marketplace/docs)
* [ Google Cloud Skills Boost  ](https://www.cloudskillsboost.google/paths)
* [ Google Cloud Solution Center  ](https://cloud.google.com/solutions)
* [ Google Cloud Support  ](https://cloud.google.com/support-hub)
* [ Google Cloud Tech Youtube Channel  ](https://www.youtube.com/@googlecloudtech)

* English
* Deutsch
* Español – América Latina
* Français
* Português – Brasil
* 中文 – 简体
* 日本語
* 한국어

Sign in

* [ BigQuery ](https://cloud.google.com/bigquery)

[ Guides ](https://cloud.google.com/bigquery/docs/introduction) [ Reference
](https://cloud.google.com/bigquery/quotas) [ Samples
](https://cloud.google.com/bigquery/docs/samples) [ Resources
](https://cloud.google.com/bigquery/docs/release-notes)

[ Contact Us ](https://cloud.google.com/contact) [ Start free
](//console.cloud.google.com/freetrial)

[ ![Google Cloud](https://www.gstatic.com/devrel-
devsite/prod/vc851b65627ca98cc752c9ae13e5f506cd6dbb7ed1bb4c8df6090c5f9130ed83c/cloud/images/cloud-
logo.svg) ](/)

*

* [ Documentation  ](/docs)
* [ Guides  ](/bigquery/docs/introduction)
* [ Reference  ](/bigquery/quotas)
* [ Samples  ](/bigquery/docs/samples)
* [ Resources  ](/bigquery/docs/release-notes)
* [ Technology areas  ](/docs/tech-area-overviews)
* More
* [ Cross-product tools  ](/docs/cross-product-overviews)
* More
* [ Related sites  ](/)
* More
* [ Console  ](//console.cloud.google.com/)
* [ Contact Us  ](/contact)
* [ Start free  ](//console.cloud.google.com/freetrial)

* Quotas and limits

* [ Quotas and limits reference  ](/bigquery/quotas)
* [ Troubleshoot quota errors  ](/bigquery/docs/troubleshoot-quotas)
* BigQuery command-line tool

* [ bq command-line tool reference  ](/bigquery/docs/reference/bq-cli-reference)
* SQL in BigQuery

* GoogleSQL reference

* [ Query syntax  ](/bigquery/docs/reference/standard-sql/query-syntax)
* General reference

* [ Data types  ](/bigquery/docs/reference/standard-sql/data-types)
* [ Lexical structure and syntax  ](/bigquery/docs/reference/standard-sql/lexical)
* [ Conversion rules  ](/bigquery/docs/reference/standard-sql/conversion_rules)
* [ Format elements  ](/bigquery/docs/reference/standard-sql/format-elements)
* [ Collation  ](/bigquery/docs/reference/standard-sql/collation-concepts)
* [ Text analysis  ](/bigquery/docs/reference/standard-sql/text-analysis)
* [ BI Engine optimized functions  ](/bigquery/docs/bi-engine-optimized-sql)

* Expressions

* [ Function calls  ](/bigquery/docs/reference/standard-sql/functions-reference)
* [ Aggregate function calls  ](/bigquery/docs/reference/standard-sql/aggregate-function-calls)
* [ Window function calls  ](/bigquery/docs/reference/standard-sql/window-function-calls)
* [ Operators  ](/bigquery/docs/reference/standard-sql/operators)
* [ Conditional expressions  ](/bigquery/docs/reference/standard-sql/conditional_expressions)
* [ Subqueries  ](/bigquery/docs/reference/standard-sql/subqueries)

* Functions

* [ All functions and operators  ](/bigquery/docs/reference/standard-sql/functions-and-operators)
* [ AEAD encryption functions  ](/bigquery/docs/reference/standard-sql/aead_encryption_functions)
* [ Aggregate functions  ](/bigquery/docs/reference/standard-sql/aggregate_functions)
* [ Approximate aggregate functions  ](/bigquery/docs/reference/standard-sql/approximate_aggregate_functions)
* [ Array functions  ](/bigquery/docs/reference/standard-sql/array_functions)
* [ Bit functions  ](/bigquery/docs/reference/standard-sql/bit_functions)
* [ Conversion functions  ](/bigquery/docs/reference/standard-sql/conversion_functions)
* [ Date functions  ](/bigquery/docs/reference/standard-sql/date_functions)
* [ Datetime functions  ](/bigquery/docs/reference/standard-sql/datetime_functions)
* [ Debugging functions  ](/bigquery/docs/reference/standard-sql/debugging_functions)
* [ Differentially private aggregate functions  ](/bigquery/docs/reference/standard-sql/aggregate-dp-functions)
* [ Federated query functions  ](/bigquery/docs/reference/standard-sql/federated_query_functions)
* [ DLP encryption functions  ](/bigquery/docs/reference/standard-sql/dlp_functions)
* [ Geography functions  ](/bigquery/docs/reference/standard-sql/geography_functions)
* [ Hash functions  ](/bigquery/docs/reference/standard-sql/hash_functions)
* [ HyperLogLog++ functions  ](/bigquery/docs/reference/standard-sql/hll_functions)
* [ Interval functions  ](/bigquery/docs/reference/standard-sql/interval_functions)
* [ JSON functions  ](/bigquery/docs/reference/standard-sql/json_functions)
* [ Mathematical functions  ](/bigquery/docs/reference/standard-sql/mathematical_functions)
* [ Navigation functions  ](/bigquery/docs/reference/standard-sql/navigation_functions)
* [ Net functions  ](/bigquery/docs/reference/standard-sql/net_functions)
* [ Numbering functions  ](/bigquery/docs/reference/standard-sql/numbering_functions)
* [ Range functions  ](/bigquery/docs/reference/standard-sql/range-functions)
* [ Search functions  ](/bigquery/docs/reference/standard-sql/search_functions)
* [ Security functions  ](/bigquery/docs/reference/standard-sql/security_functions)
* [ Statistical aggregate functions  ](/bigquery/docs/reference/standard-sql/statistical_aggregate_functions)
* [ String functions  ](/bigquery/docs/reference/standard-sql/string_functions)
* [ Table functions (built-in)  ](/bigquery/docs/reference/standard-sql/table-functions-built-in)
* [ Text analysis functions  ](/bigquery/docs/reference/standard-sql/text-analysis-functions)
* [ Time functions  ](/bigquery/docs/reference/standard-sql/time_functions)
* [ Time series functions  ](/bigquery/docs/reference/standard-sql/time-series-functions)
* [ Timestamp functions  ](/bigquery/docs/reference/standard-sql/timestamp_functions)
* [ Utility functions  ](/bigquery/docs/reference/standard-sql/utility-functions)

* Statements

* [ Data definition language (DDL)  ](/bigquery/docs/reference/standard-sql/data-definition-language)
* [ Data manipulation language (DML)  ](/bigquery/docs/reference/standard-sql/dml-syntax)
* [ Data control language (DCL)  ](/bigquery/docs/reference/standard-sql/data-control-language)
* [ Procedural language  ](/bigquery/docs/reference/standard-sql/procedural-language)
* [ Export and load statements  ](/bigquery/docs/reference/standard-sql/other-statements)
* [ Debugging statements  ](/bigquery/docs/reference/standard-sql/debugging-statements)

* BigQuery ML SQL reference

* Creating and training models

* [ CREATE MODEL statement overview  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create)
* Regression and classification

* [ Linear and logistic regression  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-glm)
* [ Boosted trees  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-boosted-tree)
* [ Random forest  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-random-forest)
* [ Deep neural networks  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-dnn-models)
* [ Wide & Deep networks  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-wnd-models)
* [ AutoML models  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-automl)

* Clustering

* [ K-means  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-kmeans)

* Dimensionality reduction

* [ Principal component analysis  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-pca)
* [ Autoencoder  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-autoencoder)

* Collaborative filtering

* [ Matrix factorization  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-matrix-factorization)

* Time series forecasting

* [ Univariate forecasting with ARIMA_PLUS models  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-time-series)
* [ Multivariate forecasting with ARIMA_PLUS_XREG models  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-multivariate-time-series)

* Importing models

* [ Open Neural Network Exchange (ONNX)  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-onnx)
* [ TensorFlow  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-tensorflow)
* [ TensorFlow Lite  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-tflite)
* [ XGBoost  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-xgboost)

* Remote models

* [ LLMs  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-remote-model)
* [ Cloud AI services  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-remote-model-service)
* [ Vertex AI hosted models  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-remote-model-https)

* Feature engineering

* [ Feature transformation  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-transform)
* [ ML.TRANSFORM  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-transform)
* [ ML.FEATURE_INFO  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-feature)
* General functions

* [ ML.IMPUTER  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-imputer)

* Numerical functions

* [ ML.BUCKETIZE  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-bucketize)
* [ ML.MAX_ABS_SCALER  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-max-abs-scaler)
* [ ML.MIN_MAX_SCALER  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-min-max-scaler)
* [ ML.NORMALIZER  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-normalizer)
* [ ML.POLYNOMIAL_EXPAND  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-polynomial-expand)
* [ ML.QUANTILE_BUCKETIZE  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-quantile-bucketize)
* [ ML.ROBUST_SCALER  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-robust-scaler)
* [ ML.STANDARD_SCALER  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-standard-scaler)

* Categorical functions

* [ ML.FEATURE_CROSS  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-feature-cross)
* [ ML.HASH_BUCKETIZE  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-hash-bucketize)
* [ ML.LABEL_ENCODER  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-label-encoder)
* [ ML.MULTI_HOT_ENCODER  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-multi-hot-encoder)
* [ ML.ONE_HOT_ENCODER  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-one-hot-encoder)

* Text functions

* [ ML.NGRAMS  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-ngrams)
* [ ML.BAG_OF_WORDS  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-bag-of-words)
* [ ML.TF_IDF  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-tf-idf)

* Image functions

* [ ML.CONVERT_COLOR_SPACE  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-convert-color-space)
* [ ML.CONVERT_IMAGE_TYPE  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-convert-image-type)
* [ ML.DECODE_IMAGE  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-decode-image)
* [ ML.RESIZE_IMAGE  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-resize-image)

* Point-in-time lookup functions

* [ ML.FEATURES_AT_TIME  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-feature-time)
* [ ML.ENTITY_FEATURES_AT_TIME  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-entity-feature-time)

* Hyperparameter tuning functions

* [ ML.TRIAL_INFO  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-trial-info)

* Evaluation functions

* [ ML.EVALUATE  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-evaluate)
* [ ML.ROC_CURVE  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-roc)
* [ ML.CONFUSION_MATRIX  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-confusion)
* [ ML.ARIMA_EVALUATE  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-arima-evaluate)
* [ ML.TRAINING_INFO  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-train)
* [ ML.RECONSTRUCTION_LOSS  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-reconstruction-loss)
* [ ML.HOLIDAY_INFO  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-holiday-info)

* Inference functions

* [ ML.PREDICT  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-predict)
* [ ML.FORECAST  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-forecast)
* [ ML.RECOMMEND  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-recommend)
* [ ML.DETECT_ANOMALIES  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-detect-anomalies)

* Generative AI functions

* [ ML.GENERATE_TEXT  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-generate-text)
* [ ML.GENERATE_EMBEDDING  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-generate-embedding)

* AI functions

* [ ML.UNDERSTAND_TEXT  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-understand-text)
* [ ML.TRANSLATE  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-translate)
* [ ML.PROCESS_DOCUMENT  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-process-document)
* [ ML.TRANSCRIBE  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-transcribe)
* [ ML.ANNOTATE_IMAGE  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-annotate-image)

* AI Explanation functions

* [ ML.EXPLAIN_PREDICT  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-explain-predict)
* [ ML.EXPLAIN_FORECAST  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-explain-forecast)
* [ ML.GLOBAL_EXPLAIN  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-global-explain)
* [ ML.FEATURE_IMPORTANCE  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-importance)
* [ ML.ADVANCED_WEIGHTS  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-advanced-weights)

* Model weights functions

* [ ML.WEIGHTS  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-weights)
* [ ML.CENTROIDS  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-centroids)
* [ ML.PRINCIPAL_COMPONENTS  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-principal-components)
* [ ML.PRINCIPAL_COMPONENT_INFO  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-principal-component-info)
* [ ML.ARIMA_COEFFICIENTS  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-arima-coefficients)

* Model monitoring functions

* [ ML.DESCRIBE_DATA  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-describe-data)
* [ ML.VALIDATE_DATA_DRIFT  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-validate-data-drift)
* [ ML.VALIDATE_DATA_SKEW  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-validate-data-skew)
* [ ML.TFDV_DESCRIBE  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-tfdv-describe)
* [ ML.TFDV_VALIDATE  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-tfdv-validate)

* Math utility functions

* [ ML.DISTANCE  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-distance)
* [ ML.LP_NORM  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-lp-norm)

* Model management statements

* [ EXPORT MODEL statement  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-export-model)
* [ ALTER MODEL statement  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-alter-model)
* [ DROP MODEL statement  ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-drop-model)

* INFORMATION SCHEMA views

* [ Introduction  ](/bigquery/docs/information-schema-intro)
* Access control

* [ OBJECT_PRIVILEGES view  ](/bigquery/docs/information-schema-object-privileges)

* BI Engine

* [ BI_CAPACITIES  ](/bigquery/docs/information-schema-bi-capacities)
* [ BI_CAPACITY_CHANGES  ](/bigquery/docs/information-schema-bi-capacity-changes)

* Configurations

* [ EFFECTIVE_PROJECT_OPTIONS view  ](/bigquery/docs/information-schema-effective-project-options)
* [ ORGANIZATION_OPTIONS view  ](/bigquery/docs/information-schema-organization-options)
* [ ORGANIZATION_OPTIONS_CHANGES view  ](/bigquery/docs/information-schema-organization-options-changes)
* [ PROJECT_OPTIONS view  ](/bigquery/docs/information-schema-project-options)
* [ PROJECT_OPTIONS_CHANGES view  ](/bigquery/docs/information-schema-project-options-changes)

* Datasets

* [ SCHEMATA view  ](/bigquery/docs/information-schema-datasets-schemata)
* [ SCHEMATA_LINKS view  ](/bigquery/docs/information-schema-datasets-schemata-links)
* [ SCHEMATA_OPTIONS view  ](/bigquery/docs/information-schema-datasets-schemata-options)
* [ SHARED_DATASET_USAGE view  ](/bigquery/docs/information-schema-shared-dataset-usage)
* [ SCHEMATA_REPLICAS view  ](/bigquery/docs/information-schema-schemata-replicas)

* Jobs

* [ JOBS view  ](/bigquery/docs/information-schema-jobs)
* [ JOBS_BY_USER view  ](/bigquery/docs/information-schema-jobs-by-user)
* [ JOBS_BY_FOLDER view  ](/bigquery/docs/information-schema-jobs-by-folder)
* [ JOBS_BY_ORGANIZATION view  ](/bigquery/docs/information-schema-jobs-by-organization)

* Jobs by timeslice

* [ JOBS_TIMELINE view  ](/bigquery/docs/information-schema-jobs-timeline)
* [ JOBS_TIMELINE_BY_USER view  ](/bigquery/docs/information-schema-jobs-timeline-by-user)
* [ JOBS_TIMELINE_BY_FOLDER view  ](/bigquery/docs/information-schema-jobs-timeline-by-folder)
* [ JOBS_TIMELINE_BY_ORGANIZATION view  ](/bigquery/docs/information-schema-jobs-timeline-by-organization)

* Reservations

* [ ASSIGNMENTS view  ](/bigquery/docs/information-schema-assignments)
* [ ASSIGNMENT_CHANGES view  ](/bigquery/docs/information-schema-assignments-changes)
* [ CAPACITY_COMMITMENTS view  ](/bigquery/docs/information-schema-capacity-commitments)
* [ CAPACITY_COMMITMENT_CHANGES view  ](/bigquery/docs/information-schema-capacity-commitment-changes)
* [ RESERVATIONS view  ](/bigquery/docs/information-schema-reservations)
* [ RESERVATION_CHANGES view  ](/bigquery/docs/information-schema-reservation-changes)
* [ RESERVATIONS_TIMELINE view  ](/bigquery/docs/information-schema-reservation-timeline)

* Routines

* [ PARAMETERS view  ](/bigquery/docs/information-schema-parameters)
* [ ROUTINES view  ](/bigquery/docs/information-schema-routines)
* [ ROUTINE_OPTIONS view  ](/bigquery/docs/information-schema-routine-options)

* Search indexes

* [ SEARCH_INDEXES view  ](/bigquery/docs/information-schema-indexes)
* [ SEARCH_INDEX_COLUMNS view  ](/bigquery/docs/information-schema-index-columns)

* Sessions

* [ SESSIONS_BY_PROJECT view  ](/bigquery/docs/information-schema-sessions-by-project)
* [ SESSIONS_BY_USER view  ](/bigquery/docs/information-schema-sessions-by-user)

* Streaming

* [ STREAMING_TIMELINE view  ](/bigquery/docs/information-schema-streaming)
* [ STREAMING_TIMELINE_BY_FOLDER view  ](/bigquery/docs/information-schema-streaming-by-folder)
* [ STREAMING_TIMELINE_BY_ORGANIZATION view  ](/bigquery/docs/information-schema-streaming-by-organization)

* Tables

* [ COLUMNS view  ](/bigquery/docs/information-schema-columns)
* [ COLUMN_FIELD_PATHS view  ](/bigquery/docs/information-schema-column-field-paths)
* [ CONSTRAINT_COLUMN_USAGE view  ](/bigquery/docs/information-schema-constraint-column-usage)
* [ KEY_COLUMN_USAGE view  ](/bigquery/docs/information-schema-key-column-usage)
* [ PARTITIONS view  ](/bigquery/docs/information-schema-partitions)
* [ TABLES view  ](/bigquery/docs/information-schema-tables)
* [ TABLE_OPTIONS view  ](/bigquery/docs/information-schema-table-options)
* [ TABLE_CONSTRAINTS view  ](/bigquery/docs/information-schema-table-constraints)
* [ TABLE_SNAPSHOTS view  ](/bigquery/docs/information-schema-snapshots)
* [ TABLE_STORAGE view  ](/bigquery/docs/information-schema-table-storage)
* [ TABLE_STORAGE_BY_ORGANIZATION view  ](/bigquery/docs/information-schema-table-storage-by-organization)
* [ TABLE_STORAGE_USAGE_TIMELINE view  ](/bigquery/docs/information-schema-table-storage-usage)
* [ TABLE_STORAGE_USAGE_TIMELINE_BY_ORGANIZATION view  ](/bigquery/docs/information-schema-table-storage-usage-by-organization)

* Vector indexes

* [ VECTOR_INDEXES view  ](/bigquery/docs/information-schema-vector-indexes)
* [ VECTOR_INDEX_COLUMNS view  ](/bigquery/docs/information-schema-vector-index-columns)
* [ VECTOR_INDEX_OPTIONS view  ](/bigquery/docs/information-schema-vector-index-options)

* Views

* [ VIEWS view  ](/bigquery/docs/information-schema-views)
* [ MATERIALIZED_VIEWS view  ](/bigquery/docs/information-schema-materialized-views)

* Write API

* [ WRITE_API_TIMELINE view  ](/bigquery/docs/information-schema-write-api)
* [ WRITE_API_TIMELINE_BY_FOLDER view  ](/bigquery/docs/information-schema-write-api-by-folder)
* [ WRITE_API_TIMELINE_BY_ORGANIZATION view  ](/bigquery/docs/information-schema-write-api-by-organization)

* Legacy SQL reference

* [ Migrating to GoogleSQL  ](/bigquery/docs/reference/standard-sql/migrating-from-legacy-sql)
* [ Functions and operators  ](/bigquery/docs/reference/legacy-sql)
* [ Data types  ](/bigquery/docs/data-types)
* [ Querying nested and repeated fields  ](/bigquery/docs/legacy-nested-repeated)
* [ User-defined functions  ](/bigquery/docs/user-defined-functions-legacy)
* [ Table decorators  ](/bigquery/docs/table-decorators)

* BigQuery DataFrames Python API

* [ BigQuery DataFrames  ](/bigquery/docs/reference/bigquery-dataframes)
* BigQuery APIs

* BigQuery API reference

* [ BigQuery APIs and libraries overview  ](/bigquery/docs/reference/libraries-overview)
* BigQuery API reference

* [ BigQuery client libraries  ](/bigquery/docs/reference/libraries)
* [ BigQuery REST API  ](/bigquery/docs/reference/rest)
* REST reference (v2)

* REST Resources

* datasets

* [ Overview  ](/bigquery/docs/reference/rest/v2/datasets)
* [ delete  ](/bigquery/docs/reference/rest/v2/datasets/delete)
* [ get  ](/bigquery/docs/reference/rest/v2/datasets/get)
* [ insert  ](/bigquery/docs/reference/rest/v2/datasets/insert)
* [ list  ](/bigquery/docs/reference/rest/v2/datasets/list)
* [ patch  ](/bigquery/docs/reference/rest/v2/datasets/patch)
* [ undelete  ](/bigquery/docs/reference/rest/v2/datasets/undelete)
* [ update  ](/bigquery/docs/reference/rest/v2/datasets/update)

* jobs

* [ Overview  ](/bigquery/docs/reference/rest/v2/jobs)
* [ cancel  ](/bigquery/docs/reference/rest/v2/jobs/cancel)
* [ delete  ](/bigquery/docs/reference/rest/v2/jobs/delete)
* [ get  ](/bigquery/docs/reference/rest/v2/jobs/get)
* [ getQueryResults  ](/bigquery/docs/reference/rest/v2/jobs/getQueryResults)
* [ insert  ](/bigquery/docs/reference/rest/v2/jobs/insert)
* [ list  ](/bigquery/docs/reference/rest/v2/jobs/list)
* [ query  ](/bigquery/docs/reference/rest/v2/jobs/query)

* models

* [ Overview  ](/bigquery/docs/reference/rest/v2/models)
* [ delete  ](/bigquery/docs/reference/rest/v2/models/delete)
* [ get  ](/bigquery/docs/reference/rest/v2/models/get)
* [ list  ](/bigquery/docs/reference/rest/v2/models/list)
* [ patch  ](/bigquery/docs/reference/rest/v2/models/patch)

* projects

* [ Overview  ](/bigquery/docs/reference/rest/v2/projects)
* [ getServiceAccount  ](/bigquery/docs/reference/rest/v2/projects/getServiceAccount)
* [ list  ](/bigquery/docs/reference/rest/v2/projects/list)

* routines

* [ Overview  ](/bigquery/docs/reference/rest/v2/routines)
* [ delete  ](/bigquery/docs/reference/rest/v2/routines/delete)
* [ get  ](/bigquery/docs/reference/rest/v2/routines/get)
* [ insert  ](/bigquery/docs/reference/rest/v2/routines/insert)
* [ list  ](/bigquery/docs/reference/rest/v2/routines/list)
* [ update  ](/bigquery/docs/reference/rest/v2/routines/update)

* rowAccessPolicies

* [ Overview  ](/bigquery/docs/reference/rest/v2/rowAccessPolicies)
* [ getIamPolicy  ](/bigquery/docs/reference/rest/v2/rowAccessPolicies/getIamPolicy)
* [ list  ](/bigquery/docs/reference/rest/v2/rowAccessPolicies/list)
* [ testIamPermissions  ](/bigquery/docs/reference/rest/v2/rowAccessPolicies/testIamPermissions)

* tabledata

* [ Overview  ](/bigquery/docs/reference/rest/v2/tabledata)
* [ insertAll  ](/bigquery/docs/reference/rest/v2/tabledata/insertAll)
* [ list  ](/bigquery/docs/reference/rest/v2/tabledata/list)

* tables

* [ Overview  ](/bigquery/docs/reference/rest/v2/tables)
* [ delete  ](/bigquery/docs/reference/rest/v2/tables/delete)
* [ get  ](/bigquery/docs/reference/rest/v2/tables/get)
* [ getIamPolicy  ](/bigquery/docs/reference/rest/v2/tables/getIamPolicy)
* [ insert  ](/bigquery/docs/reference/rest/v2/tables/insert)
* [ list  ](/bigquery/docs/reference/rest/v2/tables/list)
* [ patch  ](/bigquery/docs/reference/rest/v2/tables/patch)
* [ setIamPolicy  ](/bigquery/docs/reference/rest/v2/tables/setIamPolicy)
* [ testIamPermissions  ](/bigquery/docs/reference/rest/v2/tables/testIamPermissions)
* [ update  ](/bigquery/docs/reference/rest/v2/tables/update)

* Types

* [ ConnectionProperty  ](/bigquery/docs/reference/rest/v2/ConnectionProperty)
* [ DataFormatOptions  ](/bigquery/docs/reference/rest/v2/DataFormatOptions)
* [ DatasetAccessEntry  ](/bigquery/docs/reference/rest/v2/DatasetAccessEntry)
* [ DmlStats  ](/bigquery/docs/reference/rest/v2/DmlStats)
* [ EncryptionConfiguration  ](/bigquery/docs/reference/rest/v2/EncryptionConfiguration)
* [ GetPolicyOptions  ](/bigquery/docs/reference/rest/v2/GetPolicyOptions)
* [ Job  ](/bigquery/docs/reference/rest/v2/Job)
* [ JobReference  ](/bigquery/docs/reference/rest/v2/JobReference)
* [ Policy  ](/bigquery/docs/reference/rest/v2/Policy)
* [ ProjectReference  ](/bigquery/docs/reference/rest/v2/ProjectReference)
* [ QueryParameter  ](/bigquery/docs/reference/rest/v2/QueryParameter)
* [ RoundingMode  ](/bigquery/docs/reference/rest/v2/RoundingMode)
* [ RowAccessPolicyReference  ](/bigquery/docs/reference/rest/v2/RowAccessPolicyReference)
* [ SessionInfo  ](/bigquery/docs/reference/rest/v2/SessionInfo)
* [ StandardSqlDataType  ](/bigquery/docs/reference/rest/v2/StandardSqlDataType)
* [ StandardSqlField  ](/bigquery/docs/reference/rest/v2/StandardSqlField)
* [ TableReference  ](/bigquery/docs/reference/rest/v2/TableReference)
* [ TargetType  ](/bigquery/docs/reference/rest/v2/TargetType)
* [ TestIamPermissionsResponse  ](/bigquery/docs/reference/rest/v2/TestIamPermissionsResponse)

* [ API uploads  ](/bigquery/docs/reference/api-uploads)

* BigQuery Data Policy API reference

* [ Data Policy REST reference  ](/bigquery/docs/reference/bigquerydatapolicy/rest)
* v1

* REST Resources

* projects.locations.dataPolicies

* [ Overview  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1/projects.locations.dataPolicies)
* [ create  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1/projects.locations.dataPolicies/create)
* [ delete  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1/projects.locations.dataPolicies/delete)
* [ get  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1/projects.locations.dataPolicies/get)
* [ getIamPolicy  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1/projects.locations.dataPolicies/getIamPolicy)
* [ list  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1/projects.locations.dataPolicies/list)
* [ patch  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1/projects.locations.dataPolicies/patch)
* [ rename  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1/projects.locations.dataPolicies/rename)
* [ setIamPolicy  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1/projects.locations.dataPolicies/setIamPolicy)
* [ testIamPermissions  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1/projects.locations.dataPolicies/testIamPermissions)

* v1beta1

* REST Resources

* projects.locations.dataPolicies

* [ Overview  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1beta1/projects.locations.dataPolicies)
* [ create  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1beta1/projects.locations.dataPolicies/create)
* [ delete  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1beta1/projects.locations.dataPolicies/delete)
* [ get  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1beta1/projects.locations.dataPolicies/get)
* [ getIamPolicy  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1beta1/projects.locations.dataPolicies/getIamPolicy)
* [ list  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1beta1/projects.locations.dataPolicies/list)
* [ patch  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1beta1/projects.locations.dataPolicies/patch)
* [ setIamPolicy  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1beta1/projects.locations.dataPolicies/setIamPolicy)
* [ testIamPermissions  ](/bigquery/docs/reference/bigquerydatapolicy/rest/v1beta1/projects.locations.dataPolicies/testIamPermissions)

* BigQuery Connections API reference

* [ BigQuery Connection client libraries  ](/bigquery/docs/reference/bigqueryconnection)
* [ BigQuery Connection REST API  ](/bigquery/docs/reference/bigqueryconnection/rest)
* RPC reference

* [ Overview  ](/bigquery/docs/reference/bigqueryconnection/rpc)
* [ google.cloud.bigquery.connection.v1  ](/bigquery/docs/reference/bigqueryconnection/rpc/google.cloud.bigquery.connection.v1)
* [ google.cloud.bigquery.connection.v1beta1  ](/bigquery/docs/reference/bigqueryconnection/rpc/google.cloud.bigquery.connection.v1beta1)
* [ google.iam.v1  ](/bigquery/docs/reference/bigqueryconnection/rpc/google.iam.v1)
* [ google.type  ](/bigquery/docs/reference/bigqueryconnection/rpc/google.type)

* REST reference (v1)

* REST Resources

* projects.locations.connections

* [ Overview  ](/bigquery/docs/reference/bigqueryconnection/rest/v1/projects.locations.connections)
* [ create  ](/bigquery/docs/reference/bigqueryconnection/rest/v1/projects.locations.connections/create)
* [ delete  ](/bigquery/docs/reference/bigqueryconnection/rest/v1/projects.locations.connections/delete)
* [ get  ](/bigquery/docs/reference/bigqueryconnection/rest/v1/projects.locations.connections/get)
* [ getIamPolicy  ](/bigquery/docs/reference/bigqueryconnection/rest/v1/projects.locations.connections/getIamPolicy)
* [ list  ](/bigquery/docs/reference/bigqueryconnection/rest/v1/projects.locations.connections/list)
* [ patch  ](/bigquery/docs/reference/bigqueryconnection/rest/v1/projects.locations.connections/patch)
* [ setIamPolicy  ](/bigquery/docs/reference/bigqueryconnection/rest/v1/projects.locations.connections/setIamPolicy)
* [ testIamPermissions  ](/bigquery/docs/reference/bigqueryconnection/rest/v1/projects.locations.connections/testIamPermissions)

* REST reference (v1beta1)

* REST Resources

* projects.locations.connections

* [ Overview  ](/bigquery/docs/reference/bigqueryconnection/rest/v1beta1/projects.locations.connections)
* [ create  ](/bigquery/docs/reference/bigqueryconnection/rest/v1beta1/projects.locations.connections/create)
* [ delete  ](/bigquery/docs/reference/bigqueryconnection/rest/v1beta1/projects.locations.connections/delete)
* [ get  ](/bigquery/docs/reference/bigqueryconnection/rest/v1beta1/projects.locations.connections/get)
* [ getIamPolicy  ](/bigquery/docs/reference/bigqueryconnection/rest/v1beta1/projects.locations.connections/getIamPolicy)
* [ list  ](/bigquery/docs/reference/bigqueryconnection/rest/v1beta1/projects.locations.connections/list)
* [ patch  ](/bigquery/docs/reference/bigqueryconnection/rest/v1beta1/projects.locations.connections/patch)
* [ setIamPolicy  ](/bigquery/docs/reference/bigqueryconnection/rest/v1beta1/projects.locations.connections/setIamPolicy)
* [ testIamPermissions  ](/bigquery/docs/reference/bigqueryconnection/rest/v1beta1/projects.locations.connections/testIamPermissions)
* [ updateCredential  ](/bigquery/docs/reference/bigqueryconnection/rest/v1beta1/projects.locations.connections/updateCredential)

* BigQuery Migration API reference

* [ BigQuery Migration client libraries  ](/bigquery/docs/reference/migration)
* [ BigQuery Migration REST API  ](/bigquery/docs/reference/migration/rest)
* REST reference (v2)

* REST Resources

* projects.locations.workflows

* [ Overview  ](/bigquery/docs/reference/migration/rest/v2/projects.locations.workflows)
* [ create  ](/bigquery/docs/reference/migration/rest/v2/projects.locations.workflows/create)
* [ delete  ](/bigquery/docs/reference/migration/rest/v2/projects.locations.workflows/delete)
* [ get  ](/bigquery/docs/reference/migration/rest/v2/projects.locations.workflows/get)
* [ list  ](/bigquery/docs/reference/migration/rest/v2/projects.locations.workflows/list)
* [ start  ](/bigquery/docs/reference/migration/rest/v2/projects.locations.workflows/start)

* projects.locations.workflows.subtasks

* [ Overview  ](/bigquery/docs/reference/migration/rest/v2/projects.locations.workflows.subtasks)
* [ get  ](/bigquery/docs/reference/migration/rest/v2/projects.locations.workflows.subtasks/get)
* [ list  ](/bigquery/docs/reference/migration/rest/v2/projects.locations.workflows.subtasks/list)

* Types

* [ Distribution  ](/bigquery/docs/reference/migration/rest/Shared.Types/Distribution)
* [ ErrorInfo  ](/bigquery/docs/reference/migration/rest/Shared.Types/ErrorInfo)
* [ MetricKind  ](/bigquery/docs/reference/migration/rest/Shared.Types/MetricKind)
* [ ResourceInfo  ](/bigquery/docs/reference/migration/rest/Shared.Types/ResourceInfo)
* [ ValueType  ](/bigquery/docs/reference/migration/rest/Shared.Types/ValueType)

* REST reference (v2alpha)

* REST Resources

* projects.locations.workflows

* [ Overview  ](/bigquery/docs/reference/migration/rest/v2alpha/projects.locations.workflows)
* [ create  ](/bigquery/docs/reference/migration/rest/v2alpha/projects.locations.workflows/create)
* [ delete  ](/bigquery/docs/reference/migration/rest/v2alpha/projects.locations.workflows/delete)
* [ get  ](/bigquery/docs/reference/migration/rest/v2alpha/projects.locations.workflows/get)
* [ list  ](/bigquery/docs/reference/migration/rest/v2alpha/projects.locations.workflows/list)
* [ start  ](/bigquery/docs/reference/migration/rest/v2alpha/projects.locations.workflows/start)

* projects.locations.workflows.subtasks

* [ Overview  ](/bigquery/docs/reference/migration/rest/v2alpha/projects.locations.workflows.subtasks)
* [ get  ](/bigquery/docs/reference/migration/rest/v2alpha/projects.locations.workflows.subtasks/get)
* [ list  ](/bigquery/docs/reference/migration/rest/v2alpha/projects.locations.workflows.subtasks/list)

* RPC reference

* [ Overview  ](/bigquery/docs/reference/migration/rpc)
* [ google.api  ](/bigquery/docs/reference/migration/rpc/google.api)
* [ google.cloud.bigquery.migration.tasks.assessment.v2alpha  ](/bigquery/docs/reference/migration/rpc/google.cloud.bigquery.migration.tasks.assessment.v2alpha)
* [ google.cloud.bigquery.migration.tasks.translation.v2alpha  ](/bigquery/docs/reference/migration/rpc/google.cloud.bigquery.migration.tasks.translation.v2alpha)
* [ google.cloud.bigquery.migration.v2  ](/bigquery/docs/reference/migration/rpc/google.cloud.bigquery.migration.v2)
* [ google.cloud.bigquery.migration.v2alpha  ](/bigquery/docs/reference/migration/rpc/google.cloud.bigquery.migration.v2alpha)
* [ google.rpc  ](/bigquery/docs/reference/migration/rpc/google.rpc)

* BigQuery Storage API reference

* [ Storage API client libraries  ](/bigquery/docs/reference/storage/libraries)
* RPC reference

* [ Overview  ](/bigquery/docs/reference/storage/rpc)
* [ google.cloud.bigquery.storage.v1  ](/bigquery/docs/reference/storage/rpc/google.cloud.bigquery.storage.v1)
* [ google.cloud.bigquery.storage.v1beta1  ](/bigquery/docs/reference/storage/rpc/google.cloud.bigquery.storage.v1beta1)
* [ google.cloud.bigquery.storage.v1beta2  ](/bigquery/docs/reference/storage/rpc/google.cloud.bigquery.storage.v1beta2)
* [ google.rpc  ](/bigquery/docs/reference/storage/rpc/google.rpc)

* BigQuery Reservation API reference

* [ BigQuery Reservation API client libraries  ](/bigquery/docs/reference/reservations)
* [ BigQuery Reservation REST API  ](/bigquery/docs/reference/reservations/rest)
* RPC reference

* [ Overview  ](/bigquery/docs/reference/reservations/rpc)
* [ google.cloud.bigquery.reservation.v1  ](/bigquery/docs/reference/reservations/rpc/google.cloud.bigquery.reservation.v1)
* [ google.rpc  ](/bigquery/docs/reference/reservations/rpc/google.rpc)

* REST reference (v1)

* REST Resources

* projects.locations

* [ Overview  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations)
* [ getBiReservation  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations/getBiReservation)
* [ searchAllAssignments  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations/searchAllAssignments)
* [ searchAssignments  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations/searchAssignments)
* [ updateBiReservation  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations/updateBiReservation)

* projects.locations.capacityCommitments

* [ Overview  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.capacityCommitments)
* [ create  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.capacityCommitments/create)
* [ delete  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.capacityCommitments/delete)
* [ get  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.capacityCommitments/get)
* [ list  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.capacityCommitments/list)
* [ merge  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.capacityCommitments/merge)
* [ patch  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.capacityCommitments/patch)
* [ split  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.capacityCommitments/split)

* projects.locations.reservations

* [ Overview  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.reservations)
* [ create  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.reservations/create)
* [ delete  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.reservations/delete)
* [ get  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.reservations/get)
* [ list  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.reservations/list)
* [ patch  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.reservations/patch)

* projects.locations.reservations.assignments

* [ Overview  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.reservations.assignments)
* [ create  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.reservations.assignments/create)
* [ delete  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.reservations.assignments/delete)
* [ list  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.reservations.assignments/list)
* [ move  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.reservations.assignments/move)
* [ patch  ](/bigquery/docs/reference/reservations/rest/v1/projects.locations.reservations.assignments/patch)

* Types

* [ BiReservation  ](/bigquery/docs/reference/reservations/rest/v1/BiReservation)
* [ Edition  ](/bigquery/docs/reference/reservations/rest/v1/Edition)

* BigQuery Analytics Hub API reference

* [ Analytics Hub client libraries  ](/bigquery/docs/reference/analytics-hub)
* [ Analytics Hub REST API  ](/bigquery/docs/reference/analytics-hub/rest)
* REST reference (v1)

* REST Resources

* organizations.locations.dataExchanges

* [ Overview  ](/bigquery/docs/reference/analytics-hub/rest/v1/organizations.locations.dataExchanges)
* [ list  ](/bigquery/docs/reference/analytics-hub/rest/v1/organizations.locations.dataExchanges/list)

* projects.locations.dataExchanges

* [ Overview  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges)
* [ create  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges/create)
* [ delete  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges/delete)
* [ get  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges/get)
* [ getIamPolicy  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges/getIamPolicy)
* [ list  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges/list)
* [ listSubscriptions  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges/listSubscriptions)
* [ patch  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges/patch)
* [ setIamPolicy  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges/setIamPolicy)
* [ subscribe  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges/subscribe)
* [ testIamPermissions  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges/testIamPermissions)

* projects.locations.dataExchanges.listings

* [ Overview  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges.listings)
* [ create  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges.listings/create)
* [ delete  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges.listings/delete)
* [ get  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges.listings/get)
* [ getIamPolicy  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges.listings/getIamPolicy)
* [ list  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges.listings/list)
* [ listSubscriptions  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges.listings/listSubscriptions)
* [ patch  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges.listings/patch)
* [ setIamPolicy  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges.listings/setIamPolicy)
* [ subscribe  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges.listings/subscribe)
* [ testIamPermissions  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.dataExchanges.listings/testIamPermissions)

* projects.locations.subscriptions

* [ Overview  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.subscriptions)
* [ delete  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.subscriptions/delete)
* [ get  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.subscriptions/get)
* [ list  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.subscriptions/list)
* [ refresh  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.subscriptions/refresh)
* [ revoke  ](/bigquery/docs/reference/analytics-hub/rest/v1/projects.locations.subscriptions/revoke)

* Types

* [ ListSharedResourceSubscriptionsResponse  ](/bigquery/docs/reference/analytics-hub/rest/v1/ListSharedResourceSubscriptionsResponse)
* [ Operation  ](/bigquery/docs/reference/analytics-hub/rest/v1/Operation)

* REST reference (v1beta1)

* REST Resources

* organizations.locations.dataExchanges

* [ Overview  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/organizations.locations.dataExchanges)
* [ list  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/organizations.locations.dataExchanges/list)

* projects.locations.dataExchanges

* [ Overview  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges)
* [ create  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges/create)
* [ delete  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges/delete)
* [ get  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges/get)
* [ getIamPolicy  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges/getIamPolicy)
* [ list  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges/list)
* [ patch  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges/patch)
* [ setIamPolicy  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges/setIamPolicy)
* [ testIamPermissions  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges/testIamPermissions)

* projects.locations.dataExchanges.listings

* [ Overview  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges.listings)
* [ create  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges.listings/create)
* [ delete  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges.listings/delete)
* [ get  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges.listings/get)
* [ getIamPolicy  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges.listings/getIamPolicy)
* [ list  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges.listings/list)
* [ patch  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges.listings/patch)
* [ setIamPolicy  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges.listings/setIamPolicy)
* [ subscribe  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges.listings/subscribe)
* [ testIamPermissions  ](/bigquery/docs/reference/analytics-hub/rest/v1beta1/projects.locations.dataExchanges.listings/testIamPermissions)

* BigQuery Data Transfer Service API reference

* [ BigQuery Data Transfer Service client libraries  ](/bigquery/docs/reference/datatransfer/libraries)
* [ BigQuery Data Transfer Service REST API  ](/bigquery/docs/reference/datatransfer/rest)
* REST reference

* REST Resources

* projects

* [ Overview  ](/bigquery/docs/reference/datatransfer/rest/v1/projects)
* [ enrollDataSources  ](/bigquery/docs/reference/datatransfer/rest/v1/projects/enrollDataSources)

* projects.dataSources

* [ Overview  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.dataSources)
* [ checkValidCreds  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.dataSources/checkValidCreds)
* [ get  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.dataSources/get)
* [ list  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.dataSources/list)

* projects.locations

* [ Overview  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations)
* [ enrollDataSources  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations/enrollDataSources)
* [ get  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations/get)
* [ list  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations/list)
* [ unenrollDataSources  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations/unenrollDataSources)

* projects.locations.dataSources

* [ Overview  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.dataSources)
* [ checkValidCreds  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.dataSources/checkValidCreds)
* [ get  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.dataSources/get)
* [ list  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.dataSources/list)

* projects.locations.transferConfigs

* [ Overview  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.transferConfigs)
* [ create  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.transferConfigs/create)
* [ delete  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.transferConfigs/delete)
* [ get  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.transferConfigs/get)
* [ list  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.transferConfigs/list)
* [ patch  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.transferConfigs/patch)
* [ scheduleRuns  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.transferConfigs/scheduleRuns)
* [ startManualRuns  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.transferConfigs/startManualRuns)

* projects.locations.transferConfigs.runs

* [ Overview  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.transferConfigs.runs)
* [ delete  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.transferConfigs.runs/delete)
* [ get  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.transferConfigs.runs/get)
* [ list  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.transferConfigs.runs/list)

* projects.locations.transferConfigs.runs.transferLogs

* [ Overview  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.transferConfigs.runs.transferLogs)
* [ list  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.locations.transferConfigs.runs.transferLogs/list)

* projects.transferConfigs

* [ Overview  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.transferConfigs)
* [ create  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.transferConfigs/create)
* [ delete  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.transferConfigs/delete)
* [ get  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.transferConfigs/get)
* [ list  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.transferConfigs/list)
* [ patch  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.transferConfigs/patch)
* [ scheduleRuns  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.transferConfigs/scheduleRuns)
* [ startManualRuns  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.transferConfigs/startManualRuns)

* projects.transferConfigs.runs

* [ Overview  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.transferConfigs.runs)
* [ delete  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.transferConfigs.runs/delete)
* [ get  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.transferConfigs.runs/get)
* [ list  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.transferConfigs.runs/list)

* projects.transferConfigs.runs.transferLogs

* [ Overview  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.transferConfigs.runs.transferLogs)
* [ list  ](/bigquery/docs/reference/datatransfer/rest/v1/projects.transferConfigs.runs.transferLogs/list)

* Types

* [ CheckValidCredsResponse  ](/bigquery/docs/reference/datatransfer/rest/v1/CheckValidCredsResponse)
* [ Code  ](/bigquery/docs/reference/datatransfer/rest/v1/Code)
* [ EmailPreferences  ](/bigquery/docs/reference/datatransfer/rest/v1/EmailPreferences)
* [ ListDataSourcesResponse  ](/bigquery/docs/reference/datatransfer/rest/v1/ListDataSourcesResponse)
* [ ListTransferConfigsResponse  ](/bigquery/docs/reference/datatransfer/rest/v1/ListTransferConfigsResponse)
* [ ListTransferLogsResponse  ](/bigquery/docs/reference/datatransfer/rest/v1/ListTransferLogsResponse)
* [ ListTransferRunsResponse  ](/bigquery/docs/reference/datatransfer/rest/v1/ListTransferRunsResponse)
* [ RunAttempt  ](/bigquery/docs/reference/datatransfer/rest/v1/RunAttempt)
* [ ScheduleTransferRunsResponse  ](/bigquery/docs/reference/datatransfer/rest/v1/ScheduleTransferRunsResponse)
* [ StartManualTransferRunsResponse  ](/bigquery/docs/reference/datatransfer/rest/v1/StartManualTransferRunsResponse)
* [ TimeRange  ](/bigquery/docs/reference/datatransfer/rest/v1/TimeRange)
* [ TransferState  ](/bigquery/docs/reference/datatransfer/rest/v1/TransferState)

* RPC reference

* [ Overview  ](/bigquery/docs/reference/datatransfer/rpc)
* [ google.cloud.bigquery.datatransfer.v1  ](/bigquery/docs/reference/datatransfer/rpc/google.cloud.bigquery.datatransfer.v1)
* [ google.cloud.location  ](/bigquery/docs/reference/datatransfer/rpc/google.cloud.location)
* [ google.rpc  ](/bigquery/docs/reference/datatransfer/rpc/google.rpc)

* BigQuery BigLake API reference

* [ BigLake REST API  ](/bigquery/docs/reference/biglake/rest)
* REST reference (v1)

* REST Resources

* projects.locations.catalogs

* [ Overview  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs)
* [ create  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs/create)
* [ delete  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs/delete)
* [ get  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs/get)
* [ list  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs/list)

* projects.locations.catalogs.databases

* [ Overview  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs.databases)
* [ create  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs.databases/create)
* [ delete  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs.databases/delete)
* [ get  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs.databases/get)
* [ list  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs.databases/list)
* [ patch  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs.databases/patch)

* projects.locations.catalogs.databases.tables

* [ Overview  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs.databases.tables)
* [ create  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs.databases.tables/create)
* [ delete  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs.databases.tables/delete)
* [ get  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs.databases.tables/get)
* [ list  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs.databases.tables/list)
* [ patch  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs.databases.tables/patch)
* [ rename  ](/bigquery/docs/reference/biglake/rest/v1/projects.locations.catalogs.databases.tables/rename)

* REST reference (v1alpha1)

* REST Resources

* projects.locations.catalogs

* [ Overview  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs)
* [ create  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs/create)
* [ delete  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs/delete)
* [ get  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs/get)
* [ list  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs/list)

* projects.locations.catalogs.databases

* [ Overview  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases)
* [ create  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases/create)
* [ delete  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases/delete)
* [ get  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases/get)
* [ list  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases/list)
* [ patch  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases/patch)

* projects.locations.catalogs.databases.locks

* [ Overview  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases.locks)
* [ check  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases.locks/check)
* [ create  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases.locks/create)
* [ delete  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases.locks/delete)
* [ list  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases.locks/list)

* projects.locations.catalogs.databases.tables

* [ Overview  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases.tables)
* [ create  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases.tables/create)
* [ delete  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases.tables/delete)
* [ get  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases.tables/get)
* [ list  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases.tables/list)
* [ patch  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases.tables/patch)
* [ rename  ](/bigquery/docs/reference/biglake/rest/v1alpha1/projects.locations.catalogs.databases.tables/rename)

* BigQuery routines

* [ System procedures reference  ](/bigquery/docs/reference/system-procedures)
* [ System variables reference  ](/bigquery/docs/reference/system-variables)
* BigQuery audit logging

* BigQuery audit logging reference

* [ Overview  ](/bigquery/docs/reference/auditlogs)
* Types

* [ AuditData  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/AuditData)
* [ AuditLogConfig.LogType  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/AuditLogConfig.LogType)
* [ BigQueryAuditMetadata  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata)
* [ BigQueryAuditMetadata.AccessChange.Action  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.AccessChange.Action)
* [ BigQueryAuditMetadata.ConnectionChange.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.ConnectionChange.Reason)
* [ BigQueryAuditMetadata.CreateDisposition  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.CreateDisposition)
* [ BigQueryAuditMetadata.DatasetChange.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.DatasetChange.Reason)
* [ BigQueryAuditMetadata.DatasetCreation.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.DatasetCreation.Reason)
* [ BigQueryAuditMetadata.DatasetDeletion.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.DatasetDeletion.Reason)
* [ BigQueryAuditMetadata.JobConfig.Query.Priority  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.JobConfig.Query.Priority)
* [ BigQueryAuditMetadata.JobConfig.Type  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.JobConfig.Type)
* [ BigQueryAuditMetadata.JobDeletion.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.JobDeletion.Reason)
* [ BigQueryAuditMetadata.JobInsertion.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.JobInsertion.Reason)
* [ BigQueryAuditMetadata.JobState  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.JobState)
* [ BigQueryAuditMetadata.ModelCreation.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.ModelCreation.Reason)
* [ BigQueryAuditMetadata.ModelDataChange.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.ModelDataChange.Reason)
* [ BigQueryAuditMetadata.ModelDataRead.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.ModelDataRead.Reason)
* [ BigQueryAuditMetadata.ModelDeletion.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.ModelDeletion.Reason)
* [ BigQueryAuditMetadata.ModelMetadataChange.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.ModelMetadataChange.Reason)
* [ BigQueryAuditMetadata.OperationType  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.OperationType)
* [ BigQueryAuditMetadata.QueryStatementType  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.QueryStatementType)
* [ BigQueryAuditMetadata.RoutineChange.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.RoutineChange.Reason)
* [ BigQueryAuditMetadata.RoutineCreation.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.RoutineCreation.Reason)
* [ BigQueryAuditMetadata.RoutineDeletion.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.RoutineDeletion.Reason)
* [ BigQueryAuditMetadata.SearchIndexCreation.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.SearchIndexCreation.Reason)
* [ BigQueryAuditMetadata.SearchIndexDeletion.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.SearchIndexDeletion.Reason)
* [ BigQueryAuditMetadata.TableChange.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.TableChange.Reason)
* [ BigQueryAuditMetadata.TableCreation.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.TableCreation.Reason)
* [ BigQueryAuditMetadata.TableDataChange.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.TableDataChange.Reason)
* [ BigQueryAuditMetadata.TableDataRead.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.TableDataRead.Reason)
* [ BigQueryAuditMetadata.TableDeletion.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.TableDeletion.Reason)
* [ BigQueryAuditMetadata.UnlinkDataset.Reason  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.UnlinkDataset.Reason)
* [ BigQueryAuditMetadata.WriteDisposition  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.WriteDisposition)
* [ BindingDelta.Action  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/BindingDelta.Action)
* [ DatasetAccessEntry  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/DatasetAccessEntry)
* [ DatasetAccessEntry.TargetType  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/DatasetAccessEntry.TargetType)
* [ Expr  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/Expr)
* [ JoinRestrictionPolicy.JoinCondition  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/JoinRestrictionPolicy.JoinCondition)
* [ Policy  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/Policy)
* [ RoutineReference  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/RoutineReference)
* [ Status  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/Status)
* [ TableReference  ](/bigquery/docs/reference/auditlogs/rest/Shared.Types/TableReference)

* [ AI solutions, generative AI, and ML  ](/docs/ai-ml)
* [ Application development  ](/docs/application-development)
* [ Application hosting  ](/docs/application-hosting)
* [ Compute  ](/docs/compute-area)
* [ Data analytics and pipelines  ](/docs/data)
* [ Databases  ](/docs/databases)
* [ Distributed, hybrid, and multicloud  ](/docs/dhm-cloud)
* [ Industry solutions  ](/docs/industry)
* [ Networking  ](/docs/networking)
* [ Observability and monitoring  ](/docs/observability)
* [ Security  ](/docs/security)
* [ Storage  ](/docs/storage)

* [ Access and resources management  ](/docs/access-resources)
* [ Cloud SDK, languages, frameworks, and tools  ](/docs/devtools)
* [ Costs and usage management  ](/docs/costs-usage)
* [ Infrastructure as code  ](/docs/iac)
* [ Migration  ](/docs/migration)

* [ Google Cloud Home  ](/)
* [ Free Trial and Free Tier  ](/free)
* [ Architecture Center  ](/architecture)
* [ Blog  ](https://cloud.google.com/blog)
* [ Contact Sales  ](/contact)
* [ Google Cloud Developer Center  ](/developers)
* [ Google Developer Center  ](https://developers.google.com/)
* [ Google Cloud Marketplace (in console)  ](https://console.cloud.google.com/marketplace)
* [ Google Cloud Marketplace Documentation  ](/marketplace/docs)
* [ Google Cloud Skills Boost  ](https://www.cloudskillsboost.google/paths)
* [ Google Cloud Solution Center  ](/solutions)
* [ Google Cloud Support  ](/support-hub)
* [ Google Cloud Tech Youtube Channel  ](https://www.youtube.com/@googlecloudtech)

* [ Home ](https://cloud.google.com/)
* [ BigQuery ](https://cloud.google.com/bigquery)
* [ Documentation ](https://cloud.google.com/bigquery/docs)
* [ Reference ](https://cloud.google.com/bigquery/quotas)

Send feedback  Stay organized with collections  Save and categorize content
based on your preferences.

#  The CREATE MODEL statement for ARIMA_PLUS models

This document describes the ` CREATE MODEL ` statement for creating univariate
time series models in BigQuery.

Forecasting takes place when you create the model. You can use the [ `
ML.FORECAST ` ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-
forecast) and [ ` ML.EXPLAIN_FORECAST ` ](/bigquery/docs/reference/standard-
sql/bigqueryml-syntax-explain-forecast) functions to retrieve the forecasting
values and compute the prediction intervals.

For information about the supported SQL statements and functions for each
model type, see [ End-to-end user journey for each model
](/bigquery/docs/e2e-journey) .

##  Time series modeling pipeline

The BigQuery ML time series modeling pipeline includes multiple modules. The [
ARIMA
](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average)
model is the most computationally expensive module, which is why the model is
named ` ARIMA_PLUS ` .

![SINGLE_TIME_SERIES_DIAGRAM](/static/bigquery/images/BQ_ARIMA_diagram.png)

The modeling pipeline for the ` ARIMA_PLUS ` time series models performs the
following functions:

* Infer the data frequency of the time series.
* Handle irregular time intervals.
* Handle duplicated timestamps by taking the mean value.
* Interpolate missing data using local linear interpolation.
* Detect and clean spike and dip outliers.
* Detect and adjust abrupt step (level) changes.
* Detect and adjust holiday effect.
* Detect multiple seasonal patterns within a single time series by using [ Seasonal and Trend decomposition using Loess (STL) ](https://otexts.com/fpp2/stl.html) , and extrapolate seasonality by using [ double exponential smoothing (ETS) ](https://en.wikipedia.org/wiki/Exponential_smoothing#Double_exponential_smoothing) .
* Detect and model the trend using the ARIMA model and the [ auto.ARIMA ](https://otexts.com/fpp2/arima-r.html) algorithm for automatic hyperparameter tuning. In auto.ARIMA, dozens of candidate models are trained and evaluated in parallel. The model with the lowest [ Akaike information criterion (AIC) ](https://en.wikipedia.org/wiki/Akaike_information_criterion) is selected as the best model.

##  Large-scale time series

You can forecast up to 100,000,000 time series simultaneously with a single
query by using the  ` TIME_SERIES_ID_COL ` option. With this option, different
modeling pipelines run in parallel, as long as enough [ slots
](/bigquery/docs/slots) are available. The following diagram shows this
process:

![MULTIPLE_TIME_SERIES_DIAGRAM](/static/bigquery/images/BQ_Multiple_ARIMA_diagram.png)

##  Large-scale time series forecasting best practices

Forecasting many time series simultaneously can lead to long-running queries,
because query processing isn't completely parallel due to limited slot
capacity. The following best practices can help you avoid long-running queries
when forecasting many time series simultaneously:

* When you have a large number (for example, 100,000) of time series to forecast, first forecast a small number of time series (for example, 1,000) to see how long the query takes. You can then estimate how long your entire time series forecast will take.
* You can use the ` AUTO_ARIMA_MAX_ORDER ` option to balance between query run time and forecast accuracy. Increasing ` AUTO_ARIMA_MAX_ORDER ` expands the hyperparameter search space to try more complex ARIMA models, that is, ARIMA models with higher non-seasonal p and q. This increases forecast accuracy but also increases query run time. Decreasing the value of ` AUTO_ARIMA_MAX_ORDER ` decreases forecast accuracy but also decreases query run time. For example, if you specify a value of ` 3 ` instead of using the default value of ` 5 ` for this option, the query run time is reduced by at least 50%. The forecast accuracy might drop slightly for some of the time series. If a shorter training time is important to your use case, use a smaller value for ` AUTO_ARIMA_MAX_ORDER ` .
* The model training time for each time series has a linear relationship to its length, which is based on the number of data points. The longer the time series, the longer the training takes. However, not all data points contribute equally to the model fitting process. Instead, the more recent the data point is, the more it contributes to the process. Therefore, if you have a long time series, for example ten years of daily data, you don't need to train a time series model using all of the data points. The most recent two or three years of data points are enough.
* You can use the ` TIME_SERIES_LENGTH_FRACTION ` , ` MIN_TIME_SERIES_LENGTH ` and ` MAX_TIME_SERIES_LENGTH ` training options to enable fast model training with little to no loss of forecasting accuracy. The idea behind these options is that while periodic modeling, such as seasonality, requires a certain number of time points, trend modeling doesn't need many time points. However, trend modeling is much more computationally expensive than other time series components. By using the aforementioned training options, you can efficiently model the trend component with a subset of the time series, while the other time series components use the entire time series.
* To avoid a single long-running query, use BigQuery [ multi-statement queries ](/bigquery/docs/multi-statement-queries) .

You can try these best practices by following the [ Scalable forecasting with
millions of time series in BigQuery ](/bigquery/docs/arima-speed-up-tutorial)
tutorial.

##  ` CREATE MODEL ` syntax



{CREATE MODEL | CREATE MODEL IF NOT EXISTS | CREATE OR REPLACE MODEL}
model_name
OPTIONS(model_option_list)
AS { query_statement |
(
training_data AS (query_statement),
custom_holiday AS (holiday_statement)
)
}

model_option_list:
MODEL_TYPE = 'ARIMA_PLUS'
[, TIME_SERIES_TIMESTAMP_COL = string_value ]
[, TIME_SERIES_DATA_COL = string_value ]
[, TIME_SERIES_ID_COL = { string_value | string_array } ]
[, HORIZON = int64_value ]
[, AUTO_ARIMA = { TRUE | FALSE } ]
[, AUTO_ARIMA_MAX_ORDER = int64_value ]
[, AUTO_ARIMA_MIN_ORDER = int64_value ]
[, NON_SEASONAL_ORDER = (int64_value, int64_value, int64_value) ]
[, DATA_FREQUENCY = { 'AUTO_FREQUENCY' | 'PER_MINUTE' | 'HOURLY' | 'DAILY' | 'WEEKLY' | 'MONTHLY' | 'QUARTERLY' | 'YEARLY' } ]
[, INCLUDE_DRIFT = { TRUE | FALSE } ]
[, HOLIDAY_REGION = string_value | string_array ]
[, CLEAN_SPIKES_AND_DIPS = { TRUE | FALSE } ]
[, ADJUST_STEP_CHANGES = { TRUE | FALSE } ]
[, TIME_SERIES_LENGTH_FRACTION = float64_value ]
[, MIN_TIME_SERIES_LENGTH = int64_value ]
[, MAX_TIME_SERIES_LENGTH = int64_value ]
[, TREND_SMOOTHING_WINDOW_SIZE = int64_value ]
[, DECOMPOSE_TIME_SERIES = { TRUE | FALSE } ]
[, FORECAST_LIMIT_LOWER_BOUND = float64_value ]
[, FORECAST_LIMIT_UPPER_BOUND = float64_value ]
[, SEASONALITIES = string_array ]
[, HIERARCHICAL_TIME_SERIES_COLS = {string_array } ]


###  ` CREATE MODEL `

Creates and trains a new model in the specified dataset. If the model name
exists, ` CREATE MODEL ` returns an error.

###  ` CREATE MODEL IF NOT EXISTS `

Creates and trains a new model only if the model doesn't exist in the
specified dataset.

###  ` CREATE OR REPLACE MODEL `

Creates and trains a model and replaces an existing model with the same name
in the specified dataset.

###  ` model_name `

The name of the model you're creating or replacing. The model name must be
unique in the dataset: no other model or table can have the same name. The
model name must follow the same naming rules as a BigQuery table. A model name
can:

* Contain up to 1,024 characters
* Contain letters (upper or lower case), numbers, and underscores

` model_name ` is not case-sensitive.

If you don't have a default project configured, then you must prepend the
project ID to the model name in the following format, including backticks:

`[PROJECT_ID].[DATASET].[MODEL]`

For example, `myproject.mydataset.mymodel`.

###  ` MODEL_TYPE `

**Syntax**



MODEL_TYPE = 'ARIMA_PLUS'


**Description**

Specifies the model type. This option is required.

**Note:** The ` ARIMA ` model type is deprecated. While the model training
pipelines of ` ARIMA ` and ` ARIMA_PLUS ` models are the same, ` ARIMA_PLUS `
supports more capabilities, including use of the [ ` DECOMPOSE_TIME_SERIES `
option ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-time-
series#decompose_time_series) and the ability to work with the [ `
ML.ARIMA_EVALUATE ` ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-
arima-evaluate) and [ ` ML.EXPLAIN_FORECAST `
](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-explain-forecast)
functions.

###  ` TIME_SERIES_TIMESTAMP_COL `

**Syntax**

` TIME_SERIES_TIMESTAMP_COL =  string_value  `

**Description**

The name of the column that provides the time points used in training the
model. The column must be of one of the following data types:

* ` TIMESTAMP `
* ` DATE `
* ` DATETIME `

**Arguments**

A ` STRING ` value.

###  ` TIME_SERIES_DATA_COL `

**Syntax**

` TIME_SERIES_DATA_COL =  string_value  `

**Description**

The name of the column that contains the data to forecast. The column must be
of one of the following data types:

* ` INT64 `
* ` NUMERIC `
* ` BIGNUMERIC `
* ` FLOAT64 `

**Arguments**

A ` STRING ` value.

###  ` TIME_SERIES_ID_COL `

**Syntax**

` TIME_SERIES_ID_COL = {  string_value  |  string_array  } `

**Description**

The names of the ID columns. Specify one or more values for this option when
you want to fit and forecast multiple time series using a single query. Each
ID identifies a unique time series. The columns must be of one of the
following data types:

* ` STRING `
* ` INT64 `
* ` ARRAY<STRING> `
* ` ARRAY<INT64> `

**Arguments**

A ` STRING ` or ` ARRAY<STRING> ` value.

###  ` HORIZON `

**Syntax**

` HORIZON =  int64_value  `

**Description**

The number of time points to forecast.

When forecasting multiple time series at once, this parameter applies to each
time series.

**Arguments**

An ` INT64 ` value. The default value is ` 1,000 ` . The maximum value is `
10,000 ` .

###  ` AUTO_ARIMA `

**Syntax**



AUTO_ARIMA = { TRUE | FALSE }


**Description**

Determines whether the training process uses auto.ARIMA or not. If ` TRUE ` ,
training automatically finds the best non-seasonal order (that is, the p, d, q
tuple) and decides whether or not to include a linear drift term when d is 1.
If ` FALSE ` , you must specify the ` NON_SEASONAL_ORDER ` option.

When forecasting multiple time series at the same time, you must use the
auto.ARIMA algorithm for each time series, so this option must be ` TRUE ` .

**Arguments**

A ` BOOL ` value. The default value is ` TRUE ` .

###  ` AUTO_ARIMA_MAX_ORDER `

**Syntax**

` AUTO_ARIMA_MAX_ORDER =  int64_value  `

**Description**

The maximum value for the sum of non-seasonal p and q. This value determines
the parameter search space in the auto.ARIMA algorithm, in combination with
the ` AUTO_ARIMA_MIN_ORDER ` value. This option is disabled when the `
AUTO_ARIMA ` value is ` FALSE ` .

**Arguments**

An ` INT64 ` value between ` 1 ` and ` 5 ` , inclusive. The default value is `
5 ` .

If non-seasonal d is determined to be 0 or 2, the number of candidate models
evaluated for each supported value is as follows:

* ` 1 ` : 3 candidate models
* ` 2 ` : 6 candidate models
* ` 3 ` : 10 candidate models
* ` 4 ` : 15 candidate models
* ` 5 ` : 21 candidate models

If non-seasonal d is determined to be 1, the number of candidate models to
evaluate is doubled, because there's an additional drift term to consider for
all of the existing candidate models.

**Note:** the number of bytes processed by the input ` SELECT ` statement is
multiplied by the number of candidate models, which is controlled by the `
AUTO_ARIMA_MAX_ORDER ` and ` AUTO_ARIMA_MIN_ORDER ` options. This affects the
pricing. See [ BigQuery ML pricing ](/bigquery/pricing#bqml) for details.

###  ` AUTO_ARIMA_MIN_ORDER `

**Syntax**

` AUTO_ARIMA_MIN_ORDER =  int64_value  `

**Description**

The minimum value for the sum of non-seasonal p and q. This value determines
the parameter search space in the auto.ARIMA algorithm, in combination with
the ` AUTO_ARIMA_MAX_ORDER ` value. Setting this option to ` 1 ` or greater
lets the model exclude some flat forecasting results. This option is disabled
when ` AUTO_ARIMA ` is ` FALSE ` .

**Arguments**

The value is a ` INT64 ` . The default value is ` 0 ` .

**Note:** If ` AUTO_ARIMA_MAX_ORDER ` is set, the ` AUTO_ARIMA_MIN_ORDER `
value must be less than the ` AUTO_ARIMA_MAX_ORDER ` value.

###  ` NON_SEASONAL_ORDER `

**Syntax**

` NON_SEASONAL_ORDER =  (p_value, d_value, q_value)  `

**Description**

The tuple of non-seasonal p, d, q for the ` ARIMA_PLUS ` model. There are no
default values, and you must specify all three values. p and q must be a value
between ` 0 ` and ` 5 ` , inclusive. d must be a value between ` 0 ` and ` 2 `
, inclusive.

You can't use this option when forecasting multiple time series at the same
time, because the auto.ARIMA algorithm must be used for each time series.

The ` AUTO_ARIMA ` value must be ` FALSE ` to use this option.

**Arguments**

A tuple of three ` INT64 ` values. For example, ` (1, 2, 1) ` .

###  ` DATA_FREQUENCY `

**Syntax**



DATA_FREQUENCY = { 'AUTO_FREQUENCY' | 'PER_MINUTE' | 'HOURLY' | 'DAILY' | 'WEEKLY' | 'MONTHLY' | 'QUARTERLY' | 'YEARLY' }


**Description**

The data frequency of the input time series. The finest supported granularity
is ` PER_MINUTE ` .

When forecasting multiple time series at once, this argument applies to all
individual time series.

**Arguments**

This option accepts the following values:

* ` AUTO_FREQUENCY ` : This is the default. The training process automatically infers the data frequency, which can be any of the other supported values for this option.
* ` PER_MINUTE `
* ` HOURLY `
* ` DAILY `
* ` WEEKLY `
* ` MONTHLY `
* ` QUARTERLY `
* ` YEARLY `

###  ` INCLUDE_DRIFT `

**Syntax**



INCLUDE_DRIFT = { TRUE | FALSE }


**Description**

Determines whether the ` ARIMA_PLUS ` model should include a linear drift term
or not. The drift term is applicable when non-seasonal d is 1.

* When the ` AUTO_ARIMA ` value is ` FALSE ` , this argument defaults to ` FALSE ` . You can set it to ` TRUE ` only when non-seasonal d is 1. Otherwise the ` CREATE MODEL ` statement returns an invalid query error.
* When the ` AUTO_ARIMA ` value is ` TRUE ` , BigQuery ML automatically determines whether or not to include a linear drift term, so you can't use this option.

**Arguments**

A ` BOOL ` value. The default value is ` FALSE ` .

###  ` HOLIDAY_REGION `

**Syntax**

` HOLIDAY_REGION =  string_value  |  string_array  `

**Description**

The geographical region based on which the holiday effect is applied in
modeling. By default, holiday effect modeling isn't used. To use it, specify
one or more holiday regions using this option. If you include more than one
region string, the union of the holidays in all the provided regions are taken
into account when modeling.

Holiday effect modeling is only applicable when the time series is daily or
weekly, and longer than a year. If the input time series doesn't meet these
requirements, holiday effect modeling isn't used even if you specify this
option.

For more information about the holidays included in each region, see  Holiday
data  .

**Arguments**

A ` STRING ` or ` ARRAY<STRING> ` value.

Use a single string value to identify one region. For example:



HOLIDAY_REGION = 'GLOBAL'


Use an array of string values to identify multiple regions. For example:



HOLIDAY_REGION = ['US', 'GB']


This option accepts the following values:

**Global**

* ` GLOBAL `

**Continental regions**

* ` NA ` : North America
* ` JAPAC ` : Japan and Asia Pacific
* ` EMEA ` : Europe, the Middle East and Africa
* ` LAC ` : Latin America and the Caribbean

**Countries**

* ` AE ` : United Arab Emirates
* ` AR ` : Argentina
* ` AT ` : Austria
* ` AU ` : Australia
* ` BE ` : Belgium
* ` BR ` : Brazil
* ` CA ` : Canada
* ` CH ` : Switzerland
* ` CL ` : Chile
* ` CN ` : China
* ` CO ` : Colombia
* ` CZ ` : Czechia
* ` DE ` : Germany
* ` DK ` : Denmark
* ` DZ ` : Algeria
* ` EC ` : Ecuador
* ` EE ` : Estonia
* ` EG ` : Egypt
* ` ES ` : Spain
* ` FI ` : Finland
* ` FR ` : France
* ` GB ` : United Kingdom
* ` GR ` : Greece
* ` HK ` : Hong Kong
* ` HU ` : Hungary
* ` ID ` : Indonesia
* ` IE ` : Ireland
* ` IL ` : Israel
* ` IN ` : India
* ` IR ` : Iran
* ` IT ` : Italy
* ` JP ` : Japan
* ` KR ` : South Korea
* ` LV ` : Latvia
* ` MA ` : Morocco
* ` MX ` : Mexico
* ` MY ` : Malaysia
* ` NG ` : Nigeria
* ` NL ` : Netherlands
* ` NO ` : Norway
* ` NZ ` : New Zealand
* ` PE ` : Peru
* ` PH ` : Philippines
* ` PK ` : Pakistan
* ` PL ` : Poland
* ` PT ` : Portugal
* ` RO ` : Romania
* ` RS ` : Serbia
* ` RU ` : Russia
* ` SA ` : Saudi Arabia
* ` SE ` : Sweden
* ` SG ` : Singapore
* ` SI ` : Slovenia
* ` SK ` : Slovakia
* ` TH ` : Thailand
* ` TR ` : Turkey
* ` TW ` : Taiwan
* ` UA ` : Ukraine
* ` US ` : United States
* ` VE ` : Venezuela
* ` VN ` : Vietnam
* ` ZA ` : South Africa

###  ` CLEAN_SPIKES_AND_DIPS `

**Syntax**



CLEAN_SPIKES_AND_DIPS = { TRUE | FALSE }


**Description**

Determines whether or not to perform automatic spikes and dips detection and
cleanup in the ` ARIMA_PLUS ` model training pipeline. The spikes and dips are
replaced with local linear interpolated values when they're detected.

**Arguments**

A ` BOOL ` value. The default value is ` TRUE ` .

###  ` ADJUST_STEP_CHANGES `

**Syntax**



ADJUST_STEP_CHANGES = { TRUE | FALSE }


**Description**

Determines whether or not to perform automatic step change detection and
adjustment in the ` ARIMA_PLUS ` model training pipeline.

**Arguments**

A ` BOOL ` value. The default value is ` TRUE ` .

###  ` TIME_SERIES_LENGTH_FRACTION `

**Syntax**

` TIME_SERIES_LENGTH_FRACTION =  float64_value  `

**Description**

The fraction of the interpolated length of the time series that's used to
model the time series trend component. All of the time points of the time
series are used to model the non-trend component. For example, if the time
series has 100 time points, then specifying a ` TIME_SERIES_LENGTH_FRACTION `
of ` 0.5 ` uses the most recent 50 time points for trend modeling. This
training option accelerates modeling training without sacrificing much
forecasting accuracy.

You can use the ` TIME_SERIES_LENGTH_FRACTION ` option with the `
MIN_TIME_SERIES_LENGTH ` option, but not with the ` MAX_TIME_SERIES_LENGTH `
option.

**Arguments**

A ` FLOAT64 ` value in the range ` (0, 1) ` . The default behavior is to use
all the points in the time series.

###  ` MIN_TIME_SERIES_LENGTH `

**Syntax**

` MIN_TIME_SERIES_LENGTH =  int64_value  `

**Description**

The minimum number of time points that are used in modeling the trend
component of the time series. If you use this option, you must also specify a
value for the ` TIME_SERIES_LENGTH_FRACTION ` option. For example, if you use
` TIME_SERIES_ID_COL ` to forecast two time series, one with 100 time points
and another with 30 time points, then setting ` TIME_SERIES_LENGTH_FRACTION `
to ` 0.5 ` and ` MIN_TIME_SERIES_LENGTH ` to ` 20 ` results in the last 50
points of first time series being used for trend modeling. For the second time
series, the last 20 points rather than the last 15 points ( ` 30 * 0.5 ` ) are
used in trend modeling because the ` MIN_TIME_SERIES_LENGTH ` value is ` 20 `
. This option ensures that enough time points are available when you use `
TIME_SERIES_LENGTH_FRACTION ` in trend modeling. This is particularly
important when forecasting multiple time series in a single query using the `
TIME_SERIES_ID_COL ` option. If the total number of time points is less than
the ` MIN_TIME_SERIES_LENGTH ` value, then the query uses all available time
points.

You can use the ` MIN_TIME_SERIES_LENGTH ` option with the `
TIME_SERIES_LENGTH_FRACTION ` option, but not with the `
MAX_TIME_SERIES_LENGTH ` option.

**Arguments**

An ` INT64 ` value greater than or equal to ` 4 ` . The default value is ` 20
` .

###  ` MAX_TIME_SERIES_LENGTH `

**Syntax**

` MAX_TIME_SERIES_LENGTH =  int64_value  `

**Description**

The maximum number of time points in a time series that can be used in
modeling the trend component of the time series. For example, if you are
forecasting two time series simultaneously by specifying the `
TIME_SERIES_ID_COL ` option, and one time series has 100 time points while the
other one has 50 time points, then by setting ` MAX_TIME_SERIES_LENGTH ` to `
30 ` , both of the time series use the last 30 time points for trend modeling.

You can't use the ` MAX_TIME_SERIES_LENGTH ` with the `
TIME_SERIES_LENGTH_FRACTION ` or ` MIN_TIME_SERIES_LENGTH ` options.

**Arguments**

An ` INT64 ` value greater than or equal to ` 4 ` . There is no default value.
We recommend trying ` 30 ` as a starting value.

###  ` TREND_SMOOTHING_WINDOW_SIZE `

**Syntax**

` TREND_SMOOTHING_WINDOW_SIZE =  int64_value  `

**Description**

The smoothing window size for the trend component. When you specify a value, a
center moving average smoothing is applied on the history trend. When the
smoothing window is out of the boundary at the beginning or the end of the
trend, the first element or the last element is padded to fill the smoothing
window before the average is applied.

Specifying a value for ` TREND_SMOOTHING_WINDOW_SIZE ` doesn't affect
forecasting results. It only affects the smoothness of the trend component,
which you can see by using the [ ` ML.EXPLAIN_FORECAST ` function
](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-explain-forecast) .

**Arguments**

An ` INT64 ` value. There is no default value. You must specify a positive
value to smooth the trend.

###  ` DECOMPOSE_TIME_SERIES `

**Syntax**



DECOMPOSE_TIME_SERIES = { TRUE | FALSE }


**Description**

Determines whether the separate components of both the history and forecast
parts of the time series (such as holiday effect and seasonal components) are
saved in the ` ARIMA_PLUS ` model.

Time series decomposition takes place when you create the model. The [ `
ML.EXPLAIN_FORECAST ` ](/bigquery/docs/reference/standard-sql/bigqueryml-
syntax-explain-forecast) function retrieves the separate components of both
the training and the forecasting data and computes the confidence intervals.
Because the decomposition results are saved in the model, the training data
can be partially or fully recovered from the decomposition results.

**Arguments**

A ` BOOL ` value. The default value is ` TRUE ` .

###  ` FORECAST_LIMIT_LOWER_BOUND `

**Syntax**

` FORECAST_LIMIT_LOWER_BOUND =  float64_value  `

**Description**

The lower bound of the forecasting values. When you specify the `
FORECAST_LIMIT_LOWER_BOUND ` option, all of the forecast values must be
greater than the specified value. For example, if you set `
FORECAST_LIMIT_LOWER_BOUND ` to ` 0 ` , then all of the forecast values are
larger than ` 0 ` . Also, all values less than or equal to the `
FORECAST_LIMIT_LOWER_BOUND ` value are excluded from modelling. The
forecasting limit [ ensures that forecasts stay within limits
](https://otexts.com/fpp2/limits.html) .

If you specify a value for the ` FORECAST_LIMIT_UPPER_BOUND ` option, the `
FORECAST_LIMIT_UPPER_BOUND ` value must be greater than the `
FORECAST_LIMIT_LOWER_BOUND ` value.

**Arguments**

A ` FLOAT_64 ` value greater than or equal to ` -1.7976931348623157E+308 ` .
The default value is ` NAN ` .

###  ` FORECAST_LIMIT_UPPER_BOUND `

**Syntax**

` FORECAST_LIMIT_UPPER_BOUND =  float64_value  `

**Description**

The upper bound of the forecasting values. When you specify the `
FORECAST_LIMIT_UPPER_BOUND ` option, all of the forecast values must be less
than the specified value. For example, if you set ` FORECAST_LIMIT_UPPER_BOUND
` to ` 100 ` , then all of the forecast values are less than ` 100 ` . Also,
all values greater than or equal to the ` FORECAST_LIMIT_UPPER_BOUND ` value
are excluded from modelling. The forecasting limit ensures that [ forecasts
stay within limits ](https://otexts.com/fpp2/limits.html) .

If you specify a value for the ` FORECAST_LIMIT_LOWER_BOUND ` option, the `
FORECAST_LIMIT_LOWER_BOUND ` value must be less than the `
FORECAST_LIMIT_UPPER_BOUND ` value.

**Arguments**

A ` FLOAT_64 ` value less thanor equal to ` 1.7976931348623157E+308 ` . The
default value is ` NAN ` .

###  ` SEASONALITIES `

**Syntax**

` SEASONALITIES =  string_array  `

**Description**

The seasonality of the time series data refers to the presence of variations
that occur at certain regular intervals such as weekly, monthly or quarterly.
Specifying the seasonality helps the model more accurately learn and predict
the cyclic trends in your data. The input seasonality you provide as an
argument is ignored if it's more granular than the finest seasonal granularity
detected in the time series data. For example, if you input ` ['DAILY',
'WEEKLY', 'MONTHLY'] ` for this option, but input time series contains weekly
data, then the ` DAILY ` variable is ignored during the model training.

**Arguments**

An ` ARRAY<STRING> ` value. The following string values are accepted:

* ` AUTO ` : This is the default. The training process automatically infers the seasonalities by data frequency.
* ` NO_SEASONALITY ` : Deactivates automatic seasonality detection.
* ` DAILY `
* ` WEEKLY `
* ` MONTHLY `
* ` QUARTERLY `
* ` YEARLY `

You can only use the ` NO_SEASONALITY ` or ` AUTO ` values by themselves. For
example, ` ['NO_SEASONALITY', 'DAILY'] ` isn't a valid value for this option.

###  ` HIERARCHICAL_TIME_SERIES_COLS `

**Syntax**

` HIERARCHICAL_TIME_SERIES_COLS = {  string_array  } `

**Description**

The column names used to generate hierarchical time series forecasts. Specify
one or more values for this option to aggregate and roll up values for all
time series. The column order represents the hierarchy structure, where the
left-most column is the parent. The columns must be of one of the following
data types:

* ` STRING `
* ` INT64 `

**Arguments**

An ` ARRAY<STRING> ` value.

###  ` AS `

All time series forecasting models support the following ` AS ` clause syntax
for specifying the training data:



AS query_statement


For time series forecasting models that have a ` DATA_FREQUENCY ` value of
either ` DAILY ` or ` AUTO_FREQUENCY ` , you can optionally use the following
` AS ` clause syntax to perform [ custom holiday modeling
](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-time-
series#custom_holidays) in addition to specifying the training data:



AS (
training_data AS (query_statement),
custom_holiday AS (holiday_statement)
)


####  ` query_statement `

The ` query_statement ` argument specifies the query that is used to generate
the training data. For information about the supported SQL syntax of the `
query_statement ` clause, see [ GoogleSQL query syntax
](/bigquery/docs/reference/standard-sql/query-syntax#sql-syntax) .

####  ` holiday_statement `

The ` holiday_statement ` argument specifies the query that provides custom
holiday modeling information for time series forecast models. This query must
return 50,000 rows or less and must contain the following columns:

* ` region ` : Required. A ` STRING ` value that identifies the region to target for holiday modeling. Use one of the following options:

* An upper-case [ holiday region code ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-time-series#holiday_region) . Use this option to overwrite or supplement the holidays for the specified region. You can see the holidays for a region by running ` SELECT * FROM bigquery-public-data.ml_datasets.holidays_and_events_for_forecasting WHERE region =  region  ` .
* An arbitrary string. Use this option to specify a custom region that you want to model holidays for. For example, you could specify ` London ` if you are only modeling holidays for that city.

Be sure not to use an existing holiday region code when you are trying to
model for a custom region. For example, if you want to model a holiday in
California, and specify ` CA ` as the ` region ` value, the service recognizes
that as the holiday region code for Canada and targets that region. Because
the argument is case-sensitive, you could specify ` ca ` , ` California ` , or
some other value that isn't a holiday region code.

* ` holiday_name ` : Required. A ` STRING ` value that identifies the holiday to target for holiday modeling. Use one of the following options:

* The holiday name as it is represented in the ` bigquery-public-data.ml_datasets.holidays_and_events_for_forecasting ` public table, including case. Use this option to  overwrite  or  supplement  the specified holiday.
* A string that represents a custom holiday. The string must be a valid column name so that it can be used in ` ML.EXPLAIN_FORECAST ` output. For example, it cannot contain space. For more information on column naming, see [ Column names ](/bigquery/docs/schemas#column_names) .
* ` primary_date ` : Required. A ` DATE ` value that specifies the date the holiday falls on.

* ` preholiday_days ` : Optional. An ` INT64 ` value that specifies the start of the holiday window around the holiday that is taken into account when modeling. Must be greater than or equal to ` 1 ` . Defaults to ` 1 ` .

* ` postholiday_days ` : Optional. An ` INT64 ` value that specifies the end of the holiday window around the holiday that is taken into account when modeling. Must be greater than or equal to ` 1 ` . Defaults to ` 1 ` .

The ` preholiday_days ` and ` postholiday_days ` arguments together describe
the holiday window around the holiday that is taken into account when
modeling. The holiday window is defined as ` [primary_date - preholiday_days,
primary_date + postholiday_days] ` and is inclusive of the pre- and post-
holiday days. The sum of the values for these arguments must be less than `
365 ` and must be the same across the given holiday. For example, if you are
modeling Arbor Day for several different years, you must specify the same
holiday window for all of those years.

To achieve the best holiday modeling result, provide as much historical and
forecast information about the occurrences of each included holiday as
possible. For example, if you have time series data from 2018 to 2022 and
would like to forecast for 2023, you get the best result by providing the
custom holiday information for all of those years, similar to the following:



CREATE OR REPLACE MODEL `mydataset.arima_model`
OPTIONS (
model_type = 'ARIMA_PLUS',
holiday_region = 'US',...) AS (
training_data AS (SELECT * FROM `mydataset.timeseries_data`),
custom_holiday AS (
SELECT
'US' AS region,
'Halloween' AS holiday_name,
primary_date,
5 AS preholiday_days,
1 AS postholiday_days
FROM
UNNEST(
[
DATE('2018-10-31'),
DATE('2019-10-31'),
DATE('2020-10-31'),
DATE('2021-10-31'),
DATE('2022-10-31'),
DATE('2023-10-31')])
AS primary_date
)
)


##  Holiday data

When you perform holiday modeling by specifying the ` HOLIDAY_REGION ` option,
the model uses holiday data from the region or regions you specify. For
example, the following table describes the holiday data used in the ` US `
region for the year 2022-2023.

* ` region ` specifies the geographic region to which the holiday applies. The supported regions are listed in  ` HOLIDAY_REGION ` .
* ` holiday_name ` contains the name of the holiday.
* ` primary_date ` specifies the date of the holiday. For holidays that span multiple days, this is usually the first day of the holiday.
* ` preholiday_days ` describes the number of days the holiday effect starts before the ` primary_date ` value.
* ` postholiday_days ` describes the number of days the holiday effect ends after the ` primary_date ` value.

region  |  holiday_name  |  primary_date  |  preholiday_days  |
postholiday_days
---|---|---|---|---
US  |  Christmas  |  2022-12-25  |  10  |  1
US  |  Christmas  |  2023-12-25  |  10  |  1
US  |  MothersDay  |  2022-05-08  |  6  |  1
US  |  MothersDay  |  2023-05-14  |  6  |  1
US  |  NewYear  |  2022-01-01  |  5  |  3
US  |  NewYear  |  2023-01-01  |  5  |  3
US  |  DaylightSavingEnd  |  2022-11-06  |  1  |  1
US  |  DaylightSavingEnd  |  2023-11-05  |  1  |  1
US  |  DaylightSavingStart  |  2022-03-13  |  1  |  1
US  |  DaylightSavingStart  |  2023-03-12  |  1  |  1
US  |  Thanksgiving  |  2022-11-24  |  3  |  5
US  |  Thanksgiving  |  2023-11-23  |  3  |  5
US  |  Valentine  |  2022-02-14  |  3  |  1
US  |  Valentine  |  2023-02-14  |  3  |  1
US  |  EasterMonday  |  2022-04-18  |  8  |  1
US  |  EasterMonday  |  2023-04-10  |  8  |  1
US  |  Halloween  |  2022-10-31  |  1  |  1
US  |  Halloween  |  2023-10-31  |  1  |  1
US  |  StPatrickDay  |  2022-03-17  |  1  |  1
US  |  StPatrickDay  |  2023-03-17  |  1  |  1
US  |  ColumbusDay  |  2022-10-10  |  1  |  1
US  |  ColumbusDay  |  2023-10-09  |  1  |  1
US  |  IndependenceDay  |  2022-07-04  |  1  |  1
US  |  IndependenceDay  |  2023-07-04  |  1  |  1
US  |  Juneteenth  |  2022-06-19  |  1  |  1
US  |  Juneteenth  |  2023-06-19  |  1  |  1
US  |  LaborDay  |  2022-09-05  |  1  |  1
US  |  LaborDay  |  2023-09-04  |  1  |  1
US  |  MemorialDay  |  2022-05-30  |  1  |  1
US  |  MemorialDay  |  2023-05-29  |  1  |  1
US  |  MLKDay  |  2022-01-17  |  1  |  1
US  |  MLKDay  |  2023-01-16  |  1  |  1
US  |  PresidentDay  |  2022-02-21  |  1  |  1
US  |  PresidentDay  |  2023-02-20  |  1  |  1
US  |  Superbowl  |  2022-02-13  |  1  |  1
US  |  Superbowl  |  2023-02-05  |  1  |  1
US  |  VeteranDay  |  2022-11-11  |  1  |  1
US  |  VeteranDay  |  2023-11-11  |  1  |  1

You can also see the holidays for a region by running ` SELECT * FROM
bigquery-public-data.ml_datasets.holidays_and_events_for_forecasting WHERE
region =  region  ` .

The ` bigquery-public-data.ml_datasets.holidays_and_events_for_forecasting `
table only contains holidays and events from the following regions:

* ` AU ` : Australia
* ` CA ` : Canada
* ` CH ` : Switzerland
* ` CL ` : Chile
* ` CZ ` : Czechia
* ` DE ` : Germany
* ` DK ` : Denmark
* ` EMEA ` : Europe, the Middle East and Africa
* ` ES ` : Spain
* ` FR ` : France
* ` GB ` : United Kingdom
* ` GLOBAL `
* ` ID ` : Indonesia
* ` IN ` : India
* ` IT ` : Italy
* ` JAPAC ` : Japan and Asia Pacific
* ` JP ` : Japan
* ` KR ` : South Korea
* ` LAC ` : Latin America and the Caribbean
* ` MX ` : Mexico
* ` MY ` : Malaysia
* ` NA ` : North America
* ` NL ` : Netherlands
* ` NZ ` : New Zealand
* ` PT ` : Portugal
* ` SK ` : Slovakia
* ` US ` : United States
* ` ZA ` : South Africa

##  Custom holidays

You can combine use of the  ` holiday_statement ` argument  and the  `
HOLIDAY_REGION ` option  to enable several different custom holiday scenarios,
as described in the following sections.

###  Supplement built-in holidays with additional custom holidays

To model one or more custom holidays in addition to a region's built-in
holidays, specify the target holiday region with the ` HOLIDAY_REGION `
option, and then provide the new holiday metadata in the ` holiday_statement `
argument.

The following example models all built-in holidays for the ` US ` holiday
region, and additionally models the custom holiday ` members_day ` :



CREATE OR REPLACE MODEL `mydataset.arima_model`
OPTIONS (
model_type = 'ARIMA_PLUS',
holiday_region = 'US',...) AS (
training_data AS (SELECT * FROM `mydataset`.timeseries_data`),
custom_holiday AS (
SELECT
'US' AS region,
'members_day' AS holiday_name,
primary_date,
2 AS preholiday_days,
2 AS postholiday_days
FROM
UNNEST(
[
DATE('2016-06-15'),
DATE('2017-06-07'),
DATE('2018-06-06')])
AS primary_date
)
);


###  Model only custom holidays

To model only custom holidays, don't specify a value for the ` HOLIDAY_REGION
` option, and provide the new holiday metadata in the ` holiday_statement `
argument.

The following example models only the custom holiday ` members_day ` for the `
US ` holiday region:



CREATE OR REPLACE MODEL `mydataset.arima_model`
OPTIONS (
model_type = 'ARIMA_PLUS',
-- Don't specify HOLIDAY_REGION
...) AS (
training_data AS (SELECT * FROM `mydataset.timeseries_data`),
custom_holiday AS (
SELECT
'US' AS region,
'members_day' AS holiday_name,
primary_date,
2 AS preholiday_days,
2 AS postholiday_days
FROM
UNNEST(
[
DATE('2016-06-15'),
DATE('2017-06-07'),
DATE('2018-06-06')])
AS primary_date
)
);


###  Change the metadata for built-in holidays

You can change the primary date and holiday effect window used by the model
for one or more built-in holidays. To do this, specify the target holiday
region with the ` HOLIDAY_REGION ` option, and then provide the modified
holiday metadata in the ` holiday_statement ` argument.

The following example models all built-in holidays for the ` US ` holiday
region, but models 3 years of the ` EasterMonday ` holiday with a 3-day
holiday effect window instead of the default 9-day holiday effect window:



OPTIONS (
model_type = 'ARIMA_PLUS',
holiday_region = 'US',...) AS (
training_data AS (SELECT * FROM `mydataset.timeseries_data`),
custom_holiday AS (
SELECT
'US' AS region,
'EasterMonday' AS holiday_name,
primary_date,
1 AS preholiday_days,
1 AS postholiday_days
FROM
UNNEST(
[
DATE('2016-03-28'),
DATE('2017-04-17'),
DATE('2018-04-02')])
AS primary_date
)
);


###  Model a subset of built-in holidays

To model only a subset of built-in holidays, don't specify a value for the `
HOLIDAY_REGION ` option, and provide a query based on the ` bigquery-public-
data.ml_datasets.holidays_and_events_for_forecasting ` public table to specify
the set of holidays to model.

The following example models all built-in holidays for the ` US ` holiday
region except for the ` Christmas ` and ` NewYears ` holidays:



CREATE OR REPLACE MODEL `mydataset.arima_model`
OPTIONS (
model_type = 'ARIMA_PLUS',
-- Don't specify HOLIDAY_REGION
...) AS (
training_data AS (SELECT * FROM `mydataset.timeseries_data`),
custom_holiday AS (
SELECT *
FROM `bigquery-public-data.ml_datasets.holiday`
WHERE
region = 'US'
AND (holiday_name != 'Christmas' OR holiday_name != 'NewYear')
)
);


###  Custom holiday limitations

* Custom holiday modeling only works for models that have a ` data_frequency ` value of either ` DAILY ` or ` AUTO_FREQUENCY ` . If you use ` AUTO_FREQUENCY ` , the actual frequency of the time series data needs to be daily.
* You can't use the [ ` TRANSFORM ` clause ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create#transform) of the ` CREATE MODEL ` statement if you are performing custom holiday modeling.
* Custom holiday modeling uses an algorithm that automatically detects the significance of the holiday effect within the provided holiday effect window, and only extracts the holiday effect on the days that the algorithm classifies as significant. For example, if ` primary date ` is ` 01/02 ` with ` preholiday_days ` and ` postholiday_days ` set to ` 1 ` , the algorithm analyzes the holiday effect for these three days: ` [01/01, 01/02, 01/03] ` . In the ` ML.EXPLAIN_FORECAST ` output, it is not guaranteed that all three of these days will have a holiday effect. Only those days within this window that have a significant holiday effect are associated with a non-zero holiday effect in the output.

##  Limitations

` ARIMA_PLUS ` models have the following limitations:

* For the input time series, the maximum length is 1,000,000 time points and the minimum length is 3 time points. When forecasting multiple time series at the same time, the limit applies to each time series.
* The maximum number of time series to forecast simultaneously using the ID columns is 100,000,000.
* When forecasting multiple time series simultaneously using the ID column, any invalid time series that fail the model fitting are ignored and don't appear in the results of forecast. For example, a single point time series. A warning message is shown in this case, and you can use the [ ` ML.ARIMA_EVALUATE ` function ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-arima-evaluate) to retrieve the error message.
* The maximum time points to forecast is 10,000.
* Holiday effect modeling is effective only for approximately 5 years.
* After a multiple time series model is trained, the evaluation tab in the BigQuery page on the Google Cloud console only shows the evaluation metrics for the first 100 time series. To see the evaluation metrics for all of the time series, use the [ ` ML.ARIMA_EVALUATE ` function ](/bigquery/docs/reference/standard-sql/bigqueryml-syntax-arima-evaluate) .

##  Examples

The following examples show how to create different types of ` ARIMA_PLUS `
time series models.

###  Forecast a single time series

This example shows how to create a time series model that forecasts a single
time series:



CREATE MODEL `project_id.mydataset.mymodel`
OPTIONS(MODEL_TYPE='ARIMA_PLUS',
time_series_timestamp_col='date',
time_series_data_col='transaction') AS
SELECT
date,
transaction
FROM
`mydataset.mytable`


###  Forecast multiple time series

This example shows how to create multiple time series models, one for each
input time series:



CREATE MODEL `project_id.mydataset.mymodel`
OPTIONS(MODEL_TYPE='ARIMA_PLUS',
time_series_timestamp_col='date',
time_series_data_col='transaction',
time_series_id_col='company_name') AS
SELECT
date,
transaction,
company_name
FROM
`mydataset.mytable`


###  Forecast multiple time series using multiple time series ID columns

This example shows how to create multiple time series models for multiple IDs:



CREATE MODEL `project_id.mydataset.mymodel`
OPTIONS(MODEL_TYPE='ARIMA_PLUS',
time_series_timestamp_col='date',
time_series_data_col='transaction',
time_series_id_col=['company_name', 'department_name']) AS
SELECT
date,
transaction,
company_name,
department_name
FROM
`mydataset.mytable`


###  Forecast multiple time series more quickly by using a fraction of the
time points

This example shows how to create multiple time series models while improving
training speed by using the ` TIME_SERIES_LENGTH_FRACTION ` and `
MIN_TIME_SERIES_LENGTH ` options:



CREATE MODEL `project_id.mydataset.mymodel`
OPTIONS(MODEL_TYPE='ARIMA_PLUS',
time_series_timestamp_col='date',
time_series_data_col='transaction',
time_series_id_col=['company_name', 'department_name'],
time_series_length_fraction=0.5,
min_time_series_length=30) AS
SELECT
date,
transaction,
company_name,
department_name
FROM
`mydataset.mytable`


###  Forecast multiple time series more quickly by defining a maximum number
of time points

This example shows how to create multiple time series models while improving
training speed by using ` MAX_TIME_SERIES_LENGTH ` option:



CREATE MODEL `project_id.mydataset.mymodel`
OPTIONS(MODEL_TYPE='ARIMA_PLUS',
time_series_timestamp_col='date',
time_series_data_col='transaction',
time_series_id_col=['company_name', 'department_name'],
max_time_series_length=50) AS
SELECT
date,
transaction,
company_name,
department_name
FROM
`mydataset.mytable`


##  What's next

* Try the following tutorials to learn more about creating time series models:
* [ Perform single time series forecasting from Google Analytics data ](/bigquery/docs/arima-single-time-series-forecasting-tutorial)
* [ Perform multiple time series forecasting with a single query from NYC Citi Bike trips data ](/bigquery/docs/arima-multiple-time-series-forecasting-tutorial)
* [ Scalable forecasting with millions of time series in BigQuery ](/bigquery/docs/arima-speed-up-tutorial)
* [ Use custom holidays in a time series forecasting model ](/bigquery/docs/time-series-forecasting-holidays-tutorial)
* [ Limit forecasted values for a time series model ](/bigquery/docs/arima-time-series-forecasting-with-limits-tutorial)
* [ Hierarchical time series forecasting ](/bigquery/docs/arima-time-series-forecasting-with-hierarchical-time-series)
* [ Explore a notebook solution that helps you build a time series demand forecasting model ](https://github.com/GoogleCloudPlatform/analytics-componentized-patterns/tree/master/retail/time-series/bqml-demand-forecasting) .
* Learn how to [ analyze your data in Google Cloud using Gemini ](/duet-ai/docs/discover/analyze-data-duet-ai) .

Send feedback

Except as otherwise noted, the content of this page is licensed under the [
Creative Commons Attribution 4.0 License
](https://creativecommons.org/licenses/by/4.0/) , and code samples are
licensed under the [ Apache 2.0 License
](https://www.apache.org/licenses/LICENSE-2.0) . For details, see the [ Google
Developers Site Policies ](https://developers.google.com/site-policies) . Java
is a registered trademark of Oracle and/or its affiliates.

Last updated 2024-04-29 UTC.

[{ "type": "thumb-down", "id": "hardToUnderstand", "label":"Hard to
understand" },{ "type": "thumb-down", "id":
"incorrectInformationOrSampleCode", "label":"Incorrect information or sample
code" },{ "type": "thumb-down", "id": "missingTheInformationSamplesINeed",
"label":"Missing the information/samples I need" },{ "type": "thumb-down",
"id": "otherDown", "label":"Other" }]  [{ "type": "thumb-up", "id":
"easyToUnderstand", "label":"Easy to understand" },{ "type": "thumb-up", "id":
"solvedMyProblem", "label":"Solved my problem" },{ "type": "thumb-up", "id":
"otherUp", "label":"Other" }]  Need to tell us more?

* ###  Why Google

* [ Choosing Google Cloud ](/why-google-cloud/)
* [ Trust and security ](/trust-center/)
* [ Open cloud ](/open-cloud/)
* [ Multicloud ](/multicloud/)
* [ Global infrastructure ](/infrastructure/)
* [ Customers and case studies ](/customers/)
* [ Analyst reports ](/analyst-reports/)
* [ Whitepapers ](/whitepapers/)
* [ Blog ](//cloud.google.com/blog/)
* ###  Products and pricing

* [ Google Cloud pricing ](/pricing/)
* [ Google Workspace pricing ](//workspace.google.com/pricing.html)
* [ See all products ](/products/)
* ###  Solutions

* [ Infrastructure modernization ](/solutions/infrastructure-modernization/)
* [ Databases ](/solutions/databases/)
* [ Application modernization ](/solutions/application-modernization/)
* [ Smart analytics ](/solutions/smart-analytics/)
* [ Artificial Intelligence ](/solutions/ai/)
* [ Security ](/solutions/security/)
* [ Productivity & work transformation ](https://workspace.google.com/enterprise/)
* [ Industry solutions ](/solutions/#industry-solutions)
* [ DevOps solutions ](/solutions/devops/)
* [ Small business solutions ](/solutions/#section-14)
* [ See all solutions ](/solutions/)
* ###  Resources

* [ Google Cloud documentation ](/docs/)
* [ Google Cloud quickstarts ](/docs/get-started/)
* [ Google Cloud Marketplace ](/marketplace/)
* [ Learn about cloud computing ](/discover/)
* [ Support ](/support-hub/)
* [ Code samples ](/docs/samples)
* [ Cloud Architecture Center ](/architecture/)
* [ Training ](/learn/training/)
* [ Certifications ](/learn/certification/)
* [ Google for Developers ](//developers.google.com)
* [ Google Cloud for Startups ](/startup/)
* [ System status ](//status.cloud.google.com)
* [ Release Notes ](/release-notes)
* ###  Engage

* [ Contact sales ](/contact/)
* [ Find a Partner ](//cloud.google.com/find-a-partner)
* [ Become a Partner ](/partners/become-a-partner/)
* [ Events ](/events/)
* [ Podcasts ](/podcasts/)
* [ Developer Center ](/developers/)
* [ Press Corner ](https://www.googlecloudpresscorner.com/)
* [ Google Cloud on YouTube ](//www.youtube.com/googlecloud)
* [ Google Cloud Tech on YouTube ](//www.youtube.com/googlecloudplatform)
* [ Follow on X ](//x.com/googlecloud)
* [ Join User Research ](//userresearch.google.com/?reserved=1&utm_source=website&Q_Language=en&utm_medium=own_srch&utm_campaign=CloudWebFooter&utm_term=0&utm_content=0&productTag=clou&campaignDate=jul19&pType=devel&referral_code=jk212693)
* [ We're hiring. Join Google Cloud! ](//careers.google.com/cloud)
* [ Google Cloud Community ](https://www.googlecloudcommunity.com/)

* [ About Google ](//about.google/)
* [ Privacy ](//policies.google.com/privacy)
* [ Site terms ](//www.google.com/intl/en/policies/terms/regional.html)
* [ Google Cloud terms ](/product-terms/)
* Manage cookies
* [ Our third decade of climate action: join us ](/sustainability)
* Sign up for the Google Cloud newsletter  [ Subscribe ](/newsletter/)

* English
* Deutsch
* Español – América Latina
* Français
* Português – Brasil
* 中文 – 简体
* 日本語
* 한국어

