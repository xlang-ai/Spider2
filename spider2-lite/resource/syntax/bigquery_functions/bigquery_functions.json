[
    {
        "category": "aead-encryption-functions",
        "description": "GoogleSQL for BigQuery supports the following AEAD encryption functions. For a description of how the AEAD encryption functions work, see [ AEAD encryption concepts ](/bigquery/docs/aead-encryption-concepts) .",
        "source": "aead_encryption_functions.txt",
        "functions": {
            "AEAD.DECRYPT_BYTES": {
                "name": "AEAD.DECRYPT_BYTES",
                "summary": "Uses the matching key from a keyset to decrypt a `\nBYTES ` ciphertext.",
                "description": "AEAD.DECRYPT_BYTES(keyset, ciphertext, additional_data)\n\n**Description**\n\nUses the matching key from ` keyset ` to decrypt ` ciphertext ` and verifies the integrity of the data using ` additional_data ` . Returns an error if decryption or verification fails.\n\n` keyset ` is a serialized ` BYTES ` value returned by one of the ` KEYS `\nfunctions or a ` STRUCT ` returned by ` KEYS.KEYSET_CHAIN ` . ` keyset ` must contain the key that was used to encrypt ` ciphertext ` , and the key must be in an ` 'ENABLED' ` state, or else the function returns an error. `\nAEAD.DECRYPT_BYTES ` identifies the matching key in ` keyset ` by finding the key with the key ID that matches the one encrypted in ` ciphertext ` .\n\n` ciphertext ` is a ` BYTES ` value that is the result of a call to `\nAEAD.ENCRYPT ` where the input ` plaintext ` was of type ` BYTES ` .\n\nIf ` ciphertext ` includes an initialization vector (IV), it should be the first bytes of ` ciphertext ` . If ` ciphertext ` includes an authentication tag, it should be the last bytes of ` ciphertext ` . If the IV and authentic tag are one (SIV), it should be the first bytes of ` ciphertext ` . The IV and authentication tag commonly require 16 bytes, but may vary in size.\n\n` additional_data ` is a ` STRING ` or ` BYTES ` value that binds the ciphertext to its context. This forces the ciphertext to be decrypted in the same context in which it was encrypted. This function casts any ` STRING `\nvalue to ` BYTES ` . This must be the same as the ` additional_data ` provided to ` AEAD.ENCRYPT ` to encrypt ` ciphertext ` , ignoring its type, or else the function returns an error.\n\n**Return Data Type**\n\n` BYTES `\n\n**Example**\n\nThis example creates a table of unique IDs with associated plaintext values and keysets. Then it uses these keysets to encrypt the plaintext values as `\nBYTES ` and store them in a new table. Finally, it uses ` AEAD.DECRYPT_BYTES `\nto decrypt the encrypted values and display them as plaintext.\n\nThe following statement creates a table ` CustomerKeysets ` containing a column of unique IDs, a column of ` AEAD_AES_GCM_256 ` keysets, and a column of favorite animals.\n\n\nCREATE TABLE aead.CustomerKeysets AS SELECT 1 AS customer_id,\nKEYS.NEW_KEYSET('AEAD_AES_GCM_256') AS keyset,\nb'jaguar' AS favorite_animal UNION ALL SELECT 2 AS customer_id,\nKEYS.NEW_KEYSET('AEAD_AES_GCM_256') AS keyset,\nb'zebra' AS favorite_animal UNION ALL SELECT 3 AS customer_id,\nKEYS.NEW_KEYSET('AEAD_AES_GCM_256') AS keyset,\nb'nautilus' AS favorite_animal;\n\nThe following statement creates a table ` EncryptedCustomerData ` containing a column of unique IDs and a column of ciphertext. The statement encrypts the plaintext ` favorite_animal ` using the keyset value from ` CustomerKeysets `\ncorresponding to each unique ID.\n\n\nCREATE TABLE aead.EncryptedCustomerData AS SELECT customer_id,\nAEAD.ENCRYPT(keyset, favorite_animal, CAST(CAST(customer_id AS STRING) AS BYTES)) AS encrypted_animal FROM aead.CustomerKeysets AS ck;\n\nThe following query uses the keysets in the ` CustomerKeysets ` table to decrypt data in the ` EncryptedCustomerData ` table.\n\n\nSELECT ecd.customer_id,\nAEAD.DECRYPT_BYTES( (SELECT ck.keyset FROM aead.CustomerKeysets AS ck WHERE ecd.customer_id = ck.customer_id),\necd.encrypted_animal,\nCAST(CAST(customer_id AS STRING) AS BYTES) ) AS favorite_animal FROM aead.EncryptedCustomerData AS ecd;"
            },
            "AEAD.DECRYPT_STRING": {
                "name": "AEAD.DECRYPT_STRING",
                "summary": "Uses the matching key from a keyset to decrypt a `\nBYTES ` ciphertext into a ` STRING ` plaintext.",
                "description": "AEAD.DECRYPT_STRING(keyset, ciphertext, additional_data)\n\n**Description**\n\nLike  ` AEAD.DECRYPT_BYTES ` , but where ` additional_data ` is of type `\nSTRING ` .\n\n**Return Data Type**\n\n` STRING `"
            },
            "AEAD.ENCRYPT": {
                "name": "AEAD.ENCRYPT",
                "summary": "Encrypts ` STRING ` plaintext, using the primary cryptographic key in a keyset.",
                "description": "AEAD.ENCRYPT(keyset, plaintext, additional_data)\n\n**Description**\n\nEncrypts ` plaintext ` using the primary cryptographic key in ` keyset ` . The algorithm of the primary key must be ` AEAD_AES_GCM_256 ` . Binds the ciphertext to the context defined by ` additional_data ` . Returns ` NULL ` if any input is ` NULL ` .\n\n` keyset ` is a serialized ` BYTES ` value returned by one of the ` KEYS `\nfunctions or a ` STRUCT ` returned by ` KEYS.KEYSET_CHAIN ` .\n\n` plaintext ` is the ` STRING ` or ` BYTES ` value to be encrypted.\n\n` additional_data ` is a ` STRING ` or ` BYTES ` value that binds the ciphertext to its context. This forces the ciphertext to be decrypted in the same context in which it was encrypted. ` plaintext ` and ` additional_data `\nmust be of the same type. ` AEAD.ENCRYPT(keyset, string1, string2) ` is equivalent to ` AEAD.ENCRYPT(keyset, CAST(string1 AS BYTES), CAST(string2 AS BYTES)) ` .\n\nThe output is ciphertext ` BYTES ` . The ciphertext contains a [ Tink-specific\n](https://github.com/google/tink/blob/master/docs/KEY-MANAGEMENT.md) prefix indicating the key used to perform the encryption.\n\n**Return Data Type**\n\n` BYTES `\n\n**Example**\n\nThe following query uses the keysets for each ` customer_id ` in the `\nCustomerKeysets ` table to encrypt the value of the plaintext `\nfavorite_animal ` in the ` PlaintextCustomerData ` table corresponding to that\n` customer_id ` . The output contains a column of ` customer_id ` values and a column of corresponding ciphertext output as ` BYTES ` .\n\n\nWITH CustomerKeysets AS ( SELECT 1 AS customer_id, KEYS.NEW_KEYSET('AEAD_AES_GCM_256') AS keyset UNION ALL SELECT 2, KEYS.NEW_KEYSET('AEAD_AES_GCM_256') UNION ALL SELECT 3, KEYS.NEW_KEYSET('AEAD_AES_GCM_256') ), PlaintextCustomerData AS ( SELECT 1 AS customer_id, 'elephant' AS favorite_animal UNION ALL SELECT 2, 'walrus' UNION ALL SELECT 3, 'leopard'\n) SELECT pcd.customer_id,\nAEAD.ENCRYPT( (SELECT keyset FROM CustomerKeysets AS ck WHERE ck.customer_id = pcd.customer_id),\npcd.favorite_animal,\nCAST(pcd.customer_id AS STRING) ) AS encrypted_animal FROM PlaintextCustomerData AS pcd;"
            },
            "DETERMINISTIC_DECRYPT_BYTES": {
                "name": "DETERMINISTIC_DECRYPT_BYTES",
                "summary": "Uses the matching key from a keyset to decrypt a ` BYTES ` ciphertext, using deterministic AEAD.",
                "description": "DETERMINISTIC_DECRYPT_BYTES(keyset, ciphertext, additional_data)\n\n**Description**\n\nUses the matching key from ` keyset ` to decrypt ` ciphertext ` and verifies the integrity of the data using ` additional_data ` . Returns an error if decryption fails.\n\n` keyset ` is a serialized ` BYTES ` value or a ` STRUCT ` value returned by one of the ` KEYS ` functions. ` keyset ` must contain the key that was used to encrypt ` ciphertext ` , the key must be in an ` 'ENABLED' ` state, and the key must be of type ` DETERMINISTIC_AEAD_AES_SIV_CMAC_256 ` , or else the function returns an error. ` DETERMINISTIC_DECRYPT_BYTES ` identifies the matching key in ` keyset ` by finding the key with the key ID that matches the one encrypted in ` ciphertext ` .\n\n` ciphertext ` is a ` BYTES ` value that is the result of a call to `\nDETERMINISTIC_ENCRYPT ` where the input ` plaintext ` was of type ` BYTES ` .\n\nThe ciphertext must follow Tink's [ wire format\n](https://developers.google.com/tink/wire-format#deterministic_aead) . The first byte of ` ciphertext ` should contain a Tink key version followed by a 4 byte key hint. If ` ciphertext ` includes an initialization vector (IV), it should be the next bytes of ` ciphertext ` . If ` ciphertext ` includes an authentication tag, it should be the last bytes of ` ciphertext ` . If the IV and authentic tag are one (SIV), it should be the first bytes of ` ciphertext\n` . The IV and authentication tag commonly require 16 bytes, but may vary in size.\n\n` additional_data ` is a ` STRING ` or ` BYTES ` value that binds the ciphertext to its context. This forces the ciphertext to be decrypted in the same context in which it was encrypted. This function casts any ` STRING `\nvalue to ` BYTES ` . This must be the same as the ` additional_data ` provided to ` DETERMINISTIC_ENCRYPT ` to encrypt ` ciphertext ` , ignoring its type, or else the function returns an error.\n\n**Return Data Type**\n\n` BYTES `\n\n**Example**\n\nThis example creates a table of unique IDs with associated plaintext values and keysets. Then it uses these keysets to encrypt the plaintext values as `\nBYTES ` and store them in a new table. Finally, it uses `\nDETERMINISTIC_DECRYPT_BYTES ` to decrypt the encrypted values and display them as plaintext.\n\nThe following statement creates a table ` CustomerKeysets ` containing a column of unique IDs, a column of ` DETERMINISTIC_AEAD_AES_SIV_CMAC_256 `\nkeysets, and a column of favorite animals.\n\n\nCREATE TABLE deterministic.CustomerKeysets AS SELECT 1 AS customer_id,\nKEYS.NEW_KEYSET('DETERMINISTIC_AEAD_AES_SIV_CMAC_256') AS keyset,\nb'jaguar' AS favorite_animal UNION ALL SELECT 2 AS customer_id,\nKEYS.NEW_KEYSET('DETERMINISTIC_AEAD_AES_SIV_CMAC_256') AS keyset,\nb'zebra' AS favorite_animal UNION ALL SELECT 3 AS customer_id,\nKEYS.NEW_KEYSET('DETERMINISTIC_AEAD_AES_SIV_CMAC_256') AS keyset,\nb'nautilus' AS favorite_animal;\n\nThe following statement creates a table ` EncryptedCustomerData ` containing a column of unique IDs and a column of ciphertext. The statement encrypts the plaintext ` favorite_animal ` using the keyset value from ` CustomerKeysets `\ncorresponding to each unique ID.\n\n\nCREATE TABLE deterministic.EncryptedCustomerData AS SELECT customer_id,\nDETERMINISTIC_ENCRYPT(ck.keyset, favorite_animal, CAST(CAST(customer_id AS STRING) AS BYTES)) AS encrypted_animal FROM deterministic.CustomerKeysets AS ck;\n\nThe following query uses the keysets in the ` CustomerKeysets ` table to decrypt data in the ` EncryptedCustomerData ` table.\n\n\nSELECT ecd.customer_id,\nDETERMINISTIC_DECRYPT_BYTES( (SELECT ck.keyset FROM deterministic.CustomerKeysets AS ck WHERE ecd.customer_id = ck.customer_id),\necd.encrypted_animal,\nCAST(CAST(ecd.customer_id AS STRING) AS BYTES) ) AS favorite_animal FROM deterministic.EncryptedCustomerData AS ecd;"
            },
            "DETERMINISTIC_DECRYPT_STRING": {
                "name": "DETERMINISTIC_DECRYPT_STRING",
                "summary": "Uses the matching key from a keyset to decrypt a ` BYTES ` ciphertext into a ` STRING ` plaintext, using deterministic AEAD.",
                "description": "DETERMINISTIC_DECRYPT_STRING(keyset, ciphertext, additional_data)\n\n**Description**\n\nLike  ` DETERMINISTIC_DECRYPT_BYTES ` , but where ` plaintext ` is of type `\nSTRING ` .\n\n**Return Data Type**\n\n` STRING `"
            },
            "DETERMINISTIC_ENCRYPT": {
                "name": "DETERMINISTIC_ENCRYPT",
                "summary": "Encrypts ` STRING ` plaintext, using the primary cryptographic key in a keyset, using deterministic AEAD encryption.",
                "description": "DETERMINISTIC_ENCRYPT(keyset, plaintext, additional_data)\n\n**Description**\n\nEncrypts ` plaintext ` using the primary cryptographic key in ` keyset ` using\n[ deterministic AEAD ](https://developers.google.com/tink/deterministic-aead) . The algorithm of the primary key must be `\nDETERMINISTIC_AEAD_AES_SIV_CMAC_256 ` . Binds the ciphertext to the context defined by ` additional_data ` . Returns ` NULL ` if any input is ` NULL ` .\n\n` keyset ` is a serialized ` BYTES ` value or a ` STRUCT ` value returned by one of the ` KEYS ` functions.\n\n` plaintext ` is the ` STRING ` or ` BYTES ` value to be encrypted.\n\n` additional_data ` is a ` STRING ` or ` BYTES ` value that binds the ciphertext to its context. This forces the ciphertext to be decrypted in the same context in which it was encrypted. ` plaintext ` and ` additional_data `\nmust be of the same type. ` DETERMINISTIC_ENCRYPT(keyset, string1, string2) `\nis equivalent to ` DETERMINISTIC_ENCRYPT(keyset, CAST(string1 AS BYTES),\nCAST(string2 AS BYTES)) ` .\n\nThe output is ciphertext ` BYTES ` . The ciphertext contains a [ Tink-specific\n](https://github.com/google/tink/blob/master/docs/KEY-MANAGEMENT.md) prefix indicating the key used to perform the encryption. Given an identical ` keyset\n` and ` plaintext ` , this function returns the same ciphertext each time it is invoked (including across queries).\n\n**Return Data Type**\n\n` BYTES `\n\n**Example**\n\nThe following query uses the keysets for each ` customer_id ` in the `\nCustomerKeysets ` table to encrypt the value of the plaintext `\nfavorite_animal ` in the ` PlaintextCustomerData ` table corresponding to that\n` customer_id ` . The output contains a column of ` customer_id ` values and a column of corresponding ciphertext output as ` BYTES ` .\n\n\nWITH CustomerKeysets AS ( SELECT 1 AS customer_id,\nKEYS.NEW_KEYSET('DETERMINISTIC_AEAD_AES_SIV_CMAC_256') AS keyset UNION ALL SELECT 2, KEYS.NEW_KEYSET('DETERMINISTIC_AEAD_AES_SIV_CMAC_256') UNION ALL SELECT 3, KEYS.NEW_KEYSET('DETERMINISTIC_AEAD_AES_SIV_CMAC_256') ), PlaintextCustomerData AS ( SELECT 1 AS customer_id, 'elephant' AS favorite_animal UNION ALL SELECT 2, 'walrus' UNION ALL SELECT 3, 'leopard'\n) SELECT pcd.customer_id,\nDETERMINISTIC_ENCRYPT( (SELECT keyset FROM CustomerKeysets AS ck WHERE ck.customer_id = pcd.customer_id),\npcd.favorite_animal,\nCAST(pcd.customer_id AS STRING) ) AS encrypted_animal FROM PlaintextCustomerData AS pcd;"
            },
            "KEYS.ADD_KEY_FROM_RAW_BYTES": {
                "name": "KEYS.ADD_KEY_FROM_RAW_BYTES",
                "summary": "Adds a key to a keyset, and return the new keyset as a serialized ` BYTES ` value.",
                "description": "KEYS.ADD_KEY_FROM_RAW_BYTES(keyset, key_type, raw_key_bytes)\n\n**Description**\n\nReturns a serialized keyset as ` BYTES ` with the addition of a key to `\nkeyset ` based on ` key_type ` and ` raw_key_bytes ` .\n\nThe primary cryptographic key remains the same as in ` keyset ` . The expected length of ` raw_key_bytes ` depends on the value of ` key_type ` . The following are supported ` key_types ` :\n\n* ` 'AES_CBC_PKCS' ` : Creates a key for AES decryption using cipher block chaining and PKCS padding. ` raw_key_bytes ` is expected to be a raw key ` BYTES ` value of length 16, 24, or 32; these lengths have sizes of 128, 192, and 256 bits, respectively. GoogleSQL AEAD functions do not support keys of these types for encryption; instead, prefer ` 'AEAD_AES_GCM_256' ` or ` 'AES_GCM' ` keys.\n* ` 'AES_GCM' ` : Creates a key for AES decryption or encryption using [ Galois/Counter Mode ](https://en.wikipedia.org/wiki/Galois/Counter_Mode) . ` raw_key_bytes ` must be a raw key ` BYTES ` value of length 16 or 32; these lengths have sizes of 128 and 256 bits, respectively. When keys of this type are inputs to ` AEAD.ENCRYPT ` , the output ciphertext does not have a Tink-specific prefix indicating which key was used as input.\n\n**Return Data Type**\n\n` BYTES `\n\n**Example**\n\nThe following query creates a table of customer IDs along with raw key bytes,\ncalled ` CustomerRawKeys ` , and a table of unique IDs, called ` CustomerIds `\n. It creates a new ` 'AEAD_AES_GCM_256' ` keyset for each ` customer_id ` ;\nthen it adds a new key to each keyset, using the ` raw_key_bytes ` value corresponding to that ` customer_id ` . The output is a table where each row contains a ` customer_id ` and a keyset in ` BYTES ` , which contains the raw key added using KEYS.ADD_KEY_FROM_RAW_BYTES.\n\n\nWITH CustomerRawKeys AS ( SELECT 1 AS customer_id, b'0123456789012345' AS raw_key_bytes UNION ALL SELECT 2, b'9876543210543210' UNION ALL SELECT 3, b'0123012301230123'\n), CustomerIds AS ( SELECT 1 AS customer_id UNION ALL SELECT 2 UNION ALL SELECT 3 ) SELECT ci.customer_id,\nKEYS.ADD_KEY_FROM_RAW_BYTES( KEYS.NEW_KEYSET('AEAD_AES_GCM_256'),\n'AES_CBC_PKCS',\n(SELECT raw_key_bytes FROM CustomerRawKeys AS crk WHERE crk.customer_id = ci.customer_id) ) AS keyset FROM CustomerIds AS ci;\n\nThe output keysets each contain two things: the primary cryptographic key created using ` KEYS.NEW_KEYSET('AEAD_AES_GCM_256') ` , and the raw key added using ` KEYS.ADD_KEY_FROM_RAW_BYTES ` . If a keyset in the output is used with\n` AEAD.ENCRYPT ` , GoogleSQL uses the primary cryptographic key created using\n` KEYS.NEW_KEYSET('AEAD_AES_GCM_256') ` to encrypt the input plaintext. If the keyset is used with ` AEAD.DECRYPT_STRING ` or ` AEAD.DECRYPT_BYTES ` ,\nGoogleSQL returns the resulting plaintext if either key succeeds in decrypting the ciphertext."
            },
            "KEYS.KEYSET_CHAIN": {
                "name": "KEYS.KEYSET_CHAIN",
                "summary": "Produces a Tink keyset that is encrypted with a Cloud KMS key.",
                "description": "KEYS.KEYSET_CHAIN(kms_resource_name, first_level_keyset)\n\n**Description**\n\nCan be used in place of the ` keyset ` argument to the AEAD and deterministic encryption functions to pass a [ Tink\n](https://github.com/google/tink/blob/master/docs/KEY-MANAGEMENT.md) keyset that is encrypted with a [ Cloud KMS key ](/bigquery/docs/aead-encryption-\nconcepts#cloud_kms_protection) . This function lets you use other AEAD functions without including plaintext keys in a query.\n\nThis function takes the following arguments:\n\n* ` kms_resource_name ` : A ` STRING ` literal that contains the resource path to the Cloud KMS key that's used to decrypt ` first_level_keyset ` . This key must reside in the same Cloud region where this function is executed. A Cloud KMS key looks like this:\n\ngcp-kms://projects/my-project/locations/us/keyRings/my-key-ring/cryptoKeys/my-crypto-key\n\n* ` first_level_keyset ` : A ` BYTES ` literal that represents a [ keyset ](/bigquery/docs/aead-encryption-concepts#keysets) or [ wrapped keyset ](/bigquery/docs/aead-encryption-concepts#wrapped_keysets) .\n\n**Return Data Type**\n\n` STRUCT `\n\n**Example**\n\nThis example creates a table of example data, then shows how to encrypt that data using a wrapped (encrypted) keyset. Finally it shows how to query the encrypted version of the data.\n\nThe following statement creates a table ` RawCustomerData ` containing a column of customer ids and a column of favorite animals.\n\n\nCREATE TABLE aead.RawCustomerData AS SELECT 1 AS customer_id,\nb'jaguar' AS favorite_animal UNION ALL SELECT 2 AS customer_id,\nb'zebra' AS favorite_animal UNION ALL SELECT 3 AS customer_id,\nb'zebra' AS favorite_animal;\n\nThe following statement creates a table ` EncryptedCustomerData ` containing a column of unique IDs and a column of ciphertext. The statement encrypts the plaintext ` favorite_animal ` using the first_level_keyset provided.\n\n\nDECLARE kms_resource_name STRING;\nDECLARE first_level_keyset BYTES;\nSET kms_resource_name = 'gcp-kms://projects/my-project/locations/us/keyRings/my-key-ring/cryptoKeys/my-crypto-key';\nSET first_level_keyset = b'\\012\\044\\000\\107\\275\\360\\176\\264\\206\\332\\235\\215\\304...';\n\nCREATE TABLE aead.EncryptedCustomerData AS SELECT customer_id,\nAEAD.ENCRYPT( KEYS.KEYSET_CHAIN(kms_resource_name, first_level_keyset),\nfavorite_animal,\nCAST(CAST(customer_id AS STRING) AS BYTES) ) AS encrypted_animal FROM aead.RawCustomerData;\n\nThe following query uses the first_level_keyset to decrypt data in the `\nEncryptedCustomerData ` table.\n\n\nDECLARE kms_resource_name STRING;\nDECLARE first_level_keyset BYTES;\nSET kms_resource_name = 'gcp-kms://projects/my-project/locations/us/keyRings/my-key-ring/cryptoKeys/my-crypto-key';\nSET first_level_keyset = b'\\012\\044\\000\\107\\275\\360\\176\\264\\206\\332\\235\\215\\304...';\n\nSELECT customer_id,\nAEAD.DECRYPT_BYTES( KEYS.KEYSET_CHAIN(kms_resource_name, first_level_keyset),\nencrypted_animal,\nCAST(CAST(customer_id AS STRING) AS BYTES) ) AS favorite_animal FROM aead.EncryptedCustomerData;\n\nThe previous two steps also work with the ` DETERMINISTIC_ENCRYPT ` and `\nDETERMINISTIC_DECRYPT_BYTES ` functions. The wrapped keyset must be created using the ` DETERMINISTIC_AEAD_AES_SIV_CMAC_256 ` type.\n\nThe following statement creates a table ` EncryptedCustomerData ` containing a column of unique IDs and a column of ciphertext. The statement encrypts the plaintext ` favorite_animal ` using the first_level_keyset provided. You can see that the ciphertext for ` favorite_animal ` is the same for customers 2 and 3 since their plaintext ` favorite_animal ` is the same.\n\n\nDECLARE kms_resource_name STRING;\nDECLARE first_level_keyset BYTES;\nSET kms_resource_name = 'gcp-kms://projects/my-project/locations/us/keyRings/my-key-ring/cryptoKeys/my-crypto-key';\nSET first_level_keyset = b'\\012\\044\\000\\107\\275\\360\\176\\264\\206\\332\\235\\215\\304...';\n\nCREATE TABLE daead.EncryptedCustomerData AS SELECT customer_id,\nDETERMINISTC_ENCRYPT( KEYS.KEYSET_CHAIN(kms_resource_name, first_level_keyset),\nfavorite_animal,\nCAST(CAST(customer_id AS STRING) AS BYTES) ) AS encrypted_animal FROM daead.RawCustomerData;\n\nThe following query uses the first_level_keyset to decrypt data in the `\nEncryptedCustomerData ` table.\n\n\nDECLARE kms_resource_name STRING;\nDECLARE first_level_keyset BYTES;\nSET kms_resource_name = 'gcp-kms://projects/my-project/locations/us/keyRings/my-key-ring/cryptoKeys/my-crypto-key';\nSET first_level_keyset = b'\\012\\044\\000\\107\\275\\360\\176\\264\\206\\332\\235\\215\\304...';\n\nSELECT customer_id,\nDETERMINISTIC_DECRYPT_BYTES( KEYS.KEYSET_CHAIN(kms_resource_name, first_level_keyset),\nencrypted_animal,\nCAST(CAST(customer_id AS STRING) AS BYTES) ) AS favorite_animal FROM dead.EncryptedCustomerData;"
            },
            "KEYS.KEYSET_FROM_JSON": {
                "name": "KEYS.KEYSET_FROM_JSON",
                "summary": "Converts a ` STRING ` JSON keyset to a serialized\n` BYTES ` value.",
                "description": "KEYS.KEYSET_FROM_JSON(json_keyset)\n\n**Description**\n\nReturns the input ` json_keyset ` ` STRING ` as serialized ` BYTES ` , which is a valid input for other ` KEYS ` and ` AEAD ` functions. The JSON ` STRING\n` must be compatible with the definition of the [ google.crypto.tink.Keyset\n](https://github.com/google/tink/blob/master/proto/tink.proto) protocol buffer message: the JSON keyset should be a JSON object containing objects and name-\nvalue pairs corresponding to those in the \"keyset\" message in the google.crypto.tink.Keyset definition. You can convert the output serialized `\nBYTES ` representation back to a JSON ` STRING ` using ` KEYS.KEYSET_TO_JSON `\n.\n\n**Return Data Type**\n\n` BYTES `\n\n**Example**\n\n` KEYS.KEYSET_FROM_JSON ` takes JSON-formatted ` STRING ` values like the following:\n\n\n{\n\"key\":[\n{\n\"keyData\":{\n\"keyMaterialType\":\"SYMMETRIC\",\n\"typeUrl\":\"type.googleapis.com/google.crypto.tink.AesGcmKey\",\n\"value\":\"GiD80Z8kL6AP3iSNHhqseZGAIvq7TVQzClT7FQy8YwK3OQ==\"\n},\n\"keyId\":3101427138,\n\"outputPrefixType\":\"TINK\",\n\"status\":\"ENABLED\"\n}\n],\n\"primaryKeyId\":3101427138\n}\n\nThe following query creates a new keyset from a JSON-formatted ` STRING ` `\njson_keyset ` :\n\n\nSELECT KEYS.KEYSET_FROM_JSON(json_keyset);\n\nThis returns the ` json_keyset ` serialized as ` BYTES ` , like the following:\n\n\n\\x08\\x9d\\x8e\\x85\\x82\\x09\\x12d\\x0aX\\x0a0 type.googleapis.com/google.crypto.tink.AesGcmKey\\x12\\\"\\x1a qX\\xe4IG\\x87\\x1f\\xde\n\\xe3)+e\\x98\\x0a\\x1c}\\xfe\\x88<\\x12\\xeb\\xc1t\\xb8\\x83\\x1a\\xcd\\xa8\\x97\\x84g\\x18\\x01\n\\x10\\x01\\x18\\x9d\\x8e\\x85\\x82\\x09 \\x01"
            },
            "KEYS.KEYSET_LENGTH": {
                "name": "KEYS.KEYSET_LENGTH",
                "summary": "Gets the number of keys in the provided keyset.",
                "description": "KEYS.KEYSET_LENGTH(keyset)\n\n**Description**\n\nReturns the number of keys in the provided keyset.\n\n**Return Data Type**\n\n` INT64 `\n\n**Example**\n\nThis example references a JSON-formatted STRING called ` json_keyset ` that contains two keys:\n\n\n{\n\"primaryKeyId\":1354994251,\n\"key\":[\n{\n\"keyData\":{\n\"keyMaterialType\":\"SYMMETRIC\",\n\"typeUrl\":\"type.googleapis.com/google.crypto.tink.AesGcmKey\",\n\"value\":\"GiD9sxQRgFj4aYN78vaIlxInjZkG/uvyWSY9a8GN+ELV2Q==\"\n},\n\"keyId\":1354994251,\n\"outputPrefixType\":\"TINK\",\n\"status\":\"ENABLED\"\n}\n],\n\"key\":[\n{\n\"keyData\":{\n\"keyMaterialType\":\"SYMMETRIC\",\n\"typeUrl\":\"type.googleapis.com/google.crypto.tink.AesGcmKey\",\n\"value\":\"PRn76sxQRgFj4aYN00vaIlxInjZkG/uvyWSY9a2bLRm\"\n},\n\"keyId\":852264701,\n\"outputPrefixType\":\"TINK\",\n\"status\":\"DISABLED\"\n}\n]\n}\n\nThe following query converts ` json_keyset ` to a keyset and then returns the number of keys in the keyset:\n\n\nSELECT KEYS.KEYSET_LENGTH(KEYS.KEYSET_FROM_JSON(json_keyset)) as key_count;\n\n/*-----------*\n| key_count |\n+-----------+\n| 2         |\n*-----------*/"
            },
            "KEYS.KEYSET_TO_JSON": {
                "name": "KEYS.KEYSET_TO_JSON",
                "summary": "Gets a JSON ` STRING ` representation of a keyset.",
                "description": "KEYS.KEYSET_TO_JSON(keyset)\n\n**Description**\n\nReturns a JSON ` STRING ` representation of the input ` keyset ` . The returned JSON ` STRING ` is compatible with the definition of the [\ngoogle.crypto.tink.Keyset\n](https://github.com/google/tink/blob/master/proto/tink.proto) protocol buffer message. You can convert the JSON ` STRING ` representation back to ` BYTES `\nusing ` KEYS.KEYSET_FROM_JSON ` .\n\n**Return Data Type**\n\n` STRING `\n\n**Example**\n\nThe following query returns a new ` 'AEAD_AES_GCM_256' ` keyset as a JSON-\nformatted ` STRING ` .\n\n\nSELECT KEYS.KEYSET_TO_JSON(KEYS.NEW_KEYSET('AEAD_AES_GCM_256'));\n\nThe result is a ` STRING ` like the following.\n\n\n{\n\"key\":[\n{\n\"keyData\":{\n\"keyMaterialType\":\"SYMMETRIC\",\n\"typeUrl\":\"type.googleapis.com/google.crypto.tink.AesGcmKey\",\n\"value\":\"GiD80Z8kL6AP3iSNHhqseZGAIvq7TVQzClT7FQy8YwK3OQ==\"\n},\n\"keyId\":3101427138,\n\"outputPrefixType\":\"TINK\",\n\"status\":\"ENABLED\"\n}\n],\n\"primaryKeyId\":3101427138\n}"
            },
            "KEYS.NEW_KEYSET": {
                "name": "KEYS.NEW_KEYSET",
                "summary": "Gets a serialized keyset containing a new key based on the key type.",
                "description": "KEYS.NEW_KEYSET(key_type)\n\n**Description**\n\nReturns a serialized keyset containing a new key based on ` key_type ` . The returned keyset is a serialized ` BYTES ` representation of [\ngoogle.crypto.tink.Keyset\n](https://github.com/google/tink/blob/master/proto/tink.proto) that contains a primary cryptographic key and no additional keys. You can use the keyset with the ` AEAD.ENCRYPT ` , ` AEAD.DECRYPT_BYTES ` , and ` AEAD.DECRYPT_STRING `\nfunctions for encryption and decryption, as well as with the ` KEYS ` group of key- and keyset-related functions.\n\n` key_type ` is a ` STRING ` literal representation of the type of key to create. ` key_type ` cannot be ` NULL ` . ` key_type ` can be:\n\n* ` AEAD_AES_GCM_256 ` : Creates a 256-bit key with the pseudo-random number generator provided by [ boringSSL ](https://boringssl.googlesource.com/boringssl/) . The key uses AES-GCM for encryption and decryption operations.\n* ` DETERMINISTIC_AEAD_AES_SIV_CMAC_256 ` : Creates a 512-bit ` AES-SIV-CMAC ` key, which contains a 256-bit ` AES-CTR ` key and 256-bit ` AES-CMAC ` key. The ` AES-SIV-CMAC ` key is created with the pseudo-random number generator provided by [ boringSSL ](https://boringssl.googlesource.com/boringssl/) . The key uses AES-SIV for encryption and decryption operations.\n\n**Return Data Type**\n\n` BYTES `\n\n**Example**\n\nThe following query creates a keyset for each row in ` CustomerIds ` , which can subsequently be used to encrypt data. Each keyset contains a single encryption key with randomly-generated key data. Each row in the output contains a ` customer_id ` and an ` 'AEAD_AES_GCM_256' ` key in ` BYTES ` .\n\n\nSELECT customer_id, KEYS.NEW_KEYSET('AEAD_AES_GCM_256') AS keyset FROM ( SELECT 1 AS customer_id UNION ALL SELECT 2 UNION ALL SELECT 3 ) AS CustomerIds;"
            },
            "KEYS.NEW_WRAPPED_KEYSET": {
                "name": "KEYS.NEW_WRAPPED_KEYSET",
                "summary": "Creates a new keyset and encrypts it with a Cloud KMS key.",
                "description": "KEYS.NEW_WRAPPED_KEYSET(kms_resource_name, key_type)\n\n**Description**\n\nCreates a new keyset and encrypts it with a [ Cloud KMS key\n](/bigquery/docs/aead-encryption-concepts#cloud_kms_protection) . Returns the\n[ wrapped keyset ](/bigquery/docs/aead-encryption-concepts#wrapped_keysets) as a ` BYTES ` representation of [ google.crypto.tink.Keyset\n](https://github.com/google/tink/blob/master/proto/tink.proto) that contains a primary cryptographic key and no additional keys.\n\nThis function takes the following arguments:\n\n* ` kms_resource_name ` : A ` STRING ` literal representation of the Cloud KMS key. ` kms_resource_name ` cannot be ` NULL ` . The Cloud KMS key must reside in the same Cloud region where this function is executed. A Cloud KMS key looks like this:\n\ngcp-kms://projects/my-project/locations/us/keyRings/my-key-ring/cryptoKeys/my-crypto-key\n\n* ` key_type ` : A ` STRING ` literal representation of the keyset type. ` key_type ` cannot be ` NULL ` but can be one of the following values:\n\n* ` AEAD_AES_GCM_256 ` : Creates a 256-bit key with the pseudo-random number generator provided by [ boringSSL ](https://boringssl.googlesource.com/boringssl/) . The key uses AES-GCM for encryption and decryption operations.\n\n* ` DETERMINISTIC_AEAD_AES_SIV_CMAC_256 ` : Creates a 512-bit ` AES-SIV-CMAC ` key, which contains a 256-bit ` AES-CTR ` key and 256-bit ` AES-CMAC ` key. The ` AES-SIV-CMAC ` key is created with the pseudo-random number generator provided by [ boringSSL ](https://boringssl.googlesource.com/boringssl/) . The key uses AES-SIV for encryption and decryption operations.\n\n**Return Data Type**\n\n` BYTES `\n\n**Example**\n\nPut the following variables above each example query that you run:\n\n\nDECLARE kms_resource_name STRING;\nSET kms_resource_name = 'gcp-kms://projects/my-project/locations/us/keyRings/my-key-ring/cryptoKeys/my-crypto-key';\n\nThe following query creates a wrapped keyset, which contains the ciphertext produced by encrypting a [ Tink\n](https://github.com/google/tink/blob/master/proto/tink.proto) keyset with the specified Cloud KMS key. If you run the query multiple times, it generates multiple wrapped keysets, and each wrapped keyset is unique to each query that is run.\n\n\nSELECT KEYS.NEW_WRAPPED_KEYSET(kms_resource_name, 'AEAD_AES_GCM_256');\n\nMultiple calls to this function with the same arguments in one query returns the same value. For example, the following query only creates one wrapped keyset and returns it for each row in a table called ` my_table ` .\n\n\nSELECT\n*,\nKEYS.NEW_WRAPPED_KEYSET(kms_resource_name, 'AEAD_AES_GCM_256') FROM my_table"
            },
            "KEYS.REWRAP_KEYSET": {
                "name": "KEYS.REWRAP_KEYSET",
                "summary": "Re-encrypts a wrapped keyset with a new Cloud KMS key.",
                "description": "KEYS.REWRAP_KEYSET(source_kms_resource_name, target_kms_resource_name, wrapped_keyset)\n\n**Description**\n\nRe-encrypts a [ wrapped keyset ](/bigquery/docs/aead-encryption-\nconcepts#wrapped_keysets) with a new [ Cloud KMS key ](/bigquery/docs/aead-\nencryption-concepts#cloud_kms_protection) . Returns the wrapped keyset as a `\nBYTES ` representation of [ google.crypto.tink.Keyset\n](https://github.com/google/tink/blob/master/proto/tink.proto) that contains a primary cryptographic key and no additional keys.\n\nWhen this function is used, a wrapped keyset is decrypted by `\nsource_kms_resource_name ` and then re-encrypted by ` target_kms_resource_name\n` . During this process, the decrypted keyset is never visible to customers.\n\nThis function takes the following arguments:\n\n* ` source_kms_resource_name ` : A ` STRING ` literal representation of the Cloud KMS key you want to replace. This key must reside in the same Cloud region where this function is executed. A Cloud KMS key looks like this:\n\ngcp-kms://projects/my-project/locations/us/keyRings/my-key-ring/cryptoKeys/my-crypto-key\n\n* ` target_kms_resource_name ` : A ` STRING ` literal representation of the new Cloud KMS key that you want to use.\n\n* ` wrapped_keyset ` : A ` BYTES ` literal representation of the keyset that you want to re-encrypt.\n\n**Return Data Type**\n\n` BYTES `\n\n**Example**\n\nPut the following variables above each example query that you run:\n\n\nDECLARE source_kms_resource_name STRING;\nDECLARE target_kms_resource_name STRING;\nDECLARE wrapped_keyset BYTES;\nSET source_kms_resource_name = 'gcp-kms://projects/my-project/locations/us/keyRings/my-key-ring/cryptoKeys/my-crypto-key';\nSET target_kms_resource_name = 'gcp-kms://projects/my-project/locations/another-location/keyRings/my-key-ring/cryptoKeys/my-other-crypto-key';\nSET wrapped_keyset = b'\\012\\044\\000\\107\\275\\360\\176\\264\\206\\332\\235\\215\\304...';\n\nThe following query rewraps a wrapped keyset. If you run the query multiple times, it generates multiple wrapped keysets, and each wrapped keyset is unique to each query that is run.\n\n\nSELECT KEYS.REWRAP_KEYSET(source_kms_resource_name, target_kms_resource_name, wrapped_keyset);\n\nMultiple calls to this function with the same arguments in one query returns the same value. For example, the following query only creates one wrapped keyset and returns it for each row in a table called ` my_table ` .\n\n\nSELECT\n*,\nKEYS.REWRAP_KEYSET(source_kms_resource_name, target_kms_resource_name, wrapped_keyset) FROM my_table"
            },
            "KEYS.ROTATE_KEYSET": {
                "name": "KEYS.ROTATE_KEYSET",
                "summary": "Adds a new primary cryptographic key to a keyset,\nbased on the key type.",
                "description": "KEYS.ROTATE_KEYSET(keyset, key_type)\n\n**Description**\n\nAdds a new key to ` keyset ` based on ` key_type ` . This new key becomes the primary cryptographic key of the new keyset. Returns the new keyset serialized as ` BYTES ` .\n\nThe old primary cryptographic key from the input ` keyset ` remains an additional key in the returned keyset.\n\nThe new ` key_type ` must match the key type of existing keys in the ` keyset\n` .\n\n**Return Data Type**\n\n` BYTES `\n\n**Example**\n\nThe following statement creates a table containing a column of unique `\ncustomer_id ` values and ` 'AEAD_AES_GCM_256' ` keysets. Then, it creates a new primary cryptographic key within each keyset in the source table using `\nKEYS.ROTATE_KEYSET ` . Each row in the output contains a ` customer_id ` and an ` 'AEAD_AES_GCM_256' ` keyset in ` BYTES ` .\n\n\nWITH ExistingKeysets AS ( SELECT 1 AS customer_id, KEYS.NEW_KEYSET('AEAD_AES_GCM_256') AS keyset UNION ALL SELECT 2, KEYS.NEW_KEYSET('AEAD_AES_GCM_256') UNION ALL SELECT 3, KEYS.NEW_KEYSET('AEAD_AES_GCM_256') ) SELECT customer_id, KEYS.ROTATE_KEYSET(keyset, 'AEAD_AES_GCM_256') AS keyset FROM ExistingKeysets;"
            },
            "KEYS.ROTATE_WRAPPED_KEYSET": {
                "name": "KEYS.ROTATE_WRAPPED_KEYSET",
                "summary": "Rewraps a keyset and rotates it.",
                "description": "KEYS.ROTATE_WRAPPED_KEYSET(kms_resource_name, wrapped_keyset, key_type)\n\n**Description**\n\nTakes an existing [ wrapped keyset ](/bigquery/docs/aead-encryption-\nconcepts#wrapped_keysets) and returns a rotated and rewrapped keyset. The returned wrapped keyset is a ` BYTES ` representation of [\ngoogle.crypto.tink.Keyset\n](https://github.com/google/tink/blob/master/proto/tink.proto) .\n\nWhen this function is used, the wrapped keyset is decrypted, the new key is added, and then the keyset is re-encrypted. The primary cryptographic key from the input ` wrapped_keyset ` remains as an additional key in the returned keyset. During this rotation process, the decrypted keyset is never visible to customers.\n\nThis function takes the following arguments:\n\n* ` kms_resource_name ` : A ` STRING ` literal representation of the [ Cloud KMS key ](/bigquery/docs/aead-encryption-concepts#cloud_kms_protection) that was used to wrap the wrapped keyset. The Cloud KMS key must reside in the same Cloud region where this function is executed. A Cloud KMS key looks like this:\n\ngcp-kms://projects/my-project/locations/us/keyRings/my-key-ring/cryptoKeys/my-crypto-key\n\n* ` wrapped_keyset ` : A ` BYTES ` literal representation of the existing keyset that you want to work with.\n\n* ` key_type ` : A ` STRING ` literal representation of the keyset type. This must match the key type of existing keys in ` wrapped_keyset ` .\n\n**Return Data Type**\n\n` BYTES `\n\n**Example**\n\nPut the following variables above each example query that you run:\n\n\nDECLARE kms_resource_name STRING;\nDECLARE wrapped_keyset BYTES;\nSET kms_resource_name = 'gcp-kms://projects/my-project/locations/us/keyRings/my-key-ring/cryptoKeys/my-crypto-key';\nSET wrapped_keyset = b'\\012\\044\\000\\107\\275\\360\\176\\264\\206\\332\\235\\215\\304...';\n\nThe following query rotates a wrapped keyset. If you run the query multiple times, it generates multiple wrapped keysets, and each wrapped keyset is unique to each query that is run.\n\n\nSELECT KEYS.ROTATE_WRAPPED_KEYSET(kms_resource_name, wrapped_keyset, 'AEAD_AES_GCM_256');\n\nMultiple calls to this function with the same arguments in one query returns the same value. For example, the following query only creates one wrapped keyset and returns it for each row in a table called ` my_table ` .\n\n\nSELECT\n*,\nKEYS.ROTATE_WRAPPED_KEYSET(kms_resource_name, wrapped_keyset, 'AEAD_AES_GCM_256') FROM my_table"
            }
        }
    },
    {
        "category": "aggregate-functions",
        "description": "GoogleSQL for BigQuery supports the following general aggregate functions. To learn about the syntax for aggregate function calls, see [ Aggregate function calls ](/bigquery/docs/reference/standard-sql/aggregate-function-calls) .",
        "source": "aggregate_functions.txt",
        "functions": {
            "ANY_VALUE": {
                "name": "ANY_VALUE",
                "summary": "Gets an expression for some row.",
                "description": "ANY_VALUE( expression\n[ HAVING { MAX | MIN } expression2 ]\n)\n[ OVER over_clause ]\n\nover_clause:\n{ named_window | ( [ window_specification ] ) }\n\nwindow_specification:\n[ named_window ]\n[ PARTITION BY partition_expression [, ...] ]\n[ ORDER BY expression [ { ASC | DESC }  ] [, ...] ]\n[ window_frame_clause ]\n\n\n**Description**\n\nReturns ` expression ` for some row chosen from the group. Which row is chosen is nondeterministic, not random. Returns ` NULL ` when the input produces no rows. Returns ` NULL ` when ` expression ` is ` NULL ` for all rows in the group.\n\n` ANY_VALUE ` behaves as if ` IGNORE NULLS ` is specified; rows for which `\nexpression ` is ` NULL ` are not considered and won't be selected.\n\nIf the ` HAVING ` clause is included in the ` ANY_VALUE ` function, the ` OVER\n` clause can't be used with this function.\n\nTo learn more about the optional aggregate clauses that you can pass into this function, see [ Aggregate function calls ](/bigquery/docs/reference/standard-\nsql/aggregate-function-calls) .\n\nTo learn more about the ` OVER ` clause and how to use it, see [ Window function calls ](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n**Supported Argument Types**\n\nAny\n\n**Returned Data Types**\n\nMatches the input data type.\n\n**Examples**\n\n\nSELECT ANY_VALUE(fruit) as any_value FROM UNNEST([\"apple\", \"banana\", \"pear\"]) as fruit;\n\n/*-----------*\n| any_value |\n+-----------+\n| apple     |\n*-----------*/\n\n\nSELECT fruit,\nANY_VALUE(fruit) OVER (ORDER BY LENGTH(fruit) ROWS BETWEEN 1 PRECEDING AND CURRENT ROW) AS any_value FROM UNNEST([\"apple\", \"banana\", \"pear\"]) as fruit;\n\n/*--------+-----------*\n| fruit  | any_value |\n+--------+-----------+\n| pear   | pear      |\n| apple  | pear      |\n| banana | apple     |\n*--------+-----------*/\n\n\nWITH Store AS ( SELECT 20 AS sold, \"apples\" AS fruit UNION ALL SELECT 30 AS sold, \"pears\" AS fruit UNION ALL SELECT 30 AS sold, \"bananas\" AS fruit UNION ALL SELECT 10 AS sold, \"oranges\" AS fruit ) SELECT ANY_VALUE(fruit HAVING MAX sold) AS a_highest_selling_fruit FROM Store;\n\n/*-------------------------*\n| a_highest_selling_fruit |\n+-------------------------+\n| pears                   |\n*-------------------------*/\n\n\nWITH Store AS ( SELECT 20 AS sold, \"apples\" AS fruit UNION ALL SELECT 30 AS sold, \"pears\" AS fruit UNION ALL SELECT 30 AS sold, \"bananas\" AS fruit UNION ALL SELECT 10 AS sold, \"oranges\" AS fruit ) SELECT ANY_VALUE(fruit HAVING MIN sold) AS a_lowest_selling_fruit FROM Store;\n\n/*-------------------------*\n| a_lowest_selling_fruit  |\n+-------------------------+\n| oranges                 |\n*-------------------------*/"
            },
            "ARRAY_AGG": {
                "name": "ARRAY_AGG",
                "summary": "Gets an array of values.",
                "description": "ARRAY_AGG(\n[ DISTINCT ]\nexpression\n[ { IGNORE | RESPECT } NULLS ]\n[ ORDER BY key [ { ASC | DESC } ] [, ... ] ]\n[ LIMIT n ]\n)\n[ OVER over_clause ]\n\nover_clause:\n{ named_window | ( [ window_specification ] ) }\n\nwindow_specification:\n[ named_window ]\n[ PARTITION BY partition_expression [, ...] ]\n[ ORDER BY expression [ { ASC | DESC }  ] [, ...] ]\n[ window_frame_clause ]\n\n\n**Description**\n\nReturns an ARRAY of ` expression ` values.\n\nTo learn more about the optional aggregate clauses that you can pass into this function, see [ Aggregate function calls ](/bigquery/docs/reference/standard-\nsql/aggregate-function-calls) .\n\nIf this function is used with the ` OVER ` clause, it's part of a window function call. In a window function call, aggregate function clauses can't be used. To learn more about the ` OVER ` clause and how to use it, see [ Window function calls ](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\nAn error is raised if an array in the final query result contains a ` NULL `\nelement.\n\n**Supported Argument Types**\n\nAll data types except ARRAY.\n\n**Returned Data Types**\n\nARRAY\n\nIf there are zero input rows, this function returns ` NULL ` .\n\n**Examples**\n\n\nSELECT ARRAY_AGG(x) AS array_agg FROM UNNEST([2, 1,-2, 3, -2, 1, 2]) AS x;\n\n/*-------------------------*\n| array_agg               |\n+-------------------------+\n| [2, 1, -2, 3, -2, 1, 2] |\n*-------------------------*/\n\n\nSELECT ARRAY_AGG(DISTINCT x) AS array_agg FROM UNNEST([2, 1, -2, 3, -2, 1, 2]) AS x;\n\n/*---------------*\n| array_agg     |\n+---------------+\n| [2, 1, -2, 3] |\n*---------------*/\n\n\nSELECT ARRAY_AGG(x IGNORE NULLS) AS array_agg FROM UNNEST([NULL, 1, -2, 3, -2, 1, NULL]) AS x;\n\n/*-------------------*\n| array_agg         |\n+-------------------+\n| [1, -2, 3, -2, 1] |\n*-------------------*/\n\n\nSELECT ARRAY_AGG(x ORDER BY ABS(x)) AS array_agg FROM UNNEST([2, 1, -2, 3, -2, 1, 2]) AS x;\n\n/*-------------------------*\n| array_agg               |\n+-------------------------+\n| [1, 1, 2, -2, -2, 2, 3] |\n*-------------------------*/\n\n\nSELECT ARRAY_AGG(x LIMIT 5) AS array_agg FROM UNNEST([2, 1, -2, 3, -2, 1, 2]) AS x;\n\n/*-------------------*\n| array_agg         |\n+-------------------+\n| [2, 1, -2, 3, -2] |\n*-------------------*/\n\n\nWITH vals AS ( SELECT 1 x UNION ALL SELECT -2 x UNION ALL SELECT 3 x UNION ALL SELECT -2 x UNION ALL SELECT 1 x ) SELECT ARRAY_AGG(DISTINCT x ORDER BY x) as array_agg FROM vals;\n\n/*------------*\n| array_agg  |\n+------------+\n| [-2, 1, 3] |\n*------------*/\n\n\nWITH vals AS ( SELECT 1 x, 'a' y UNION ALL SELECT 1 x, 'b' y UNION ALL SELECT 2 x, 'a' y UNION ALL SELECT 2 x, 'c' y ) SELECT x, ARRAY_AGG(y) as array_agg FROM vals GROUP BY x;\n\n/*---------------*\n| x | array_agg |\n+---------------+\n| 1 | [a, b]    |\n| 2 | [a, c]    |\n*---------------*/\n\n\nSELECT x,\nARRAY_AGG(x) OVER (ORDER BY ABS(x)) AS array_agg FROM UNNEST([2, 1, -2, 3, -2, 1, 2]) AS x;\n\n/*----+-------------------------*\n| x  | array_agg               |\n+----+-------------------------+\n| 1  | [1, 1]                  |\n| 1  | [1, 1]                  |\n| 2  | [1, 1, 2, -2, -2, 2]    |\n| -2 | [1, 1, 2, -2, -2, 2]    |\n| -2 | [1, 1, 2, -2, -2, 2]    |\n| 2  | [1, 1, 2, -2, -2, 2]    |\n| 3  | [1, 1, 2, -2, -2, 2, 3] |\n*----+-------------------------*/"
            },
            "ARRAY_CONCAT_AGG": {
                "name": "ARRAY_CONCAT_AGG",
                "summary": "Concatenates arrays and returns a single array as a result.",
                "description": "ARRAY_CONCAT_AGG( expression\n[ ORDER BY key [ { ASC | DESC } ] [, ... ] ]\n[ LIMIT n ]\n)\n\n**Description**\n\nConcatenates elements from ` expression ` of type ` ARRAY ` , returning a single array as a result.\n\nThis function ignores ` NULL ` input arrays, but respects the ` NULL `\nelements in non- ` NULL ` input arrays. An error is raised, however, if an array in the final query result contains a ` NULL ` element. Returns ` NULL `\nif there are zero input rows or ` expression ` evaluates to ` NULL ` for all rows.\n\nTo learn more about the optional aggregate clauses that you can pass into this function, see [ Aggregate function calls ](/bigquery/docs/reference/standard-\nsql/aggregate-function-calls) .\n\n**Supported Argument Types**\n\n` ARRAY `\n\n**Returned Data Types**\n\n` ARRAY `\n\n**Examples**\n\n\nSELECT FORMAT(\"%T\", ARRAY_CONCAT_AGG(x)) AS array_concat_agg FROM ( SELECT [NULL, 1, 2, 3, 4] AS x UNION ALL SELECT NULL UNION ALL SELECT [5, 6]\nUNION ALL SELECT [7, 8, 9]\n);\n\n/*-----------------------------------*\n| array_concat_agg                  |\n+-----------------------------------+\n| [NULL, 1, 2, 3, 4, 5, 6, 7, 8, 9] |\n*-----------------------------------*/\n\n\nSELECT FORMAT(\"%T\", ARRAY_CONCAT_AGG(x ORDER BY ARRAY_LENGTH(x))) AS array_concat_agg FROM ( SELECT [1, 2, 3, 4] AS x UNION ALL SELECT [5, 6]\nUNION ALL SELECT [7, 8, 9]\n);\n\n/*-----------------------------------*\n| array_concat_agg                  |\n+-----------------------------------+\n| [5, 6, 7, 8, 9, 1, 2, 3, 4]       |\n*-----------------------------------*/\n\n\nSELECT FORMAT(\"%T\", ARRAY_CONCAT_AGG(x LIMIT 2)) AS array_concat_agg FROM ( SELECT [1, 2, 3, 4] AS x UNION ALL SELECT [5, 6]\nUNION ALL SELECT [7, 8, 9]\n);\n\n/*--------------------------*\n| array_concat_agg         |\n+--------------------------+\n| [1, 2, 3, 4, 5, 6]       |\n*--------------------------*/\n\n\nSELECT FORMAT(\"%T\", ARRAY_CONCAT_AGG(x ORDER BY ARRAY_LENGTH(x) LIMIT 2)) AS array_concat_agg FROM ( SELECT [1, 2, 3, 4] AS x UNION ALL SELECT [5, 6]\nUNION ALL SELECT [7, 8, 9]\n);\n\n/*------------------*\n| array_concat_agg |\n+------------------+\n| [5, 6, 7, 8, 9]  |\n*------------------*/"
            },
            "AVG": {
                "name": "AVG",
                "summary": "Gets the average of non- ` NULL ` values.",
                "description": "AVG(\n[ DISTINCT ]\nexpression )\n[ OVER over_clause ]\n\nover_clause:\n{ named_window | ( [ window_specification ] ) }\n\nwindow_specification:\n[ named_window ]\n[ PARTITION BY partition_expression [, ...] ]\n[ ORDER BY expression [ { ASC | DESC }  ] [, ...] ]\n[ window_frame_clause ]\n\n\n**Description**\n\nReturns the average of non- ` NULL ` values in an aggregated group.\n\nTo learn more about the optional aggregate clauses that you can pass into this function, see [ Aggregate function calls ](/bigquery/docs/reference/standard-\nsql/aggregate-function-calls) .\n\nThis function can be used with the [ ` AGGREGATION_THRESHOLD ` clause\n](/bigquery/docs/reference/standard-sql/query-syntax#agg_threshold_clause) .\n\nIf this function is used with the ` OVER ` clause, it's part of a window function call. In a window function call, aggregate function clauses can't be used. To learn more about the ` OVER ` clause and how to use it, see [ Window function calls ](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n` AVG ` can be used with differential privacy. For more information, see [\nDifferentially private aggregate functions\n](/bigquery/docs/reference/standard-sql/aggregate-dp-functions) .\n\nCaveats:\n\n* If the aggregated group is empty or the argument is ` NULL ` for all rows in the group, returns ` NULL ` .\n* If the argument is ` NaN ` for any row in the group, returns ` NaN ` .\n* If the argument is ` [+|-]Infinity ` for any row in the group, returns either ` [+|-]Infinity ` or ` NaN ` .\n* If there is numeric overflow, produces an error.\n* If a [ floating-point type ](/bigquery/docs/reference/standard-sql/data-types#floating_point_types) is returned, the result is [ non-deterministic ](/bigquery/docs/reference/standard-sql/data-types#floating-point-semantics) , which means you might receive a different result each time you use this function.\n\n**Supported Argument Types**\n\n* Any numeric input type\n* ` INTERVAL `\n\n**Returned Data Types**\n\nINPUT  |  ` INT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 ` |  `\nINTERVAL `\n---|---|---|---|---|---\nOUTPUT  |  ` FLOAT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 ` |  `\nINTERVAL `\n\n**Examples**\n\n\nSELECT AVG(x) as avg FROM UNNEST([0, 2, 4, 4, 5]) as x;\n\n/*-----*\n| avg |\n+-----+\n| 3   |\n*-----*/\n\n\nSELECT AVG(DISTINCT x) AS avg FROM UNNEST([0, 2, 4, 4, 5]) AS x;\n\n/*------*\n| avg  |\n+------+\n| 2.75 |\n*------*/\n\n\nSELECT x,\nAVG(x) OVER (ORDER BY x ROWS BETWEEN 1 PRECEDING AND CURRENT ROW) AS avg FROM UNNEST([0, 2, NULL, 4, 4, 5]) AS x;\n\n/*------+------*\n| x    | avg  |\n+------+------+\n| NULL | NULL |\n| 0    | 0    |\n| 2    | 1    |\n| 4    | 3    |\n| 4    | 4    |\n| 5    | 4.5  |\n*------+------*/"
            },
            "BIT_AND": {
                "name": "BIT_AND",
                "summary": "Performs a bitwise AND operation on an expression.",
                "description": "BIT_AND( expression )\n\n**Description**\n\nPerforms a bitwise AND operation on ` expression ` and returns the result.\n\nTo learn more about the optional aggregate clauses that you can pass into this function, see [ Aggregate function calls ](/bigquery/docs/reference/standard-\nsql/aggregate-function-calls) .\n\n**Supported Argument Types**\n\n* INT64\n\n**Returned Data Types**\n\nINT64\n\n**Examples**\n\n\nSELECT BIT_AND(x) as bit_and FROM UNNEST([0xF001, 0x00A1]) as x;\n\n/*---------*\n| bit_and |\n+---------+\n| 1       |\n*---------*/"
            },
            "BIT_OR": {
                "name": "BIT_OR",
                "summary": "Performs a bitwise OR operation on an expression.",
                "description": "BIT_OR( expression )\n\n**Description**\n\nPerforms a bitwise OR operation on ` expression ` and returns the result.\n\nTo learn more about the optional aggregate clauses that you can pass into this function, see [ Aggregate function calls ](/bigquery/docs/reference/standard-\nsql/aggregate-function-calls) .\n\n**Supported Argument Types**\n\n* INT64\n\n**Returned Data Types**\n\nINT64\n\n**Examples**\n\n\nSELECT BIT_OR(x) as bit_or FROM UNNEST([0xF001, 0x00A1]) as x;\n\n/*--------*\n| bit_or |\n+--------+\n| 61601  |\n*--------*/"
            },
            "BIT_XOR": {
                "name": "BIT_XOR",
                "summary": "Performs a bitwise XOR operation on an expression.",
                "description": "BIT_XOR(\n[ DISTINCT ]\nexpression )\n\n**Description**\n\nPerforms a bitwise XOR operation on ` expression ` and returns the result.\n\nTo learn more about the optional aggregate clauses that you can pass into this function, see [ Aggregate function calls ](/bigquery/docs/reference/standard-\nsql/aggregate-function-calls) .\n\n**Supported Argument Types**\n\n* INT64\n\n**Returned Data Types**\n\nINT64\n\n**Examples**\n\n\nSELECT BIT_XOR(x) AS bit_xor FROM UNNEST([5678, 1234]) AS x;\n\n/*---------*\n| bit_xor |\n+---------+\n| 4860    |\n*---------*/\n\n\nSELECT BIT_XOR(x) AS bit_xor FROM UNNEST([1234, 5678, 1234]) AS x;\n\n/*---------*\n| bit_xor |\n+---------+\n| 5678    |\n*---------*/\n\n\nSELECT BIT_XOR(DISTINCT x) AS bit_xor FROM UNNEST([1234, 5678, 1234]) AS x;\n\n/*---------*\n| bit_xor |\n+---------+\n| 4860    |\n*---------*/"
            },
            "COUNT": {
                "name": "COUNT",
                "summary": "Gets the number of rows in the input, or the number of rows with an expression evaluated to any value other than ` NULL ` .",
                "description": "1\\.\n\n\nCOUNT(*)\n[OVER over_clause]\n\n2\\.\n\n\nCOUNT(\n[ DISTINCT ]\nexpression )\n[ OVER over_clause ]\n\nover_clause:\n{ named_window | ( [ window_specification ] ) }\n\nwindow_specification:\n[ named_window ]\n[ PARTITION BY partition_expression [, ...] ]\n[ ORDER BY expression [ { ASC | DESC }  ] [, ...] ]\n[ window_frame_clause ]\n\n\n**Description**\n\n1. Returns the number of rows in the input.\n2. Returns the number of rows with ` expression ` evaluated to any value other than ` NULL ` .\n\nTo learn more about the optional aggregate clauses that you can pass into this function, see [ Aggregate function calls ](/bigquery/docs/reference/standard-\nsql/aggregate-function-calls) .\n\nThis function can be used with the [ ` AGGREGATION_THRESHOLD ` clause\n](/bigquery/docs/reference/standard-sql/query-syntax#agg_threshold_clause) .\n\nTo learn more about the ` OVER ` clause and how to use it, see [ Window function calls ](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\nThis function with DISTINCT supports specifying [ collation\n](/bigquery/docs/reference/standard-sql/collation-concepts#collate_about) .\n\n` COUNT ` can be used with differential privacy. For more information, see [\nDifferentially private aggregate functions\n](/bigquery/docs/reference/standard-sql/aggregate-dp-functions) .\n\n**Supported Argument Types**\n\n` expression ` can be any data type. If ` DISTINCT ` is present, ` expression\n` can only be a data type that is [ groupable\n](/bigquery/docs/reference/standard-sql/data-types#data_type_properties) .\n\n**Return Data Types**\n\nINT64\n\n**Examples**\n\nYou can use the ` COUNT ` function to return the number of rows in a table or the number of distinct values of an expression. For example:\n\n\nSELECT COUNT(*) AS count_star,\nCOUNT(DISTINCT x) AS count_dist_x FROM UNNEST([1, 4, 4, 5]) AS x;\n\n/*------------+--------------*\n| count_star | count_dist_x |\n+------------+--------------+\n| 4          | 3            |\n*------------+--------------*/\n\n\nSELECT x,\nCOUNT(*) OVER (PARTITION BY MOD(x, 3)) AS count_star,\nCOUNT(DISTINCT x) OVER (PARTITION BY MOD(x, 3)) AS count_dist_x FROM UNNEST([1, 4, 4, 5]) AS x;\n\n/*------+------------+--------------*\n| x    | count_star | count_dist_x |\n+------+------------+--------------+\n| 1    | 3          | 2            |\n| 4    | 3          | 2            |\n| 4    | 3          | 2            |\n| 5    | 1          | 1            |\n*------+------------+--------------*/\n\n\nSELECT x,\nCOUNT(*) OVER (PARTITION BY MOD(x, 3)) AS count_star,\nCOUNT(x) OVER (PARTITION BY MOD(x, 3)) AS count_x FROM UNNEST([1, 4, NULL, 4, 5]) AS x;\n\n/*------+------------+---------*\n| x    | count_star | count_x |\n+------+------------+---------+\n| NULL | 1          | 0       |\n| 1    | 3          | 3       |\n| 4    | 3          | 3       |\n| 4    | 3          | 3       |\n| 5    | 1          | 1       |\n*------+------------+---------*/\n\nIf you want to count the number of distinct values of an expression for which a certain condition is satisfied, this is one recipe that you can use:\n\n\nCOUNT(DISTINCT IF(condition, expression, NULL))\n\nHere, ` IF ` will return the value of ` expression ` if ` condition ` is `\nTRUE ` , or ` NULL ` otherwise. The surrounding ` COUNT(DISTINCT ...) ` will ignore the ` NULL ` values, so it will count only the distinct values of `\nexpression ` for which ` condition ` is ` TRUE ` .\n\nFor example, to count the number of distinct positive values of ` x ` :\n\n\nSELECT COUNT(DISTINCT IF(x > 0, x, NULL)) AS distinct_positive FROM UNNEST([1, -2, 4, 1, -5, 4, 1, 3, -6, 1]) AS x;\n\n/*-------------------*\n| distinct_positive |\n+-------------------+\n| 3                 |\n*-------------------*/\n\nOr to count the number of distinct dates on which a certain kind of event occurred:\n\n\nWITH Events AS ( SELECT DATE '2021-01-01' AS event_date, 'SUCCESS' AS event_type UNION ALL SELECT DATE '2021-01-02' AS event_date, 'SUCCESS' AS event_type UNION ALL SELECT DATE '2021-01-02' AS event_date, 'FAILURE' AS event_type UNION ALL SELECT DATE '2021-01-03' AS event_date, 'SUCCESS' AS event_type UNION ALL SELECT DATE '2021-01-04' AS event_date, 'FAILURE' AS event_type UNION ALL SELECT DATE '2021-01-04' AS event_date, 'FAILURE' AS event_type ) SELECT COUNT(DISTINCT IF(event_type = 'FAILURE', event_date, NULL)) AS distinct_dates_with_failures FROM Events;\n\n/*------------------------------*\n| distinct_dates_with_failures |\n+------------------------------+\n| 2                            |\n*------------------------------*/"
            },
            "COUNTIF": {
                "name": "COUNTIF",
                "summary": "Gets the count of ` TRUE ` values for an expression.",
                "description": "COUNTIF(\n[ DISTINCT ]\nexpression )\n[ OVER over_clause ]\n\nover_clause:\n{ named_window | ( [ window_specification ] ) }\n\nwindow_specification:\n[ named_window ]\n[ PARTITION BY partition_expression [, ...] ]\n[ ORDER BY expression [ { ASC | DESC }  ] [, ...] ]\n[ window_frame_clause ]\n\n\n**Description**\n\nReturns the count of ` TRUE ` values for ` expression ` . Returns ` 0 ` if there are zero input rows, or if ` expression ` evaluates to ` FALSE ` or `\nNULL ` for all rows.\n\nSince ` expression ` must be a ` BOOL ` , the form ` COUNTIF(DISTINCT ...) `\nis generally not useful: there is only one distinct value of ` TRUE ` . So `\nCOUNTIF(DISTINCT ...) ` will return 1 if ` expression ` evaluates to ` TRUE `\nfor one or more input rows, or 0 otherwise. Usually when someone wants to combine ` COUNTIF ` and ` DISTINCT ` , they want to count the number of distinct values of an expression for which a certain condition is satisfied.\nOne recipe to achieve this is the following:\n\n\nCOUNT(DISTINCT IF(condition, expression, NULL))\n\nNote that this uses ` COUNT ` , not ` COUNTIF ` ; the ` IF ` part has been moved inside. To learn more, see the examples for  ` COUNT ` .\n\nTo learn more about the optional aggregate clauses that you can pass into this function, see [ Aggregate function calls ](/bigquery/docs/reference/standard-\nsql/aggregate-function-calls) .\n\nThis function can be used with the [ ` AGGREGATION_THRESHOLD ` clause\n](/bigquery/docs/reference/standard-sql/query-syntax#agg_threshold_clause) .\n\nTo learn more about the ` OVER ` clause and how to use it, see [ Window function calls ](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n**Supported Argument Types**\n\nBOOL\n\n**Return Data Types**\n\nINT64\n\n**Examples**\n\n\nSELECT COUNTIF(x<0) AS num_negative, COUNTIF(x>0) AS num_positive FROM UNNEST([5, -2, 3, 6, -10, -7, 4, 0]) AS x;\n\n/*--------------+--------------*\n| num_negative | num_positive |\n+--------------+--------------+\n| 3            | 4            |\n*--------------+--------------*/\n\n\nSELECT x,\nCOUNTIF(x<0) OVER (ORDER BY ABS(x) ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) AS num_negative FROM UNNEST([5, -2, 3, 6, -10, NULL, -7, 4, 0]) AS x;\n\n/*------+--------------*\n| x    | num_negative |\n+------+--------------+\n| NULL | 0            |\n| 0    | 1            |\n| -2   | 1            |\n| 3    | 1            |\n| 4    | 0            |\n| 5    | 0            |\n| 6    | 1            |\n| -7   | 2            |\n| -10  | 2            |\n*------+--------------*/"
            },
            "GROUPING": {
                "name": "GROUPING",
                "summary": "Checks if a groupable value in the ` GROUP BY ` clause is aggregated.",
                "description": "GROUPING(groupable_value)\n\n**Description**\n\nIf a groupable item in the [ ` GROUP BY ` clause\n](/bigquery/docs/reference/standard-sql/query-syntax#group_by_clause) is aggregated (and thus not grouped), this function returns ` 1 ` . Otherwise,\nthis function returns ` 0 ` .\n\nDefinitions:\n\n* ` groupable_value ` : An expression that represents a value that can be grouped in the ` GROUP BY ` clause.\n\nDetails:\n\nThe ` GROUPING ` function is helpful if you need to determine which rows are produced by which grouping sets. A grouping set is a group of columns by which rows can be grouped together. So, if you need to filter rows by a few specific grouping sets, you can use the ` GROUPING ` function to identify which grouping sets grouped which rows by creating a matrix of the results.\n\nIn addition, you can use the ` GROUPING ` function to determine the type of `\nNULL ` produced by the ` GROUP BY ` clause. In some cases, the ` GROUP BY `\nclause produces a ` NULL ` placeholder. This placeholder represents all groupable items that are aggregated (not grouped) in the current grouping set.\nThis is different from a standard ` NULL ` , which can also be produced by a query.\n\nFor more information, see the following examples.\n\n**Returned Data Type**\n\n` INT64 `\n\n**Examples**\n\nIn the following example, it's difficult to determine which rows are grouped by the grouping value ` product_type ` or ` product_name ` . The ` GROUPING `\nfunction makes this easier to determine.\n\nPay close attention to what's in the ` product_type_agg ` and `\nproduct_name_agg ` column matrix. This determines how the rows are grouped.\n\n` product_type_agg ` |  ` product_name_agg ` |  Notes\n---|---|---\n1  |  0  |  Rows are grouped by ` product_name ` .\n0  |  1  |  Rows are grouped by ` product_type ` .\n0  |  0  |  Rows are grouped by ` product_type ` and ` product_name ` .\n1  |  1  |  Grand total row.\n\nWITH Products AS ( SELECT 'shirt' AS product_type, 't-shirt' AS product_name, 3 AS product_count UNION ALL SELECT 'shirt', 't-shirt', 8 UNION ALL SELECT 'shirt', 'polo', 25 UNION ALL SELECT 'pants', 'jeans', 6 ) SELECT product_type,\nproduct_name,\nSUM(product_count) AS product_sum,\nGROUPING(product_type) AS product_type_agg,\nGROUPING(product_name) AS product_name_agg,\nFROM Products GROUP BY GROUPING SETS(product_type, product_name, ()) ORDER BY product_name;\n\n/*--------------+--------------+-------------+------------------+------------------+\n| product_type | product_name | product_sum | product_type_agg | product_name_agg |\n+--------------+--------------+-------------+------------------+------------------+\n| NULL         | NULL         | 42          | 1                | 1                |\n| shirt        | NULL         | 36          | 0                | 1                |\n| pants        | NULL         | 6           | 0                | 1                |\n| NULL         | jeans        | 6           | 1                | 0                |\n| NULL         | polo         | 25          | 1                | 0                |\n| NULL         | t-shirt      | 11          | 1                | 0                |\n+--------------+--------------+-------------+------------------+------------------*/\n\nIn the following example, it's difficult to determine if ` NULL ` represents a\n` NULL ` placeholder or a standard ` NULL ` value in the ` product_type `\ncolumn. The ` GROUPING ` function makes it easier to determine what type of `\nNULL ` is being produced. If ` product_type_is_aggregated ` is ` 1 ` , the `\nNULL ` value for the ` product_type ` column is a ` NULL ` placeholder.\n\n\nWITH Products AS ( SELECT 'shirt' AS product_type, 't-shirt' AS product_name, 3 AS product_count UNION ALL SELECT 'shirt', 't-shirt', 8 UNION ALL SELECT NULL, 'polo', 25 UNION ALL SELECT 'pants', 'jeans', 6 ) SELECT product_type,\nproduct_name,\nSUM(product_count) AS product_sum,\nGROUPING(product_type) AS product_type_is_aggregated FROM Products GROUP BY GROUPING SETS(product_type, product_name) ORDER BY product_name;\n\n/*--------------+--------------+-------------+----------------------------+\n| product_type | product_name | product_sum | product_type_is_aggregated |\n+--------------+--------------+-------------+----------------------------+\n| shirt        | NULL         | 11          | 0                          |\n| NULL         | NULL         | 25          | 0                          |\n| pants        | NULL         | 6           | 0                          |\n| NULL         | jeans        | 6           | 1                          |\n| NULL         | polo         | 25          | 1                          |\n| NULL         | t-shirt      | 11          | 1                          |\n+--------------+--------------+-------------+----------------------------*/"
            },
            "LOGICAL_AND": {
                "name": "LOGICAL_AND",
                "summary": "Gets the logical AND of all non- ` NULL ` expressions.",
                "description": "LOGICAL_AND( expression )\n\n**Description**\n\nReturns the logical AND of all non- ` NULL ` expressions. Returns ` NULL ` if there are zero input rows or ` expression ` evaluates to ` NULL ` for all rows.\n\nTo learn more about the optional aggregate clauses that you can pass into this function, see [ Aggregate function calls ](/bigquery/docs/reference/standard-\nsql/aggregate-function-calls) .\n\nThis function can be used with the [ ` AGGREGATION_THRESHOLD ` clause\n](/bigquery/docs/reference/standard-sql/query-syntax#agg_threshold_clause) .\n\n**Supported Argument Types**\n\n` BOOL `\n\n**Return Data Types**\n\n` BOOL `\n\n**Examples**\n\n` LOGICAL_AND ` returns ` FALSE ` because not all of the values in the array are less than 3.\n\n\nSELECT LOGICAL_AND(x < 3) AS logical_and FROM UNNEST([1, 2, 4]) AS x;\n\n/*-------------*\n| logical_and |\n+-------------+\n| FALSE       |\n*-------------*/"
            },
            "LOGICAL_OR": {
                "name": "LOGICAL_OR",
                "summary": "Gets the logical OR of all non- ` NULL ` expressions.",
                "description": "LOGICAL_OR( expression )\n\n**Description**\n\nReturns the logical OR of all non- ` NULL ` expressions. Returns ` NULL ` if there are zero input rows or ` expression ` evaluates to ` NULL ` for all rows.\n\nTo learn more about the optional aggregate clauses that you can pass into this function, see [ Aggregate function calls ](/bigquery/docs/reference/standard-\nsql/aggregate-function-calls) .\n\nThis function can be used with the [ ` AGGREGATION_THRESHOLD ` clause\n](/bigquery/docs/reference/standard-sql/query-syntax#agg_threshold_clause) .\n\n**Supported Argument Types**\n\n` BOOL `\n\n**Return Data Types**\n\n` BOOL `\n\n**Examples**\n\n` LOGICAL_OR ` returns ` TRUE ` because at least one of the values in the array is less than 3.\n\n\nSELECT LOGICAL_OR(x < 3) AS logical_or FROM UNNEST([1, 2, 4]) AS x;\n\n/*------------*\n| logical_or |\n+------------+\n| TRUE       |\n*------------*/"
            },
            "MAX": {
                "name": "MAX",
                "summary": "Gets the maximum non- ` NULL ` value.",
                "description": "MAX( expression )\n[ OVER over_clause ]\n\nover_clause:\n{ named_window | ( [ window_specification ] ) }\n\nwindow_specification:\n[ named_window ]\n[ PARTITION BY partition_expression [, ...] ]\n[ ORDER BY expression [ { ASC | DESC }  ] [, ...] ]\n[ window_frame_clause ]\n\n\n**Description**\n\nReturns the maximum non- ` NULL ` value in an aggregated group.\n\nCaveats:\n\n* If the aggregated group is empty or the argument is ` NULL ` for all rows in the group, returns ` NULL ` .\n* If the argument is ` NaN ` for any row in the group, returns ` NaN ` .\n\nTo learn more about the optional aggregate clauses that you can pass into this function, see [ Aggregate function calls ](/bigquery/docs/reference/standard-\nsql/aggregate-function-calls) .\n\nTo learn more about the ` OVER ` clause and how to use it, see [ Window function calls ](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\nThis function supports specifying [ collation\n](/bigquery/docs/reference/standard-sql/collation-concepts#collate_about) .\n\n**Supported Argument Types**\n\nAny [ orderable data type ](/bigquery/docs/reference/standard-sql/data-\ntypes#data_type_properties) except for ` ARRAY ` .\n\n**Return Data Types**\n\nThe data type of the input values.\n\n**Examples**\n\n\nSELECT MAX(x) AS max FROM UNNEST([8, 37, 55, 4]) AS x;\n\n/*-----*\n| max |\n+-----+\n| 55  |\n*-----*/\n\n\nSELECT x, MAX(x) OVER (PARTITION BY MOD(x, 2)) AS max FROM UNNEST([8, NULL, 37, 55, NULL, 4]) AS x;\n\n/*------+------*\n| x    | max  |\n+------+------+\n| NULL | NULL |\n| NULL | NULL |\n| 8    | 8    |\n| 4    | 8    |\n| 37   | 55   |\n| 55   | 55   |\n*------+------*/"
            },
            "MAX_BY": {
                "name": "MAX_BY",
                "summary": "Synonym for ` ANY_VALUE(x HAVING MAX y) ` .",
                "description": "MAX_BY( x, y )\n\n**Description**\n\nSynonym for  ` ANY_VALUE(x HAVING MAX y) ` .\n\n**Return Data Types**\n\nMatches the input ` x ` data type.\n\n**Examples**\n\n\nWITH fruits AS ( SELECT \"apple\"  fruit, 3.55 price UNION ALL SELECT \"banana\"  fruit, 2.10 price UNION ALL SELECT \"pear\"  fruit, 4.30 price ) SELECT MAX_BY(fruit, price) as fruit FROM fruits;\n\n/*-------*\n| fruit |\n+-------+\n| pear  |\n*-------*/"
            },
            "MIN": {
                "name": "MIN",
                "summary": "Gets the minimum non- ` NULL ` value.",
                "description": "MIN( expression )\n[ OVER over_clause ]\n\nover_clause:\n{ named_window | ( [ window_specification ] ) }\n\nwindow_specification:\n[ named_window ]\n[ PARTITION BY partition_expression [, ...] ]\n[ ORDER BY expression [ { ASC | DESC }  ] [, ...] ]\n[ window_frame_clause ]\n\n\n**Description**\n\nReturns the minimum non- ` NULL ` value in an aggregated group.\n\nCaveats:\n\n* If the aggregated group is empty or the argument is ` NULL ` for all rows in the group, returns ` NULL ` .\n* If the argument is ` NaN ` for any row in the group, returns ` NaN ` .\n\nTo learn more about the optional aggregate clauses that you can pass into this function, see [ Aggregate function calls ](/bigquery/docs/reference/standard-\nsql/aggregate-function-calls) .\n\nTo learn more about the ` OVER ` clause and how to use it, see [ Window function calls ](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\nThis function supports specifying [ collation\n](/bigquery/docs/reference/standard-sql/collation-concepts#collate_about) .\n\n**Supported Argument Types**\n\nAny [ orderable data type ](/bigquery/docs/reference/standard-sql/data-\ntypes#data_type_properties) except for ` ARRAY ` .\n\n**Return Data Types**\n\nThe data type of the input values.\n\n**Examples**\n\n\nSELECT MIN(x) AS min FROM UNNEST([8, 37, 4, 55]) AS x;\n\n/*-----*\n| min |\n+-----+\n| 4   |\n*-----*/\n\n\nSELECT x, MIN(x) OVER (PARTITION BY MOD(x, 2)) AS min FROM UNNEST([8, NULL, 37, 4, NULL, 55]) AS x;\n\n/*------+------*\n| x    | min  |\n+------+------+\n| NULL | NULL |\n| NULL | NULL |\n| 8    | 4    |\n| 4    | 4    |\n| 37   | 37   |\n| 55   | 37   |\n*------+------*/"
            },
            "MIN_BY": {
                "name": "MIN_BY",
                "summary": "Synonym for ` ANY_VALUE(x HAVING MIN y) ` .",
                "description": "MIN_BY( x, y )\n\n**Description**\n\nSynonym for  ` ANY_VALUE(x HAVING MIN y) ` .\n\n**Return Data Types**\n\nMatches the input ` x ` data type.\n\n**Examples**\n\n\nWITH fruits AS ( SELECT \"apple\"  fruit, 3.55 price UNION ALL SELECT \"banana\"  fruit, 2.10 price UNION ALL SELECT \"pear\"  fruit, 4.30 price ) SELECT MIN_BY(fruit, price) as fruit FROM fruits;\n\n/*--------*\n| fruit  |\n+--------+\n| banana |\n*--------*/"
            },
            "STRING_AGG": {
                "name": "STRING_AGG",
                "summary": "Concatenates non- ` NULL ` ` STRING ` or ` BYTES ` values.",
                "description": "STRING_AGG(\n[ DISTINCT ]\nexpression [, delimiter]\n[ ORDER BY key [ { ASC | DESC } ] [, ... ] ]\n[ LIMIT n ]\n)\n[ OVER over_clause ]\n\nover_clause:\n{ named_window | ( [ window_specification ] ) }\n\nwindow_specification:\n[ named_window ]\n[ PARTITION BY partition_expression [, ...] ]\n[ ORDER BY expression [ { ASC | DESC }  ] [, ...] ]\n[ window_frame_clause ]\n\n\n**Description**\n\nReturns a value (either ` STRING ` or ` BYTES ` ) obtained by concatenating non- ` NULL ` values. Returns ` NULL ` if there are zero input rows or `\nexpression ` evaluates to ` NULL ` for all rows.\n\nIf a ` delimiter ` is specified, concatenated values are separated by that delimiter; otherwise, a comma is used as a delimiter.\n\nTo learn more about the optional aggregate clauses that you can pass into this function, see [ Aggregate function calls ](/bigquery/docs/reference/standard-\nsql/aggregate-function-calls) .\n\nIf this function is used with the ` OVER ` clause, it's part of a window function call. In a window function call, aggregate function clauses can't be used. To learn more about the ` OVER ` clause and how to use it, see [ Window function calls ](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n**Supported Argument Types**\n\nEither ` STRING ` or ` BYTES ` .\n\n**Return Data Types**\n\nEither ` STRING ` or ` BYTES ` .\n\n**Examples**\n\n\nSELECT STRING_AGG(fruit) AS string_agg FROM UNNEST([\"apple\", NULL, \"pear\", \"banana\", \"pear\"]) AS fruit;\n\n/*------------------------*\n| string_agg             |\n+------------------------+\n| apple,pear,banana,pear |\n*------------------------*/\n\n\nSELECT STRING_AGG(fruit, \" & \") AS string_agg FROM UNNEST([\"apple\", \"pear\", \"banana\", \"pear\"]) AS fruit;\n\n/*------------------------------*\n| string_agg                   |\n+------------------------------+\n| apple & pear & banana & pear |\n*------------------------------*/\n\n\nSELECT STRING_AGG(DISTINCT fruit, \" & \") AS string_agg FROM UNNEST([\"apple\", \"pear\", \"banana\", \"pear\"]) AS fruit;\n\n/*-----------------------*\n| string_agg            |\n+-----------------------+\n| apple & pear & banana |\n*-----------------------*/\n\n\nSELECT STRING_AGG(fruit, \" & \" ORDER BY LENGTH(fruit)) AS string_agg FROM UNNEST([\"apple\", \"pear\", \"banana\", \"pear\"]) AS fruit;\n\n/*------------------------------*\n| string_agg                   |\n+------------------------------+\n| pear & pear & apple & banana |\n*------------------------------*/\n\n\nSELECT STRING_AGG(fruit, \" & \" LIMIT 2) AS string_agg FROM UNNEST([\"apple\", \"pear\", \"banana\", \"pear\"]) AS fruit;\n\n/*--------------*\n| string_agg   |\n+--------------+\n| apple & pear |\n*--------------*/\n\n\nSELECT STRING_AGG(DISTINCT fruit, \" & \" ORDER BY fruit DESC LIMIT 2) AS string_agg FROM UNNEST([\"apple\", \"pear\", \"banana\", \"pear\"]) AS fruit;\n\n/*---------------*\n| string_agg    |\n+---------------+\n| pear & banana |\n*---------------*/\n\n\nSELECT fruit,\nSTRING_AGG(fruit, \" & \") OVER (ORDER BY LENGTH(fruit)) AS string_agg FROM UNNEST([\"apple\", NULL, \"pear\", \"banana\", \"pear\"]) AS fruit;\n\n/*--------+------------------------------*\n| fruit  | string_agg                   |\n+--------+------------------------------+\n| NULL   | NULL                         |\n| pear   | pear & pear                  |\n| pear   | pear & pear                  |\n| apple  | pear & pear & apple          |\n| banana | pear & pear & apple & banana |\n*--------+------------------------------*/"
            },
            "SUM": {
                "name": "SUM",
                "summary": "Gets the sum of non- ` NULL ` values.",
                "description": "SUM(\n[ DISTINCT ]\nexpression )\n[ OVER over_clause ]\n\nover_clause:\n{ named_window | ( [ window_specification ] ) }\n\nwindow_specification:\n[ named_window ]\n[ PARTITION BY partition_expression [, ...] ]\n[ ORDER BY expression [ { ASC | DESC }  ] [, ...] ]\n[ window_frame_clause ]\n\n\n**Description**\n\nReturns the sum of non- ` NULL ` values in an aggregated group.\n\nTo learn more about the optional aggregate clauses that you can pass into this function, see [ Aggregate function calls ](/bigquery/docs/reference/standard-\nsql/aggregate-function-calls) .\n\nThis function can be used with the [ ` AGGREGATION_THRESHOLD ` clause\n](/bigquery/docs/reference/standard-sql/query-syntax#agg_threshold_clause) .\n\nTo learn more about the ` OVER ` clause and how to use it, see [ Window function calls ](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n` SUM ` can be used with differential privacy. For more information, see [\nDifferentially private aggregate functions\n](/bigquery/docs/reference/standard-sql/aggregate-dp-functions) .\n\nCaveats:\n\n* If the aggregated group is empty or the argument is ` NULL ` for all rows in the group, returns ` NULL ` .\n* If the argument is ` NaN ` for any row in the group, returns ` NaN ` .\n* If the argument is ` [+|-]Infinity ` for any row in the group, returns either ` [+|-]Infinity ` or ` NaN ` .\n* If there is numeric overflow, produces an error.\n* If a [ floating-point type ](/bigquery/docs/reference/standard-sql/data-types#floating_point_types) is returned, the result is [ non-deterministic ](/bigquery/docs/reference/standard-sql/data-types#floating-point-semantics) , which means you might receive a different result each time you use this function.\n\n**Supported Argument Types**\n\n* Any supported numeric data type\n* ` INTERVAL `\n\n**Return Data Types**\n\nINPUT  |  ` INT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 ` |  `\nINTERVAL `\n---|---|---|---|---|---\nOUTPUT  |  ` INT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 ` |  `\nINTERVAL `\n\n**Examples**\n\n\nSELECT SUM(x) AS sum FROM UNNEST([1, 2, 3, 4, 5, 4, 3, 2, 1]) AS x;\n\n/*-----*\n| sum |\n+-----+\n| 25  |\n*-----*/\n\n\nSELECT SUM(DISTINCT x) AS sum FROM UNNEST([1, 2, 3, 4, 5, 4, 3, 2, 1]) AS x;\n\n/*-----*\n| sum |\n+-----+\n| 15  |\n*-----*/\n\n\nSELECT x,\nSUM(x) OVER (PARTITION BY MOD(x, 3)) AS sum FROM UNNEST([1, 2, 3, 4, 5, 4, 3, 2, 1]) AS x;\n\n/*---+-----*\n| x | sum |\n+---+-----+\n| 3 | 6   |\n| 3 | 6   |\n| 1 | 10  |\n| 4 | 10  |\n| 4 | 10  |\n| 1 | 10  |\n| 2 | 9   |\n| 5 | 9   |\n| 2 | 9   |\n*---+-----*/\n\n\nSELECT x,\nSUM(DISTINCT x) OVER (PARTITION BY MOD(x, 3)) AS sum FROM UNNEST([1, 2, 3, 4, 5, 4, 3, 2, 1]) AS x;\n\n/*---+-----*\n| x | sum |\n+---+-----+\n| 3 | 3   |\n| 3 | 3   |\n| 1 | 5   |\n| 4 | 5   |\n| 4 | 5   |\n| 1 | 5   |\n| 2 | 7   |\n| 5 | 7   |\n| 2 | 7   |\n*---+-----*/\n\n\nSELECT SUM(x) AS sum FROM UNNEST([]) AS x;\n\n/*------*\n| sum  |\n+------+\n| NULL |\n*------*/"
            }
        }
    },
    {
        "category": "approximate-aggregate-functions",
        "description": "GoogleSQL for BigQuery supports approximate aggregate functions. To learn about the syntax for aggregate function calls, see [ Aggregate function calls\n](/bigquery/docs/reference/standard-sql/aggregate-function-calls) .\n\nApproximate aggregate functions are scalable in terms of memory usage and time, but produce approximate results instead of exact results. These functions typically require less memory than [ exact aggregation functions\n](/bigquery/docs/reference/standard-sql/aggregate_functions) like `\nCOUNT(DISTINCT ...) ` , but also introduce statistical uncertainty. This makes approximate aggregation appropriate for large data streams for which linear memory usage is impractical, as well as for data that is already approximate.\n\nThe approximate aggregate functions in this section work directly on the input data, rather than an intermediate estimation of the data. These functions _do not allow_ users to specify the precision for the estimation with sketches. If you would like to specify precision with sketches, see:\n\n* [ HyperLogLog++ functions ](/bigquery/docs/reference/standard-sql/hll_functions#hyperloglog_functions) to estimate cardinality.",
        "source": "approximate_aggregate_functions.txt",
        "functions": {
            "APPROX_COUNT_DISTINCT": {
                "name": "APPROX_COUNT_DISTINCT",
                "summary": "Gets the approximate result for ` COUNT(DISTINCT expression) ` .",
                "description": "APPROX_COUNT_DISTINCT( expression )\n\n**Description**\n\nReturns the approximate result for ` COUNT(DISTINCT expression) ` . The value returned is a statistical estimate, not necessarily the actual value.\n\nThis function is less accurate than ` COUNT(DISTINCT expression) ` , but performs better on huge input.\n\n**Supported Argument Types**\n\nAny data type **except** :\n\n* ` ARRAY `\n* ` STRUCT `\n* ` INTERVAL `\n\n**Returned Data Types**\n\n` INT64 `\n\n**Examples**\n\n\nSELECT APPROX_COUNT_DISTINCT(x) as approx_distinct FROM UNNEST([0, 1, 1, 2, 3, 5]) as x;\n\n/*-----------------*\n| approx_distinct |\n+-----------------+\n| 5               |\n*-----------------*/"
            },
            "APPROX_QUANTILES": {
                "name": "APPROX_QUANTILES",
                "summary": "Gets the approximate quantile boundaries.",
                "description": "APPROX_QUANTILES(\n[ DISTINCT ]\nexpression, number\n[ { IGNORE | RESPECT } NULLS ]\n)\n\n**Description**\n\nReturns the approximate boundaries for a group of ` expression ` values, where\n` number ` represents the number of quantiles to create. This function returns an array of ` number ` \\+ 1 elements, sorted in ascending order, where the first element is the approximate minimum and the last element is the approximate maximum.\n\nReturns ` NULL ` if there are zero input rows or ` expression ` evaluates to `\nNULL ` for all rows.\n\nTo learn more about the optional aggregate clauses that you can pass into this function, see [ Aggregate function calls ](/bigquery/docs/reference/standard-\nsql/aggregate-function-calls) .\n\n**Supported Argument Types**\n\n* ` expression ` : Any supported data type **except** :\n\n* ` ARRAY `\n* ` STRUCT `\n* ` INTERVAL `\n* ` number ` : ` INT64 ` literal or query parameter.\n\n**Returned Data Types**\n\n` ARRAY<T> ` where ` T ` is the type specified by ` expression ` .\n\n**Examples**\n\n\nSELECT APPROX_QUANTILES(x, 2) AS approx_quantiles FROM UNNEST([1, 1, 1, 4, 5, 6, 7, 8, 9, 10]) AS x;\n\n/*------------------*\n| approx_quantiles |\n+------------------+\n| [1, 5, 10]       |\n*------------------*/\n\n\nSELECT APPROX_QUANTILES(x, 100)[OFFSET(90)] AS percentile_90 FROM UNNEST([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) AS x;\n\n/*---------------*\n| percentile_90 |\n+---------------+\n| 9             |\n*---------------*/\n\n\nSELECT APPROX_QUANTILES(DISTINCT x, 2) AS approx_quantiles FROM UNNEST([1, 1, 1, 4, 5, 6, 7, 8, 9, 10]) AS x;\n\n/*------------------*\n| approx_quantiles |\n+------------------+\n| [1, 6, 10]       |\n*------------------*/\n\n\nSELECT FORMAT(\"%T\", APPROX_QUANTILES(x, 2 RESPECT NULLS)) AS approx_quantiles FROM UNNEST([NULL, NULL, 1, 1, 1, 4, 5, 6, 7, 8, 9, 10]) AS x;\n\n/*------------------*\n| approx_quantiles |\n+------------------+\n| [NULL, 4, 10]    |\n*------------------*/\n\n\nSELECT FORMAT(\"%T\", APPROX_QUANTILES(DISTINCT x, 2 RESPECT NULLS)) AS approx_quantiles FROM UNNEST([NULL, NULL, 1, 1, 1, 4, 5, 6, 7, 8, 9, 10]) AS x;\n\n/*------------------*\n| approx_quantiles |\n+------------------+\n| [NULL, 6, 10]    |\n*------------------*/"
            },
            "APPROX_TOP_COUNT": {
                "name": "APPROX_TOP_COUNT",
                "summary": "Gets the approximate top elements and their approximate count.",
                "description": "APPROX_TOP_COUNT( expression, number )\n\n**Description**\n\nReturns the approximate top elements of ` expression ` as an array of ` STRUCT\n` s. The ` number ` parameter specifies the number of elements returned.\n\nEach ` STRUCT ` contains two fields. The first field (named ` value ` ) contains an input value. The second field (named ` count ` ) contains an `\nINT64 ` specifying the number of times the value was returned.\n\nReturns ` NULL ` if there are zero input rows.\n\nTo learn more about the optional aggregate clauses that you can pass into this function, see [ Aggregate function calls ](/bigquery/docs/reference/standard-\nsql/aggregate-function-calls) .\n\n**Supported Argument Types**\n\n* ` expression ` : Any data type that the ` GROUP BY ` clause supports.\n* ` number ` : ` INT64 ` literal or query parameter.\n\n**Returned Data Types**\n\n` ARRAY<STRUCT> `\n\n**Examples**\n\n\nSELECT APPROX_TOP_COUNT(x, 2) as approx_top_count FROM UNNEST([\"apple\", \"apple\", \"pear\", \"pear\", \"pear\", \"banana\"]) as x;\n\n/*-------------------------*\n| approx_top_count        |\n+-------------------------+\n| [{pear, 3}, {apple, 2}] |\n*-------------------------*/\n\n**NULL handling**\n\n` APPROX_TOP_COUNT ` does not ignore ` NULL ` s in the input. For example:\n\n\nSELECT APPROX_TOP_COUNT(x, 2) as approx_top_count FROM UNNEST([NULL, \"pear\", \"pear\", \"pear\", \"apple\", NULL]) as x;\n\n/*------------------------*\n| approx_top_count       |\n+------------------------+\n| [{pear, 3}, {NULL, 2}] |\n*------------------------*/"
            },
            "APPROX_TOP_SUM": {
                "name": "APPROX_TOP_SUM",
                "summary": "Gets the approximate top elements and sum, based on the approximate sum of an assigned weight.",
                "description": "APPROX_TOP_SUM( expression, weight, number )\n\n**Description**\n\nReturns the approximate top elements of ` expression ` , based on the sum of an assigned ` weight ` . The ` number ` parameter specifies the number of elements returned.\n\nIf the ` weight ` input is negative or ` NaN ` , this function returns an error.\n\nThe elements are returned as an array of ` STRUCT ` s. Each ` STRUCT `\ncontains two fields: ` value ` and ` sum ` . The ` value ` field contains the value of the input expression. The ` sum ` field is the same type as ` weight\n` , and is the approximate sum of the input weight associated with the ` value\n` field.\n\nReturns ` NULL ` if there are zero input rows.\n\nTo learn more about the optional aggregate clauses that you can pass into this function, see [ Aggregate function calls ](/bigquery/docs/reference/standard-\nsql/aggregate-function-calls) .\n\n**Supported Argument Types**\n\n* ` expression ` : Any data type that the ` GROUP BY ` clause supports.\n* ` weight ` : One of the following:\n\n* ` INT64 `\n* ` NUMERIC `\n* ` BIGNUMERIC `\n* ` FLOAT64 `\n* ` number ` : ` INT64 ` literal or query parameter.\n\n**Returned Data Types**\n\n` ARRAY<STRUCT> `\n\n**Examples**\n\n\nSELECT APPROX_TOP_SUM(x, weight, 2) AS approx_top_sum FROM UNNEST([\nSTRUCT(\"apple\" AS x, 3 AS weight),\n(\"pear\", 2),\n(\"apple\", 0),\n(\"banana\", 5),\n(\"pear\", 4)\n]);\n\n/*--------------------------*\n| approx_top_sum           |\n+--------------------------+\n| [{pear, 6}, {banana, 5}] |\n*--------------------------*/\n\n**NULL handling**\n\n` APPROX_TOP_SUM ` does not ignore ` NULL ` values for the ` expression ` and\n` weight ` parameters.\n\n\nSELECT APPROX_TOP_SUM(x, weight, 2) AS approx_top_sum FROM UNNEST([STRUCT(\"apple\" AS x, NULL AS weight), (\"pear\", 0), (\"pear\", NULL)]);\n\n/*----------------------------*\n| approx_top_sum             |\n+----------------------------+\n| [{pear, 0}, {apple, NULL}] |\n*----------------------------*/\n\n\nSELECT APPROX_TOP_SUM(x, weight, 2) AS approx_top_sum FROM UNNEST([STRUCT(\"apple\" AS x, 0 AS weight), (NULL, 2)]);\n\n/*-------------------------*\n| approx_top_sum          |\n+-------------------------+\n| [{NULL, 2}, {apple, 0}] |\n*-------------------------*/\n\n\nSELECT APPROX_TOP_SUM(x, weight, 2) AS approx_top_sum FROM UNNEST([STRUCT(\"apple\" AS x, 0 AS weight), (NULL, NULL)]);\n\n/*----------------------------*\n| approx_top_sum             |\n+----------------------------+\n| [{apple, 0}, {NULL, NULL}] |\n*----------------------------*/"
            }
        }
    },
    {
        "category": "array-functions",
        "description": "GoogleSQL for BigQuery supports the following array functions.",
        "source": "array_functions.txt",
        "functions": {
            "ARRAY": {
                "name": "ARRAY",
                "summary": "Produces an array with one element for each row in a subquery.",
                "description": "ARRAY(subquery)\n\n**Description**\n\nThe ` ARRAY ` function returns an ` ARRAY ` with one element for each row in a\n[ subquery ](/bigquery/docs/reference/standard-sql/query-syntax#subqueries) .\n\nIf ` subquery ` produces a SQL table, the table must have exactly one column.\nEach element in the output ` ARRAY ` is the value of the single column of a row in the table.\n\nIf ` subquery ` produces a value table, then each element in the output `\nARRAY ` is the entire corresponding row of the value table.\n\n**Constraints**\n\n* Subqueries are unordered, so the elements of the output ` ARRAY ` are not guaranteed to preserve any order in the source table for the subquery. However, if the subquery includes an ` ORDER BY ` clause, the ` ARRAY ` function will return an ` ARRAY ` that honors that clause.\n* If the subquery returns more than one column, the ` ARRAY ` function returns an error.\n* If the subquery returns an ` ARRAY ` typed column or ` ARRAY ` typed rows, the ` ARRAY ` function returns an error that GoogleSQL does not support ` ARRAY ` s with elements of type [ ` ARRAY ` ](/bigquery/docs/reference/standard-sql/data-types#array_type) .\n* If the subquery returns zero rows, the ` ARRAY ` function returns an empty ` ARRAY ` . It never returns a ` NULL ` ` ARRAY ` .\n\n**Return type**\n\n` ARRAY `\n\n**Examples**\n\n\nSELECT ARRAY (SELECT 1 UNION ALL SELECT 2 UNION ALL SELECT 3) AS new_array;\n\n/*-----------*\n| new_array |\n+-----------+\n| [1, 2, 3] |\n*-----------*/\n\nTo construct an ` ARRAY ` from a subquery that contains multiple columns,\nchange the subquery to use ` SELECT AS STRUCT ` . Now the ` ARRAY ` function will return an ` ARRAY ` of ` STRUCT ` s. The ` ARRAY ` will contain one `\nSTRUCT ` for each row in the subquery, and each of these ` STRUCT ` s will contain a field for each column in that row.\n\n\nSELECT ARRAY (SELECT AS STRUCT 1, 2, 3 UNION ALL SELECT AS STRUCT 4, 5, 6) AS new_array;\n\n/*------------------------*\n| new_array              |\n+------------------------+\n| [{1, 2, 3}, {4, 5, 6}] |\n*------------------------*/\n\nSimilarly, to construct an ` ARRAY ` from a subquery that contains one or more\n` ARRAY ` s, change the subquery to use ` SELECT AS STRUCT ` .\n\n\nSELECT ARRAY (SELECT AS STRUCT [1, 2, 3] UNION ALL SELECT AS STRUCT [4, 5, 6]) AS new_array;\n\n/*----------------------------*\n| new_array                  |\n+----------------------------+\n| [{[1, 2, 3]}, {[4, 5, 6]}] |\n*----------------------------*/"
            },
            "ARRAY_CONCAT": {
                "name": "ARRAY_CONCAT",
                "summary": "Concatenates one or more arrays with the same element type into a single array.",
                "description": "ARRAY_CONCAT(array_expression[, ...])\n\n**Description**\n\nConcatenates one or more arrays with the same element type into a single array.\n\nThe function returns ` NULL ` if any input argument is ` NULL ` .\n\n**Note:** You can also use the [ || concatenation operator\n](/bigquery/docs/reference/standard-sql/operators) to concatenate arrays.\n\n**Return type**\n\n` ARRAY `\n\n**Examples**\n\n\nSELECT ARRAY_CONCAT([1, 2], [3, 4], [5, 6]) as count_to_six;\n\n/*--------------------------------------------------*\n| count_to_six                                     |\n+--------------------------------------------------+\n| [1, 2, 3, 4, 5, 6]                               |\n*--------------------------------------------------*/"
            },
            "ARRAY_LENGTH": {
                "name": "ARRAY_LENGTH",
                "summary": "Gets the number of elements in an array.",
                "description": "ARRAY_LENGTH(array_expression)\n\n**Description**\n\nReturns the size of the array. Returns 0 for an empty array. Returns ` NULL `\nif the ` array_expression ` is ` NULL ` .\n\n**Return type**\n\n` INT64 `\n\n**Examples**\n\n\nWITH items AS (SELECT [\"coffee\", NULL, \"milk\" ] as list UNION ALL SELECT [\"cake\", \"pie\"] as list) SELECT ARRAY_TO_STRING(list, ', ', 'NULL'), ARRAY_LENGTH(list) AS size FROM items ORDER BY size DESC;\n\n/*--------------------+------*\n| list               | size |\n+--------------------+------+\n| coffee, NULL, milk | 3    |\n| cake, pie          | 2    |\n*--------------------+------*/"
            },
            "ARRAY_REVERSE": {
                "name": "ARRAY_REVERSE",
                "summary": "Reverses the order of elements in an array.",
                "description": "ARRAY_REVERSE(value)\n\n**Description**\n\nReturns the input ` ARRAY ` with elements in reverse order.\n\n**Return type**\n\n` ARRAY `\n\n**Examples**\n\n\nWITH example AS ( SELECT [1, 2, 3] AS arr UNION ALL SELECT [4, 5] AS arr UNION ALL SELECT [] AS arr ) SELECT arr,\nARRAY_REVERSE(arr) AS reverse_arr FROM example;\n\n/*-----------+-------------*\n| arr       | reverse_arr |\n+-----------+-------------+\n| [1, 2, 3] | [3, 2, 1]   |\n| [4, 5]    | [5, 4]      |\n| []        | []          |\n*-----------+-------------*/"
            },
            "ARRAY_TO_STRING": {
                "name": "ARRAY_TO_STRING",
                "summary": "Produces a concatenation of the elements in an array as a ` STRING ` value.",
                "description": "ARRAY_TO_STRING(array_expression, delimiter[, null_text])\n\n**Description**\n\nReturns a concatenation of the elements in ` array_expression ` as a ` STRING\n` . The value for ` array_expression ` can either be an array of ` STRING ` or\n` BYTES ` data types.\n\nIf the ` null_text ` parameter is used, the function replaces any ` NULL `\nvalues in the array with the value of ` null_text ` .\n\nIf the ` null_text ` parameter is not used, the function omits the ` NULL `\nvalue and its preceding delimiter.\n\n**Return type**\n\n` STRING `\n\n**Examples**\n\n\nWITH items AS (SELECT ['coffee', 'tea', 'milk' ] as list UNION ALL SELECT ['cake', 'pie', NULL] as list)\n\nSELECT ARRAY_TO_STRING(list, '--') AS text FROM items;\n\n/*--------------------------------*\n| text                           |\n+--------------------------------+\n| coffee--tea--milk              |\n| cake--pie                      |\n*--------------------------------*/\n\n\nWITH items AS (SELECT ['coffee', 'tea', 'milk' ] as list UNION ALL SELECT ['cake', 'pie', NULL] as list)\n\nSELECT ARRAY_TO_STRING(list, '--', 'MISSING') AS text FROM items;\n\n/*--------------------------------*\n| text                           |\n+--------------------------------+\n| coffee--tea--milk              |\n| cake--pie--MISSING             |\n*--------------------------------*/"
            },
            "GENERATE_ARRAY": {
                "name": "GENERATE_ARRAY",
                "summary": "Generates an array of values in a range.",
                "description": "GENERATE_ARRAY(start_expression, end_expression[, step_expression])\n\n**Description**\n\nReturns an array of values. The ` start_expression ` and ` end_expression `\nparameters determine the inclusive start and end of the array.\n\nThe ` GENERATE_ARRAY ` function accepts the following data types as inputs:\n\n* ` INT64 `\n* ` NUMERIC `\n* ` BIGNUMERIC `\n* ` FLOAT64 `\n\nThe ` step_expression ` parameter determines the increment used to generate array values. The default value for this parameter is ` 1 ` .\n\nThis function returns an error if ` step_expression ` is set to 0, or if any input is ` NaN ` .\n\nIf any argument is ` NULL ` , the function will return a ` NULL ` array.\n\n**Return Data Type**\n\n` ARRAY `\n\n**Examples**\n\nThe following returns an array of integers, with a default step of 1.\n\n\nSELECT GENERATE_ARRAY(1, 5) AS example_array;\n\n/*-----------------*\n| example_array   |\n+-----------------+\n| [1, 2, 3, 4, 5] |\n*-----------------*/\n\nThe following returns an array using a user-specified step size.\n\n\nSELECT GENERATE_ARRAY(0, 10, 3) AS example_array;\n\n/*---------------*\n| example_array |\n+---------------+\n| [0, 3, 6, 9]  |\n*---------------*/\n\nThe following returns an array using a negative value, ` -3 ` for its step size.\n\n\nSELECT GENERATE_ARRAY(10, 0, -3) AS example_array;\n\n/*---------------*\n| example_array |\n+---------------+\n| [10, 7, 4, 1] |\n*---------------*/\n\nThe following returns an array using the same value for the ` start_expression\n` and ` end_expression ` .\n\n\nSELECT GENERATE_ARRAY(4, 4, 10) AS example_array;\n\n/*---------------*\n| example_array |\n+---------------+\n| [4]           |\n*---------------*/\n\nThe following returns an empty array, because the ` start_expression ` is greater than the ` end_expression ` , and the ` step_expression ` value is positive.\n\n\nSELECT GENERATE_ARRAY(10, 0, 3) AS example_array;\n\n/*---------------*\n| example_array |\n+---------------+\n| []            |\n*---------------*/\n\nThe following returns a ` NULL ` array because ` end_expression ` is ` NULL `\n.\n\n\nSELECT GENERATE_ARRAY(5, NULL, 1) AS example_array;\n\n/*---------------*\n| example_array |\n+---------------+\n| NULL          |\n*---------------*/\n\nThe following returns multiple arrays.\n\n\nSELECT GENERATE_ARRAY(start, 5) AS example_array FROM UNNEST([3, 4, 5]) AS start;\n\n/*---------------*\n| example_array |\n+---------------+\n| [3, 4, 5]     |\n| [4, 5]        |\n| [5]           |\n+---------------*/"
            },
            "GENERATE_DATE_ARRAY": {
                "name": "GENERATE_DATE_ARRAY",
                "summary": "Generates an array of dates in a range.",
                "description": "GENERATE_DATE_ARRAY(start_date, end_date[, INTERVAL INT64_expr date_part])\n\n**Description**\n\nReturns an array of dates. The ` start_date ` and ` end_date ` parameters determine the inclusive start and end of the array.\n\nThe ` GENERATE_DATE_ARRAY ` function accepts the following data types as inputs:\n\n* ` start_date ` must be a ` DATE ` .\n* ` end_date ` must be a ` DATE ` .\n* ` INT64_expr ` must be an ` INT64 ` .\n* ` date_part ` must be either DAY, WEEK, MONTH, QUARTER, or YEAR.\n\nThe ` INT64_expr ` parameter determines the increment used to generate dates.\nThe default value for this parameter is 1 day.\n\nThis function returns an error if ` INT64_expr ` is set to 0.\n\n**Return Data Type**\n\n` ARRAY ` containing 0 or more ` DATE ` values.\n\n**Examples**\n\nThe following returns an array of dates, with a default step of 1.\n\n\nSELECT GENERATE_DATE_ARRAY('2016-10-05', '2016-10-08') AS example;\n\n/*--------------------------------------------------*\n| example                                          |\n+--------------------------------------------------+\n| [2016-10-05, 2016-10-06, 2016-10-07, 2016-10-08] |\n*--------------------------------------------------*/\n\nThe following returns an array using a user-specified step size.\n\n\nSELECT GENERATE_DATE_ARRAY(\n'2016-10-05', '2016-10-09', INTERVAL 2 DAY) AS example;\n\n/*--------------------------------------*\n| example                              |\n+--------------------------------------+\n| [2016-10-05, 2016-10-07, 2016-10-09] |\n*--------------------------------------*/\n\nThe following returns an array using a negative value, ` -3 ` for its step size.\n\n\nSELECT GENERATE_DATE_ARRAY('2016-10-05',\n'2016-10-01', INTERVAL -3 DAY) AS example;\n\n/*--------------------------*\n| example                  |\n+--------------------------+\n| [2016-10-05, 2016-10-02] |\n*--------------------------*/\n\nThe following returns an array using the same value for the ` start_date ` and\n` end_date ` .\n\n\nSELECT GENERATE_DATE_ARRAY('2016-10-05',\n'2016-10-05', INTERVAL 8 DAY) AS example;\n\n/*--------------*\n| example      |\n+--------------+\n| [2016-10-05] |\n*--------------*/\n\nThe following returns an empty array, because the ` start_date ` is greater than the ` end_date ` , and the ` step ` value is positive.\n\n\nSELECT GENERATE_DATE_ARRAY('2016-10-05',\n'2016-10-01', INTERVAL 1 DAY) AS example;\n\n/*---------*\n| example |\n+---------+\n| []      |\n*---------*/\n\nThe following returns a ` NULL ` array, because one of its inputs is ` NULL `\n.\n\n\nSELECT GENERATE_DATE_ARRAY('2016-10-05', NULL) AS example;\n\n/*---------*\n| example |\n+---------+\n| NULL    |\n*---------*/\n\nThe following returns an array of dates, using MONTH as the ` date_part `\ninterval:\n\n\nSELECT GENERATE_DATE_ARRAY('2016-01-01',\n'2016-12-31', INTERVAL 2 MONTH) AS example;\n\n/*--------------------------------------------------------------------------*\n| example                                                                  |\n+--------------------------------------------------------------------------+\n| [2016-01-01, 2016-03-01, 2016-05-01, 2016-07-01, 2016-09-01, 2016-11-01] |\n*--------------------------------------------------------------------------*/\n\nThe following uses non-constant dates to generate an array.\n\n\nSELECT GENERATE_DATE_ARRAY(date_start, date_end, INTERVAL 1 WEEK) AS date_range FROM ( SELECT DATE '2016-01-01' AS date_start, DATE '2016-01-31' AS date_end UNION ALL SELECT DATE \"2016-04-01\", DATE \"2016-04-30\"\nUNION ALL SELECT DATE \"2016-07-01\", DATE \"2016-07-31\"\nUNION ALL SELECT DATE \"2016-10-01\", DATE \"2016-10-31\"\n) AS items;\n\n/*--------------------------------------------------------------*\n| date_range                                                   |\n+--------------------------------------------------------------+\n| [2016-01-01, 2016-01-08, 2016-01-15, 2016-01-22, 2016-01-29] |\n| [2016-04-01, 2016-04-08, 2016-04-15, 2016-04-22, 2016-04-29] |\n| [2016-07-01, 2016-07-08, 2016-07-15, 2016-07-22, 2016-07-29] |\n| [2016-10-01, 2016-10-08, 2016-10-15, 2016-10-22, 2016-10-29] |\n*--------------------------------------------------------------*/"
            },
            "GENERATE_TIMESTAMP_ARRAY": {
                "name": "GENERATE_TIMESTAMP_ARRAY",
                "summary": "Generates an array of timestamps in a range.",
                "description": "GENERATE_TIMESTAMP_ARRAY(start_timestamp, end_timestamp,\nINTERVAL step_expression date_part)\n\n**Description**\n\nReturns an ` ARRAY ` of ` TIMESTAMPS ` separated by a given interval. The `\nstart_timestamp ` and ` end_timestamp ` parameters determine the inclusive lower and upper bounds of the ` ARRAY ` .\n\nThe ` GENERATE_TIMESTAMP_ARRAY ` function accepts the following data types as inputs:\n\n* ` start_timestamp ` : ` TIMESTAMP `\n* ` end_timestamp ` : ` TIMESTAMP `\n* ` step_expression ` : ` INT64 `\n* Allowed ` date_part ` values are: ` MICROSECOND ` , ` MILLISECOND ` , ` SECOND ` , ` MINUTE ` , ` HOUR ` , or ` DAY ` .\n\nThe ` step_expression ` parameter determines the increment used to generate timestamps.\n\n**Return Data Type**\n\nAn ` ARRAY ` containing 0 or more ` TIMESTAMP ` values.\n\n**Examples**\n\nThe following example returns an ` ARRAY ` of ` TIMESTAMP ` s at intervals of 1 day.\n\n\nSELECT GENERATE_TIMESTAMP_ARRAY('2016-10-05 00:00:00', '2016-10-07 00:00:00',\nINTERVAL 1 DAY) AS timestamp_array;\n\n/*--------------------------------------------------------------------------*\n| timestamp_array                                                          |\n+--------------------------------------------------------------------------+\n| [2016-10-05 00:00:00+00, 2016-10-06 00:00:00+00, 2016-10-07 00:00:00+00] |\n*--------------------------------------------------------------------------*/\n\nThe following example returns an ` ARRAY ` of ` TIMESTAMP ` s at intervals of 1 second.\n\n\nSELECT GENERATE_TIMESTAMP_ARRAY('2016-10-05 00:00:00', '2016-10-05 00:00:02',\nINTERVAL 1 SECOND) AS timestamp_array;\n\n/*--------------------------------------------------------------------------*\n| timestamp_array                                                          |\n+--------------------------------------------------------------------------+\n| [2016-10-05 00:00:00+00, 2016-10-05 00:00:01+00, 2016-10-05 00:00:02+00] |\n*--------------------------------------------------------------------------*/\n\nThe following example returns an ` ARRAY ` of ` TIMESTAMPS ` with a negative interval.\n\n\nSELECT GENERATE_TIMESTAMP_ARRAY('2016-10-06 00:00:00', '2016-10-01 00:00:00',\nINTERVAL -2 DAY) AS timestamp_array;\n\n/*--------------------------------------------------------------------------*\n| timestamp_array                                                          |\n+--------------------------------------------------------------------------+\n| [2016-10-06 00:00:00+00, 2016-10-04 00:00:00+00, 2016-10-02 00:00:00+00] |\n*--------------------------------------------------------------------------*/\n\nThe following example returns an ` ARRAY ` with a single element, because `\nstart_timestamp ` and ` end_timestamp ` have the same value.\n\n\nSELECT GENERATE_TIMESTAMP_ARRAY('2016-10-05 00:00:00', '2016-10-05 00:00:00',\nINTERVAL 1 HOUR) AS timestamp_array;\n\n/*--------------------------*\n| timestamp_array          |\n+--------------------------+\n| [2016-10-05 00:00:00+00] |\n*--------------------------*/\n\nThe following example returns an empty ` ARRAY ` , because ` start_timestamp `\nis later than ` end_timestamp ` .\n\n\nSELECT GENERATE_TIMESTAMP_ARRAY('2016-10-06 00:00:00', '2016-10-05 00:00:00',\nINTERVAL 1 HOUR) AS timestamp_array;\n\n/*-----------------*\n| timestamp_array |\n+-----------------+\n| []              |\n*-----------------*/\n\nThe following example returns a null ` ARRAY ` , because one of the inputs is\n` NULL ` .\n\n\nSELECT GENERATE_TIMESTAMP_ARRAY('2016-10-05 00:00:00', NULL, INTERVAL 1 HOUR) AS timestamp_array;\n\n/*-----------------*\n| timestamp_array |\n+-----------------+\n| NULL            |\n*-----------------*/\n\nThe following example generates ` ARRAY ` s of ` TIMESTAMP ` s from columns containing values for ` start_timestamp ` and ` end_timestamp ` .\n\n\nSELECT GENERATE_TIMESTAMP_ARRAY(start_timestamp, end_timestamp, INTERVAL 1 HOUR) AS timestamp_array FROM (SELECT TIMESTAMP '2016-10-05 00:00:00' AS start_timestamp,\nTIMESTAMP '2016-10-05 02:00:00' AS end_timestamp UNION ALL SELECT TIMESTAMP '2016-10-05 12:00:00' AS start_timestamp,\nTIMESTAMP '2016-10-05 14:00:00' AS end_timestamp UNION ALL SELECT TIMESTAMP '2016-10-05 23:59:00' AS start_timestamp,\nTIMESTAMP '2016-10-06 01:59:00' AS end_timestamp);\n\n/*--------------------------------------------------------------------------*\n| timestamp_array                                                          |\n+--------------------------------------------------------------------------+\n| [2016-10-05 00:00:00+00, 2016-10-05 01:00:00+00, 2016-10-05 02:00:00+00] |\n| [2016-10-05 12:00:00+00, 2016-10-05 13:00:00+00, 2016-10-05 14:00:00+00] |\n| [2016-10-05 23:59:00+00, 2016-10-06 00:59:00+00, 2016-10-06 01:59:00+00] |\n*--------------------------------------------------------------------------*/"
            }
        }
    },
    {
        "category": "bit-functions",
        "description": "GoogleSQL for BigQuery supports the following bit functions.",
        "source": "bit_functions.txt",
        "functions": {
            "BIT_COUNT": {
                "name": "BIT_COUNT",
                "summary": "Gets the number of bits that are set in an input expression.",
                "description": "BIT_COUNT(expression)\n\n**Description**\n\nThe input, ` expression ` , must be an integer or ` BYTES ` .\n\nReturns the number of bits that are set in the input ` expression ` . For signed integers, this is the number of bits in two's complement form.\n\n**Return Data Type**\n\n` INT64 `\n\n**Example**\n\n\nSELECT a, BIT_COUNT(a) AS a_bits, FORMAT(\"%T\", b) as b, BIT_COUNT(b) AS b_bits FROM UNNEST([\nSTRUCT(0 AS a, b'' AS b), (0, b'\\x00'), (5, b'\\x05'), (8, b'\\x00\\x08'),\n(0xFFFF, b'\\xFF\\xFF'), (-2, b'\\xFF\\xFF\\xFF\\xFF\\xFF\\xFF\\xFF\\xFE'),\n(-1, b'\\xFF\\xFF\\xFF\\xFF\\xFF\\xFF\\xFF\\xFF'),\n(NULL, b'\\xFF\\xFF\\xFF\\xFF\\xFF\\xFF\\xFF\\xFF\\xFF\\xFF')\n]) AS x;\n\n/*-------+--------+---------------------------------------------+--------*\n| a     | a_bits | b                                           | b_bits |\n+-------+--------+---------------------------------------------+--------+\n| 0     | 0      | b\"\"                                         | 0      |\n| 0     | 0      | b\"\\x00\"                                     | 0      |\n| 5     | 2      | b\"\\x05\"                                     | 2      |\n| 8     | 1      | b\"\\x00\\x08\"                                 | 1      |\n| 65535 | 16     | b\"\\xff\\xff\"                                 | 16     |\n| -2    | 63     | b\"\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xfe\"         | 63     |\n| -1    | 64     | b\"\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\"         | 64     |\n| NULL  | NULL   | b\"\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\" | 80     |\n*-------+--------+---------------------------------------------+--------*/"
            }
        }
    },
    {
        "category": "conversion-functions",
        "description": "GoogleSQL for BigQuery supports conversion functions. These data type conversions are explicit, but some conversions can happen implicitly. You can learn more about implicit and explicit conversion [ here\n](/bigquery/docs/reference/standard-sql/conversion_rules) .",
        "source": "conversion_functions.txt",
        "functions": {
            "CAST": {
                "name": "CAST",
                "summary": "Convert the results of an expression to the given type.",
                "description": "CAST(expression AS typename [format_clause])\n\n**Description**\n\nCast syntax is used in a query to indicate that the result type of an expression should be converted to some other type.\n\nWhen using ` CAST ` , a query can fail if GoogleSQL is unable to perform the cast. If you want to protect your queries from these types of errors, you can use  SAFE_CAST  .\n\nCasts between supported types that do not successfully map from the original value to the target domain produce runtime errors. For example, casting `\nBYTES ` to ` STRING ` where the byte sequence is not valid UTF-8 results in a runtime error.\n\nSome casts can include a [ format clause ](/bigquery/docs/reference/standard-\nsql/format-elements#formatting_syntax) , which provides instructions for how to conduct the cast. For example, you could instruct a cast to convert a sequence of bytes to a BASE64-encoded string instead of a UTF-8-encoded string.\n\nThe structure of the format clause is unique to each type of cast and more information is available in the section for that cast.\n\n**Examples**\n\nThe following query results in ` \"true\" ` if ` x ` is ` 1 ` , ` \"false\" ` for any other non- ` NULL ` value, and ` NULL ` if ` x ` is ` NULL ` .\n\n\nCAST(x=1 AS STRING)"
            },
            "PARSE_BIGNUMERIC": {
                "name": "PARSE_BIGNUMERIC",
                "summary": "Converts a ` STRING ` value to a ` BIGNUMERIC ` value.",
                "description": "PARSE_BIGNUMERIC(string_expression)\n\n**Description**\n\nConverts a ` STRING ` to a ` BIGNUMERIC ` value.\n\nThe numeric literal contained in the string must not exceed the [ maximum precision or range ](/bigquery/docs/reference/standard-sql/data-\ntypes#decimal_types) of the ` BIGNUMERIC ` type, or an error occurs. If the number of digits after the decimal point exceeds 38, then the resulting `\nBIGNUMERIC ` value rounds [ half away from zero\n](https://en.wikipedia.org/wiki/Rounding#Round_half_away_from_zero) to have 38 digits after the decimal point.\n\n\n-- This example shows how a string with a decimal point is parsed.\nSELECT PARSE_BIGNUMERIC(\"123.45\") AS parsed;\n\n/*--------*\n| parsed |\n+--------+\n| 123.45 |\n*--------*/\n\n-- This example shows how a string with an exponent is parsed.\nSELECT PARSE_BIGNUMERIC(\"123.456E37\") AS parsed;\n\n/*-----------------------------------------*\n| parsed                                  |\n+-----------------------------------------+\n| 123400000000000000000000000000000000000 |\n*-----------------------------------------*/\n\n-- This example shows the rounding when digits after the decimal point exceeds 38.\nSELECT PARSE_BIGNUMERIC(\"1.123456789012345678901234567890123456789\") as parsed;\n\n/*------------------------------------------*\n| parsed                                   |\n+------------------------------------------+\n| 1.12345678901234567890123456789012345679 |\n*------------------------------------------*/\n\nThis function is similar to using the  ` CAST AS BIGNUMERIC ` function except that the ` PARSE_BIGNUMERIC ` function only accepts string inputs and allows the following in the string:\n\n* Spaces between the sign (+/-) and the number\n* Signs (+/-) after the number\n\nRules for valid input strings:\n\nRule  |  Example Input  |  Output\n---|---|---\nThe string can only contain digits, commas, decimal points and signs.  |  \"-\n12,34567,89.0\"  |  -123456789 Whitespaces are allowed anywhere except between digits.  |  \" - 12.345 \"  |\n-12.345 Only digits and commas are allowed before the decimal point.  |  \" 12,345,678\"\n|  12345678 Only digits are allowed after the decimal point.  |  \"1.234 \"  |  1.234 Use ` E ` or ` e ` for exponents. After the ` e ` , digits and a leading sign indicator are allowed.  |  \" 123.45e-1\"  |  12.345 If the integer part is not empty, then it must contain at least one digit.  |\n\" 0,.12 -\"  |  -0.12 If the string contains a decimal point, then it must contain at least one digit.  |  \" .1\"  |  0.1 The string cannot contain more than one sign.  |  \" 0.5 +\"  |  0.5\n\n**Return Data Type**\n\n` BIGNUMERIC `\n\n**Examples**\n\nThis example shows an input with spaces before, after, and between the sign and the number:\n\n\nSELECT PARSE_BIGNUMERIC(\"  -  12.34 \") as parsed;\n\n/*--------*\n| parsed |\n+--------+\n| -12.34 |\n*--------*/\n\nThis example shows an input with an exponent as well as the sign after the number:\n\n\nSELECT PARSE_BIGNUMERIC(\"12.34e-1-\") as parsed;\n\n/*--------*\n| parsed |\n+--------+\n| -1.234 |\n*--------*/\n\nThis example shows an input with multiple commas in the integer part of the number:\n\n\nSELECT PARSE_BIGNUMERIC(\"  1,2,,3,.45 + \") as parsed;\n\n/*--------*\n| parsed |\n+--------+\n| 123.45 |\n*--------*/\n\nThis example shows an input with a decimal point and no digits in the whole number part:\n\n\nSELECT PARSE_BIGNUMERIC(\".1234  \") as parsed;\n\n/*--------*\n| parsed |\n+--------+\n| 0.1234 |\n*--------*/\n\n**Examples of invalid inputs**\n\nThis example is invalid because the whole number part contains no digits:\n\n\nSELECT PARSE_BIGNUMERIC(\",,,.1234  \") as parsed;\n\nThis example is invalid because there are whitespaces between digits:\n\n\nSELECT PARSE_BIGNUMERIC(\"1  23.4 5  \") as parsed;\n\nThis example is invalid because the number is empty except for an exponent:\n\n\nSELECT PARSE_BIGNUMERIC(\"  e1 \") as parsed;\n\nThis example is invalid because the string contains multiple signs:\n\n\nSELECT PARSE_BIGNUMERIC(\"  - 12.3 - \") as parsed;\n\nThis example is invalid because the value of the number falls outside the range of ` BIGNUMERIC ` :\n\n\nSELECT PARSE_BIGNUMERIC(\"12.34E100 \") as parsed;\n\nThis example is invalid because the string contains invalid characters:\n\n\nSELECT PARSE_BIGNUMERIC(\"$12.34\") as parsed;"
            },
            "PARSE_NUMERIC": {
                "name": "PARSE_NUMERIC",
                "summary": "Converts a ` STRING ` value to a ` NUMERIC ` value.",
                "description": "PARSE_NUMERIC(string_expression)\n\n**Description**\n\nConverts a ` STRING ` to a ` NUMERIC ` value.\n\nThe numeric literal contained in the string must not exceed the [ maximum precision or range ](/bigquery/docs/reference/standard-sql/data-\ntypes#decimal_types) of the ` NUMERIC ` type, or an error occurs. If the number of digits after the decimal point exceeds nine, then the resulting `\nNUMERIC ` value rounds [ half away from zero\n](https://en.wikipedia.org/wiki/Rounding#Round_half_away_from_zero) to have nine digits after the decimal point.\n\n\n-- This example shows how a string with a decimal point is parsed.\nSELECT PARSE_NUMERIC(\"123.45\") AS parsed;\n\n/*--------*\n| parsed |\n+--------+\n| 123.45 |\n*--------*/\n\n-- This example shows how a string with an exponent is parsed.\nSELECT PARSE_NUMERIC(\"12.34E27\") as parsed;\n\n/*-------------------------------*\n| parsed                        |\n+-------------------------------+\n| 12340000000000000000000000000 |\n*-------------------------------*/\n\n-- This example shows the rounding when digits after the decimal point exceeds 9.\nSELECT PARSE_NUMERIC(\"1.0123456789\") as parsed;\n\n/*-------------*\n| parsed      |\n+-------------+\n| 1.012345679 |\n*-------------*/\n\nThis function is similar to using the  ` CAST AS NUMERIC ` function except that the ` PARSE_NUMERIC ` function only accepts string inputs and allows the following in the string:\n\n* Spaces between the sign (+/-) and the number\n* Signs (+/-) after the number\n\nRules for valid input strings:\n\nRule  |  Example Input  |  Output\n---|---|---\nThe string can only contain digits, commas, decimal points and signs.  |  \"-\n12,34567,89.0\"  |  -123456789 Whitespaces are allowed anywhere except between digits.  |  \" - 12.345 \"  |\n-12.345 Only digits and commas are allowed before the decimal point.  |  \" 12,345,678\"\n|  12345678 Only digits are allowed after the decimal point.  |  \"1.234 \"  |  1.234 Use ` E ` or ` e ` for exponents. After the ` e ` , digits and a leading sign indicator are allowed.  |  \" 123.45e-1\"  |  12.345 If the integer part is not empty, then it must contain at least one digit.  |\n\" 0,.12 -\"  |  -0.12 If the string contains a decimal point, then it must contain at least one digit.  |  \" .1\"  |  0.1 The string cannot contain more than one sign.  |  \" 0.5 +\"  |  0.5\n\n**Return Data Type**\n\n` NUMERIC `\n\n**Examples**\n\nThis example shows an input with spaces before, after, and between the sign and the number:\n\n\nSELECT PARSE_NUMERIC(\"  -  12.34 \") as parsed;\n\n/*--------*\n| parsed |\n+--------+\n| -12.34 |\n*--------*/\n\nThis example shows an input with an exponent as well as the sign after the number:\n\n\nSELECT PARSE_NUMERIC(\"12.34e-1-\") as parsed;\n\n/*--------*\n| parsed |\n+--------+\n| -1.234 |\n*--------*/\n\nThis example shows an input with multiple commas in the integer part of the number:\n\n\nSELECT PARSE_NUMERIC(\"  1,2,,3,.45 + \") as parsed;\n\n/*--------*\n| parsed |\n+--------+\n| 123.45 |\n*--------*/\n\nThis example shows an input with a decimal point and no digits in the whole number part:\n\n\nSELECT PARSE_NUMERIC(\".1234  \") as parsed;\n\n/*--------*\n| parsed |\n+--------+\n| 0.1234 |\n*--------*/\n\n**Examples of invalid inputs**\n\nThis example is invalid because the whole number part contains no digits:\n\n\nSELECT PARSE_NUMERIC(\",,,.1234  \") as parsed;\n\nThis example is invalid because there are whitespaces between digits:\n\n\nSELECT PARSE_NUMERIC(\"1  23.4 5  \") as parsed;\n\nThis example is invalid because the number is empty except for an exponent:\n\n\nSELECT PARSE_NUMERIC(\"  e1 \") as parsed;\n\nThis example is invalid because the string contains multiple signs:\n\n\nSELECT PARSE_NUMERIC(\"  - 12.3 - \") as parsed;\n\nThis example is invalid because the value of the number falls outside the range of ` BIGNUMERIC ` :\n\n\nSELECT PARSE_NUMERIC(\"12.34E100 \") as parsed;\n\nThis example is invalid because the string contains invalid characters:\n\n\nSELECT PARSE_NUMERIC(\"$12.34\") as parsed;"
            },
            "SAFE_CAST": {
                "name": "SAFE_CAST",
                "summary": "Similar to the ` CAST ` function, but returns ` NULL ` when a runtime error is produced.",
                "description": "SAFE_CAST(expression AS typename [format_clause])\n\n**Description**\n\nWhen using ` CAST ` , a query can fail if GoogleSQL is unable to perform the cast. For example, the following query generates an error:\n\n\nSELECT CAST(\"apple\" AS INT64) AS not_a_number;\n\nIf you want to protect your queries from these types of errors, you can use `\nSAFE_CAST ` . ` SAFE_CAST ` replaces runtime errors with ` NULL ` s. However,\nduring static analysis, impossible casts between two non-castable types still produce an error because the query is invalid.\n\n\nSELECT SAFE_CAST(\"apple\" AS INT64) AS not_a_number;\n\n/*--------------*\n| not_a_number |\n+--------------+\n| NULL         |\n*--------------*/\n\nSome casts can include a [ format clause ](/bigquery/docs/reference/standard-\nsql/format-elements#formatting_syntax) , which provides instructions for how to conduct the cast. For example, you could instruct a cast to convert a sequence of bytes to a BASE64-encoded string instead of a UTF-8-encoded string.\n\nThe structure of the format clause is unique to each type of cast and more information is available in the section for that cast.\n\nIf you are casting from bytes to strings, you can also use the function, [ `\nSAFE_CONVERT_BYTES_TO_STRING ` ](/bigquery/docs/reference/standard-\nsql/string_functions#safe_convert_bytes_to_string) . Any invalid UTF-8 characters are replaced with the unicode replacement character, ` U+FFFD ` ."
            }
        }
    },
    {
        "category": "date-functions",
        "description": "GoogleSQL for BigQuery supports the following date functions.",
        "source": "date_functions.txt",
        "functions": {
            "CURRENT_DATE": {
                "name": "CURRENT_DATE",
                "summary": "Returns the current date as a ` DATE ` value.",
                "description": "CURRENT_DATE()\n\n\nCURRENT_DATE(time_zone_expression)\n\n\nCURRENT_DATE\n\n**Description**\n\nReturns the current date as a ` DATE ` object. Parentheses are optional when called with no arguments.\n\nThis function supports the following arguments:\n\n* ` time_zone_expression ` : A ` STRING ` expression that represents a [ time zone ](/bigquery/docs/reference/standard-sql/timestamp_functions#timezone_definitions) . If no time zone is specified, the default time zone, UTC, is used. If this expression is used and it evaluates to ` NULL ` , this function returns ` NULL ` .\n\nThe current date is recorded at the start of the query statement which contains this function, not when this specific function is evaluated.\n\n**Return Data Type**\n\n` DATE `\n\n**Examples**\n\nThe following query produces the current date in the default time zone:\n\n\nSELECT CURRENT_DATE() AS the_date;\n\n/*--------------*\n| the_date     |\n+--------------+\n| 2016-12-25   |\n*--------------*/\n\nThe following queries produce the current date in a specified time zone:\n\n\nSELECT CURRENT_DATE('America/Los_Angeles') AS the_date;\n\n/*--------------*\n| the_date     |\n+--------------+\n| 2016-12-25   |\n*--------------*/\n\n\nSELECT CURRENT_DATE('-08') AS the_date;\n\n/*--------------*\n| the_date     |\n+--------------+\n| 2016-12-25   |\n*--------------*/\n\nThe following query produces the current date in the default time zone.\nParentheses are not needed if the function has no arguments.\n\n\nSELECT CURRENT_DATE AS the_date;\n\n/*--------------*\n| the_date     |\n+--------------+\n| 2016-12-25   |\n*--------------*/\n\nWhen a column named ` current_date ` is present, the column name and the function call without parentheses are ambiguous. To ensure the function call,\nadd parentheses; to ensure the column name, qualify it with its [ range variable ](/bigquery/docs/reference/standard-sql/query-syntax#range_variables) . For example, the following query will select the function in the ` the_date\n` column and the table column in the ` current_date ` column.\n\n\nWITH t AS (SELECT 'column value' AS `current_date`) SELECT current_date() AS the_date, t.current_date FROM t;\n\n/*------------+--------------*\n| the_date   | current_date |\n+------------+--------------+\n| 2016-12-25 | column value |\n*------------+--------------*/"
            },
            "DATE": {
                "name": "DATE",
                "summary": "Constructs a ` DATE ` value.",
                "description": "DATE(year, month, day)\n\n\nDATE(timestamp_expression)\n\n\nDATE(timestamp_expression, time_zone_expression)\n\n\nDATE(datetime_expression)\n\n**Description**\n\nConstructs or extracts a date.\n\nThis function supports the following arguments:\n\n* ` year ` : The ` INT64 ` value for year.\n* ` month ` : The ` INT64 ` value for month.\n* ` day ` : The ` INT64 ` value for day.\n* ` timestamp_expression ` : A ` TIMESTAMP ` expression that contains the date.\n* ` time_zone_expression ` : A ` STRING ` expression that represents a [ time zone ](/bigquery/docs/reference/standard-sql/timestamp_functions#timezone_definitions) . If no time zone is specified with ` timestamp_expression ` , the default time zone, UTC, is used.\n* ` datetime_expression ` : A ` DATETIME ` expression that contains the date.\n\n**Return Data Type**\n\n` DATE `\n\n**Example**\n\n\nSELECT DATE(2016, 12, 25) AS date_ymd,\nDATE(DATETIME '2016-12-25 23:59:59') AS date_dt,\nDATE(TIMESTAMP '2016-12-25 05:30:00+07', 'America/Los_Angeles') AS date_tstz;\n\n/*------------+------------+------------*\n| date_ymd   | date_dt    | date_tstz  |\n+------------+------------+------------+\n| 2016-12-25 | 2016-12-25 | 2016-12-24 |\n*------------+------------+------------*/"
            },
            "DATE_ADD": {
                "name": "DATE_ADD",
                "summary": "Adds a specified time interval to a ` DATE ` value.",
                "description": "DATE_ADD(date_expression, INTERVAL int64_expression date_part)\n\n**Description**\n\nAdds a specified time interval to a DATE.\n\n` DATE_ADD ` supports the following ` date_part ` values:\n\n* ` DAY `\n* ` WEEK ` . Equivalent to 7 ` DAY ` s.\n* ` MONTH `\n* ` QUARTER `\n* ` YEAR `\n\nSpecial handling is required for MONTH, QUARTER, and YEAR parts when the date is at (or near) the last day of the month. If the resulting month has fewer days than the original date's day, then the resulting date is the last date of that month.\n\n**Return Data Type**\n\nDATE\n\n**Example**\n\n\nSELECT DATE_ADD(DATE '2008-12-25', INTERVAL 5 DAY) AS five_days_later;\n\n/*--------------------*\n| five_days_later    |\n+--------------------+\n| 2008-12-30         |\n*--------------------*/"
            },
            "DATE_DIFF": {
                "name": "DATE_DIFF",
                "summary": "Gets the number of unit boundaries between two ` DATE `\nvalues at a particular time granularity.",
                "description": "DATE_DIFF(start_date, end_date, granularity)\n\n**Description**\n\nGets the number of unit boundaries between two ` DATE ` values ( ` start_date\n` \\- ` end_date ` ) at a particular time granularity.\n\n**Definitions**\n\n* ` start_datet ` : The starting ` DATE ` value.\n* ` end_date ` : The ending ` DATE ` value.\n* ` granularity ` : The date part that represents the granularity. This can be:\n\n* ` DAY `\n* ` WEEK ` This date part begins on Sunday.\n* ` WEEK(<WEEKDAY>) ` : This date part begins on ` WEEKDAY ` . Valid values for ` WEEKDAY ` are ` SUNDAY ` , ` MONDAY ` , ` TUESDAY ` , ` WEDNESDAY ` , ` THURSDAY ` , ` FRIDAY ` , and ` SATURDAY ` .\n* ` ISOWEEK ` : Uses [ ISO 8601 week ](https://en.wikipedia.org/wiki/ISO_week_date) boundaries. ISO weeks begin on Monday.\n* ` MONTH ` , except when the first two arguments are ` TIMESTAMP ` values.\n* ` QUARTER `\n* ` YEAR `\n* ` ISOYEAR ` : Uses the [ ISO 8601 ](https://en.wikipedia.org/wiki/ISO_8601) week-numbering year boundary. The ISO year boundary is the Monday of the first week whose Thursday belongs to the corresponding Gregorian calendar year.\n\n**Details**\n\nIf ` end_date ` is earlier than ` start_date ` , the output is negative.\n\n**Note:** The behavior of the this function follows the type of arguments passed in. For example, ` DATE_DIFF(TIMESTAMP, TIMESTAMP, PART) ` behaves like\n` TIMESTAMP_DIFF(TIMESTAMP, TIMESTAMP, PART) ` .\n\n**Return Data Type**\n\n` INT64 `\n\n**Example**\n\n\nSELECT DATE_DIFF(DATE '2010-07-07', DATE '2008-12-25', DAY) AS days_diff;\n\n/*-----------*\n| days_diff |\n+-----------+\n| 559       |\n*-----------*/\n\n\nSELECT DATE_DIFF(DATE '2017-10-15', DATE '2017-10-14', DAY) AS days_diff,\nDATE_DIFF(DATE '2017-10-15', DATE '2017-10-14', WEEK) AS weeks_diff;\n\n/*-----------+------------*\n| days_diff | weeks_diff |\n+-----------+------------+\n| 1         | 1          |\n*-----------+------------*/\n\nThe example above shows the result of ` DATE_DIFF ` for two days in succession. ` DATE_DIFF ` with the date part ` WEEK ` returns 1 because `\nDATE_DIFF ` counts the number of date part boundaries in this range of dates.\nEach ` WEEK ` begins on Sunday, so there is one date part boundary between Saturday, 2017-10-14 and Sunday, 2017-10-15.\n\nThe following example shows the result of ` DATE_DIFF ` for two dates in different years. ` DATE_DIFF ` with the date part ` YEAR ` returns 3 because it counts the number of Gregorian calendar year boundaries between the two dates. ` DATE_DIFF ` with the date part ` ISOYEAR ` returns 2 because the second date belongs to the ISO year 2015. The first Thursday of the 2015 calendar year was 2015-01-01, so the ISO year 2015 begins on the preceding Monday, 2014-12-29.\n\n\nSELECT DATE_DIFF('2017-12-30', '2014-12-30', YEAR) AS year_diff,\nDATE_DIFF('2017-12-30', '2014-12-30', ISOYEAR) AS isoyear_diff;\n\n/*-----------+--------------*\n| year_diff | isoyear_diff |\n+-----------+--------------+\n| 3         | 2            |\n*-----------+--------------*/\n\nThe following example shows the result of ` DATE_DIFF ` for two days in succession. The first date falls on a Monday and the second date falls on a Sunday. ` DATE_DIFF ` with the date part ` WEEK ` returns 0 because this date part uses weeks that begin on Sunday. ` DATE_DIFF ` with the date part `\nWEEK(MONDAY) ` returns 1. ` DATE_DIFF ` with the date part ` ISOWEEK ` also returns 1 because ISO weeks begin on Monday.\n\n\nSELECT DATE_DIFF('2017-12-18', '2017-12-17', WEEK) AS week_diff,\nDATE_DIFF('2017-12-18', '2017-12-17', WEEK(MONDAY)) AS week_weekday_diff,\nDATE_DIFF('2017-12-18', '2017-12-17', ISOWEEK) AS isoweek_diff;\n\n/*-----------+-------------------+--------------*\n| week_diff | week_weekday_diff | isoweek_diff |\n+-----------+-------------------+--------------+\n| 0         | 1                 | 1            |\n*-----------+-------------------+--------------*/"
            },
            "DATE_FROM_UNIX_DATE": {
                "name": "DATE_FROM_UNIX_DATE",
                "summary": "Interprets an ` INT64 ` expression as the number of days since 1970-01-01.",
                "description": "DATE_FROM_UNIX_DATE(int64_expression)\n\n**Description**\n\nInterprets ` int64_expression ` as the number of days since 1970-01-01.\n\n**Return Data Type**\n\nDATE\n\n**Example**\n\n\nSELECT DATE_FROM_UNIX_DATE(14238) AS date_from_epoch;\n\n/*-----------------*\n| date_from_epoch |\n+-----------------+\n| 2008-12-25      |\n*-----------------+*/"
            },
            "DATE_SUB": {
                "name": "DATE_SUB",
                "summary": "Subtracts a specified time interval from a ` DATE ` value.",
                "description": "DATE_SUB(date_expression, INTERVAL int64_expression date_part)\n\n**Description**\n\nSubtracts a specified time interval from a DATE.\n\n` DATE_SUB ` supports the following ` date_part ` values:\n\n* ` DAY `\n* ` WEEK ` . Equivalent to 7 ` DAY ` s.\n* ` MONTH `\n* ` QUARTER `\n* ` YEAR `\n\nSpecial handling is required for MONTH, QUARTER, and YEAR parts when the date is at (or near) the last day of the month. If the resulting month has fewer days than the original date's day, then the resulting date is the last date of that month.\n\n**Return Data Type**\n\nDATE\n\n**Example**\n\n\nSELECT DATE_SUB(DATE '2008-12-25', INTERVAL 5 DAY) AS five_days_ago;\n\n/*---------------*\n| five_days_ago |\n+---------------+\n| 2008-12-20    |\n*---------------*/"
            },
            "DATE_TRUNC": {
                "name": "DATE_TRUNC",
                "summary": "Truncates a ` DATE ` value.",
                "description": "DATE_TRUNC(date_expression, date_part)\n\n**Description**\n\nTruncates a ` DATE ` value to the granularity of ` date_part ` . The ` DATE `\nvalue is always rounded to the beginning of ` date_part ` , which can be one of the following:\n\n* ` DAY ` : The day in the Gregorian calendar year that contains the ` DATE ` value.\n* ` WEEK ` : The first day of the week in the week that contains the ` DATE ` value. Weeks begin on Sundays. ` WEEK ` is equivalent to ` WEEK(SUNDAY) ` .\n* ` WEEK(WEEKDAY) ` : The first day of the week in the week that contains the ` DATE ` value. Weeks begin on ` WEEKDAY ` . ` WEEKDAY ` must be one of the following: ` SUNDAY ` , ` MONDAY ` , ` TUESDAY ` , ` WEDNESDAY ` , ` THURSDAY ` , ` FRIDAY ` , or ` SATURDAY ` .\n* ` ISOWEEK ` : The first day of the [ ISO 8601 week ](https://en.wikipedia.org/wiki/ISO_week_date) in the ISO week that contains the ` DATE ` value. The ISO week begins on Monday. The first ISO week of each ISO year contains the first Thursday of the corresponding Gregorian calendar year.\n* ` MONTH ` : The first day of the month in the month that contains the ` DATE ` value.\n* ` QUARTER ` : The first day of the quarter in the quarter that contains the ` DATE ` value.\n* ` YEAR ` : The first day of the year in the year that contains the ` DATE ` value.\n* ` ISOYEAR ` : The first day of the [ ISO 8601 ](https://en.wikipedia.org/wiki/ISO_8601) week-numbering year in the ISO year that contains the ` DATE ` value. The ISO year is the Monday of the first week whose Thursday belongs to the corresponding Gregorian calendar year.\n\n**Return Data Type**\n\nDATE\n\n**Examples**\n\n\nSELECT DATE_TRUNC(DATE '2008-12-25', MONTH) AS month;\n\n/*------------*\n| month      |\n+------------+\n| 2008-12-01 |\n*------------*/\n\nIn the following example, the original date falls on a Sunday. Because the `\ndate_part ` is ` WEEK(MONDAY) ` , ` DATE_TRUNC ` returns the ` DATE ` for the preceding Monday.\n\n\nSELECT date AS original, DATE_TRUNC(date, WEEK(MONDAY)) AS truncated FROM (SELECT DATE('2017-11-05') AS date);\n\n/*------------+------------*\n| original   | truncated  |\n+------------+------------+\n| 2017-11-05 | 2017-10-30 |\n*------------+------------*/\n\nIn the following example, the original ` date_expression ` is in the Gregorian calendar year 2015. However, ` DATE_TRUNC ` with the ` ISOYEAR ` date part truncates the ` date_expression ` to the beginning of the ISO year, not the Gregorian calendar year. The first Thursday of the 2015 calendar year was 2015-01-01, so the ISO year 2015 begins on the preceding Monday, 2014-12-29.\nTherefore the ISO year boundary preceding the ` date_expression ` 2015-06-15 is 2014-12-29.\n\n\nSELECT DATE_TRUNC('2015-06-15', ISOYEAR) AS isoyear_boundary,\nEXTRACT(ISOYEAR FROM DATE '2015-06-15') AS isoyear_number;\n\n/*------------------+----------------*\n| isoyear_boundary | isoyear_number |\n+------------------+----------------+\n| 2014-12-29       | 2015           |\n*------------------+----------------*/"
            },
            "EXTRACT": {
                "name": "EXTRACT",
                "summary": "Extracts part of a date from a ` DATE ` value.",
                "description": "EXTRACT(part FROM date_expression)\n\n**Description**\n\nReturns the value corresponding to the specified date part. The ` part ` must be one of:\n\n* ` DAYOFWEEK ` : Returns values in the range [1,7] with Sunday as the first day of the week.\n* ` DAY `\n* ` DAYOFYEAR `\n* ` WEEK ` : Returns the week number of the date in the range [0, 53]. Weeks begin with Sunday, and dates prior to the first Sunday of the year are in week 0.\n* ` WEEK(<WEEKDAY>) ` : Returns the week number of the date in the range [0, 53]. Weeks begin on ` WEEKDAY ` . Dates prior to the first ` WEEKDAY ` of the year are in week 0. Valid values for ` WEEKDAY ` are ` SUNDAY ` , ` MONDAY ` , ` TUESDAY ` , ` WEDNESDAY ` , ` THURSDAY ` , ` FRIDAY ` , and ` SATURDAY ` .\n* ` ISOWEEK ` : Returns the [ ISO 8601 week ](https://en.wikipedia.org/wiki/ISO_week_date) number of the ` date_expression ` . ` ISOWEEK ` s begin on Monday. Return values are in the range [1, 53]. The first ` ISOWEEK ` of each ISO year begins on the Monday before the first Thursday of the Gregorian calendar year.\n* ` MONTH `\n* ` QUARTER ` : Returns values in the range [1,4].\n* ` YEAR `\n* ` ISOYEAR ` : Returns the [ ISO 8601 ](https://en.wikipedia.org/wiki/ISO_8601) week-numbering year, which is the Gregorian calendar year containing the Thursday of the week to which ` date_expression ` belongs.\n\n**Return Data Type**\n\nINT64\n\n**Examples**\n\nIn the following example, ` EXTRACT ` returns a value corresponding to the `\nDAY ` date part.\n\n\nSELECT EXTRACT(DAY FROM DATE '2013-12-25') AS the_day;\n\n/*---------*\n| the_day |\n+---------+\n| 25      |\n*---------*/\n\nIn the following example, ` EXTRACT ` returns values corresponding to different date parts from a column of dates near the end of the year.\n\n\nSELECT date,\nEXTRACT(ISOYEAR FROM date) AS isoyear,\nEXTRACT(ISOWEEK FROM date) AS isoweek,\nEXTRACT(YEAR FROM date) AS year,\nEXTRACT(WEEK FROM date) AS week FROM UNNEST(GENERATE_DATE_ARRAY('2015-12-23', '2016-01-09')) AS date ORDER BY date;\n\n/*------------+---------+---------+------+------*\n| date       | isoyear | isoweek | year | week |\n+------------+---------+---------+------+------+\n| 2015-12-23 | 2015    | 52      | 2015 | 51   |\n| 2015-12-24 | 2015    | 52      | 2015 | 51   |\n| 2015-12-25 | 2015    | 52      | 2015 | 51   |\n| 2015-12-26 | 2015    | 52      | 2015 | 51   |\n| 2015-12-27 | 2015    | 52      | 2015 | 52   |\n| 2015-12-28 | 2015    | 53      | 2015 | 52   |\n| 2015-12-29 | 2015    | 53      | 2015 | 52   |\n| 2015-12-30 | 2015    | 53      | 2015 | 52   |\n| 2015-12-31 | 2015    | 53      | 2015 | 52   |\n| 2016-01-01 | 2015    | 53      | 2016 | 0    |\n| 2016-01-02 | 2015    | 53      | 2016 | 0    |\n| 2016-01-03 | 2015    | 53      | 2016 | 1    |\n| 2016-01-04 | 2016    | 1       | 2016 | 1    |\n| 2016-01-05 | 2016    | 1       | 2016 | 1    |\n| 2016-01-06 | 2016    | 1       | 2016 | 1    |\n| 2016-01-07 | 2016    | 1       | 2016 | 1    |\n| 2016-01-08 | 2016    | 1       | 2016 | 1    |\n| 2016-01-09 | 2016    | 1       | 2016 | 1    |\n*------------+---------+---------+------+------*/\n\nIn the following example, ` date_expression ` falls on a Sunday. ` EXTRACT `\ncalculates the first column using weeks that begin on Sunday, and it calculates the second column using weeks that begin on Monday.\n\n\nWITH table AS (SELECT DATE('2017-11-05') AS date) SELECT date,\nEXTRACT(WEEK(SUNDAY) FROM date) AS week_sunday,\nEXTRACT(WEEK(MONDAY) FROM date) AS week_monday FROM table;\n\n/*------------+-------------+-------------*\n| date       | week_sunday | week_monday |\n+------------+-------------+-------------+\n| 2017-11-05 | 45          | 44          |\n*------------+-------------+-------------*/"
            },
            "FORMAT_DATE": {
                "name": "FORMAT_DATE",
                "summary": "Formats a ` DATE ` value according to a specified format string.",
                "description": "FORMAT_DATE(format_string, date_expr)\n\n**Description**\n\nFormats the ` date_expr ` according to the specified ` format_string ` .\n\nSee [ Supported Format Elements For DATE ](/bigquery/docs/reference/standard-\nsql/format-elements#format_elements_date_time) for a list of format elements that this function supports.\n\n**Return Data Type**\n\nSTRING\n\n**Examples**\n\n\nSELECT FORMAT_DATE('%x', DATE '2008-12-25') AS US_format;\n\n/*------------*\n| US_format  |\n+------------+\n| 12/25/08   |\n*------------*/\n\n\nSELECT FORMAT_DATE('%b-%d-%Y', DATE '2008-12-25') AS formatted;\n\n/*-------------*\n| formatted   |\n+-------------+\n| Dec-25-2008 |\n*-------------*/\n\n\nSELECT FORMAT_DATE('%b %Y', DATE '2008-12-25') AS formatted;\n\n/*-------------*\n| formatted   |\n+-------------+\n| Dec 2008    |\n*-------------*/"
            },
            "LAST_DAY": {
                "name": "LAST_DAY",
                "summary": "Gets the last day in a specified time period that contains a `\nDATE ` value.",
                "description": "LAST_DAY(date_expression[, date_part])\n\n**Description**\n\nReturns the last day from a date expression. This is commonly used to return the last day of the month.\n\nYou can optionally specify the date part for which the last day is returned.\nIf this parameter is not used, the default value is ` MONTH ` . ` LAST_DAY `\nsupports the following values for ` date_part ` :\n\n* ` YEAR `\n* ` QUARTER `\n* ` MONTH `\n* ` WEEK ` . Equivalent to 7 ` DAY ` s.\n* ` WEEK(<WEEKDAY>) ` . ` <WEEKDAY> ` represents the starting day of the week. Valid values are ` SUNDAY ` , ` MONDAY ` , ` TUESDAY ` , ` WEDNESDAY ` , ` THURSDAY ` , ` FRIDAY ` , and ` SATURDAY ` .\n* ` ISOWEEK ` . Uses [ ISO 8601 ](https://en.wikipedia.org/wiki/ISO_week_date) week boundaries. ISO weeks begin on Monday.\n* ` ISOYEAR ` . Uses the [ ISO 8601 ](https://en.wikipedia.org/wiki/ISO_8601) week-numbering year boundary. The ISO year boundary is the Monday of the first week whose Thursday belongs to the corresponding Gregorian calendar year.\n\n**Return Data Type**\n\n` DATE `\n\n**Example**\n\nThese both return the last day of the month:\n\n\nSELECT LAST_DAY(DATE '2008-11-25', MONTH) AS last_day\n\n/*------------*\n| last_day   |\n+------------+\n| 2008-11-30 |\n*------------*/\n\n\nSELECT LAST_DAY(DATE '2008-11-25') AS last_day\n\n/*------------*\n| last_day   |\n+------------+\n| 2008-11-30 |\n*------------*/\n\nThis returns the last day of the year:\n\n\nSELECT LAST_DAY(DATE '2008-11-25', YEAR) AS last_day\n\n/*------------*\n| last_day   |\n+------------+\n| 2008-12-31 |\n*------------*/\n\nThis returns the last day of the week for a week that starts on a Sunday:\n\n\nSELECT LAST_DAY(DATE '2008-11-10', WEEK(SUNDAY)) AS last_day\n\n/*------------*\n| last_day   |\n+------------+\n| 2008-11-15 |\n*------------*/\n\nThis returns the last day of the week for a week that starts on a Monday:\n\n\nSELECT LAST_DAY(DATE '2008-11-10', WEEK(MONDAY)) AS last_day\n\n/*------------*\n| last_day   |\n+------------+\n| 2008-11-16 |\n*------------*/"
            },
            "PARSE_DATE": {
                "name": "PARSE_DATE",
                "summary": "Converts a ` STRING ` value to a ` DATE ` value.",
                "description": "PARSE_DATE(format_string, date_string)\n\n**Description**\n\nConverts a  string representation of date  to a ` DATE ` object.\n\n` format_string ` contains the [ format elements\n](/bigquery/docs/reference/standard-sql/format-\nelements#format_elements_date_time) that define how ` date_string ` is formatted. Each element in ` date_string ` must have a corresponding element in ` format_string ` . The location of each element in ` format_string ` must match the location of each element in ` date_string ` .\n\n\n-- This works because elements on both sides match.\nSELECT PARSE_DATE('%A %b %e %Y', 'Thursday Dec 25 2008');\n\n-- This produces an error because the year element is in different locations.\nSELECT PARSE_DATE('%Y %A %b %e', 'Thursday Dec 25 2008');\n\n-- This produces an error because one of the year elements is missing.\nSELECT PARSE_DATE('%A %b %e', 'Thursday Dec 25 2008');\n\n-- This works because %F can find all matching elements in date_string.\nSELECT PARSE_DATE('%F', '2000-12-30');\n\nWhen using ` PARSE_DATE ` , keep the following in mind:\n\n* **Unspecified fields.** Any unspecified field is initialized from ` 1970-01-01 ` .\n* **Case insensitivity.** Names, such as ` Monday ` , ` February ` , and so on, are case insensitive.\n* **Whitespace.** One or more consecutive white spaces in the format string matches zero or more consecutive white spaces in the date string. In addition, leading and trailing white spaces in the date string are always allowed -- even if they are not in the format string.\n* **Format precedence.** When two (or more) format elements have overlapping information (for example both ` %F ` and ` %Y ` affect the year), the last one generally overrides any earlier ones.\n\n**Return Data Type**\n\nDATE\n\n**Examples**\n\nThis example converts a ` MM/DD/YY ` formatted string to a ` DATE ` object:\n\n\nSELECT PARSE_DATE('%x', '12/25/08') AS parsed;\n\n/*------------*\n| parsed     |\n+------------+\n| 2008-12-25 |\n*------------*/\n\nThis example converts a ` YYYYMMDD ` formatted string to a ` DATE ` object:\n\n\nSELECT PARSE_DATE('%Y%m%d', '20081225') AS parsed;\n\n/*------------*\n| parsed     |\n+------------+\n| 2008-12-25 |\n*------------*/"
            },
            "UNIX_DATE": {
                "name": "UNIX_DATE",
                "summary": "Converts a ` DATE ` value to the number of days since 1970-01-01.",
                "description": "UNIX_DATE(date_expression)\n\n**Description**\n\nReturns the number of days since ` 1970-01-01 ` .\n\n**Return Data Type**\n\nINT64\n\n**Example**\n\n\nSELECT UNIX_DATE(DATE '2008-12-25') AS days_from_epoch;\n\n/*-----------------*\n| days_from_epoch |\n+-----------------+\n| 14238           |\n*-----------------*/"
            }
        }
    },
    {
        "category": "datetime-functions",
        "description": "GoogleSQL for BigQuery supports the following datetime functions.\n\nAll outputs are automatically formatted as per [ ISO 8601\n](https://en.wikipedia.org/wiki/ISO_8601) , separating date and time with a `\nT ` .",
        "source": "datetime_functions.txt",
        "functions": {
            "CURRENT_DATETIME": {
                "name": "CURRENT_DATETIME",
                "summary": "Returns the current date and time as a ` DATETIME `\nvalue.",
                "description": "CURRENT_DATETIME([time_zone])\n\n\nCURRENT_DATETIME\n\n**Description**\n\nReturns the current time as a ` DATETIME ` object. Parentheses are optional when called with no arguments.\n\nThis function supports an optional ` time_zone ` parameter. See [ Time zone definitions ](/bigquery/docs/reference/standard-\nsql/timestamp_functions#timezone_definitions) for information on how to specify a time zone.\n\nThe current date and time is recorded at the start of the query statement which contains this function, not when this specific function is evaluated.\n\n**Return Data Type**\n\n` DATETIME `\n\n**Example**\n\n\nSELECT CURRENT_DATETIME() as now;\n\n/*----------------------------*\n| now                        |\n+----------------------------+\n| 2016-05-19T10:38:47.046465 |\n*----------------------------*/\n\nWhen a column named ` current_datetime ` is present, the column name and the function call without parentheses are ambiguous. To ensure the function call,\nadd parentheses; to ensure the column name, qualify it with its [ range variable ](/bigquery/docs/reference/standard-sql/query-syntax#range_variables) . For example, the following query will select the function in the ` now `\ncolumn and the table column in the ` current_datetime ` column.\n\n\nWITH t AS (SELECT 'column value' AS `current_datetime`) SELECT current_datetime() as now, t.current_datetime FROM t;\n\n/*----------------------------+------------------*\n| now                        | current_datetime |\n+----------------------------+------------------+\n| 2016-05-19T10:38:47.046465 | column value     |\n*----------------------------+------------------*/"
            },
            "DATETIME": {
                "name": "DATETIME",
                "summary": "Constructs a ` DATETIME ` value.",
                "description": "1. DATETIME(year, month, day, hour, minute, second) 2. DATETIME(date_expression[, time_expression]) 3. DATETIME(timestamp_expression [, time_zone])\n\n**Description**\n\n1. Constructs a ` DATETIME ` object using ` INT64 ` values representing the year, month, day, hour, minute, and second.\n2. Constructs a ` DATETIME ` object using a DATE object and an optional ` TIME ` object.\n3. Constructs a ` DATETIME ` object using a ` TIMESTAMP ` object. It supports an optional parameter to [ specify a time zone ](/bigquery/docs/reference/standard-sql/timestamp_functions#timezone_definitions) . If no time zone is specified, the default time zone, UTC, is used.\n\n**Return Data Type**\n\n` DATETIME `\n\n**Example**\n\n\nSELECT DATETIME(2008, 12, 25, 05, 30, 00) as datetime_ymdhms,\nDATETIME(TIMESTAMP \"2008-12-25 05:30:00+00\", \"America/Los_Angeles\") as datetime_tstz;\n\n/*---------------------+---------------------*\n| datetime_ymdhms     | datetime_tstz       |\n+---------------------+---------------------+\n| 2008-12-25T05:30:00 | 2008-12-24T21:30:00 |\n*---------------------+---------------------*/"
            },
            "DATETIME_ADD": {
                "name": "DATETIME_ADD",
                "summary": "Adds a specified time interval to a ` DATETIME ` value.",
                "description": "DATETIME_ADD(datetime_expression, INTERVAL int64_expression part)\n\n**Description**\n\nAdds ` int64_expression ` units of ` part ` to the ` DATETIME ` object.\n\n` DATETIME_ADD ` supports the following values for ` part ` :\n\n* ` MICROSECOND `\n* ` MILLISECOND `\n* ` SECOND `\n* ` MINUTE `\n* ` HOUR `\n* ` DAY `\n* ` WEEK ` . Equivalent to 7 ` DAY ` s.\n* ` MONTH `\n* ` QUARTER `\n* ` YEAR `\n\nSpecial handling is required for MONTH, QUARTER, and YEAR parts when the date is at (or near) the last day of the month. If the resulting month has fewer days than the original DATETIME's day, then the result day is the last day of the new month.\n\n**Return Data Type**\n\n` DATETIME `\n\n**Example**\n\n\nSELECT DATETIME \"2008-12-25 15:30:00\" as original_date,\nDATETIME_ADD(DATETIME \"2008-12-25 15:30:00\", INTERVAL 10 MINUTE) as later;\n\n/*-----------------------------+------------------------*\n| original_date               | later                  |\n+-----------------------------+------------------------+\n| 2008-12-25T15:30:00         | 2008-12-25T15:40:00    |\n*-----------------------------+------------------------*/"
            },
            "DATETIME_DIFF": {
                "name": "DATETIME_DIFF",
                "summary": "Gets the number of unit boundaries between two ` DATETIME\n` values at a particular time granularity.",
                "description": "DATETIME_DIFF(start_datetime, end_datetime, granularity)\n\n**Description**\n\nGets the number of unit boundaries between two ` DATETIME ` values ( `\nstart_datetime ` \\- ` end_datetime ` ) at a particular time granularity.\n\n**Definitions**\n\n* ` start_datetime ` : The starting ` DATETIME ` value.\n* ` end_datetime ` : The ending ` DATETIME ` value.\n* ` granularity ` : The datetime part that represents the granularity. This can be:\n\n* ` MICROSECOND `\n* ` MILLISECOND `\n* ` SECOND `\n* ` MINUTE `\n* ` HOUR `\n* ` DAY `\n* ` WEEK ` : This date part begins on Sunday.\n* ` WEEK(<WEEKDAY>) ` : This date part begins on ` WEEKDAY ` . Valid values for ` WEEKDAY ` are ` SUNDAY ` , ` MONDAY ` , ` TUESDAY ` , ` WEDNESDAY ` , ` THURSDAY ` , ` FRIDAY ` , and ` SATURDAY ` .\n* ` ISOWEEK ` : Uses [ ISO 8601 week ](https://en.wikipedia.org/wiki/ISO_week_date) boundaries. ISO weeks begin on Monday.\n* ` MONTH ` , except when the first two arguments are ` TIMESTAMP ` values.\n* ` QUARTER `\n* ` YEAR `\n* ` ISOYEAR ` : Uses the [ ISO 8601 ](https://en.wikipedia.org/wiki/ISO_8601) week-numbering year boundary. The ISO year boundary is the Monday of the first week whose Thursday belongs to the corresponding Gregorian calendar year.\n\n**Details**\n\nIf ` end_datetime ` is earlier than ` start_datetime ` , the output is negative. Produces an error if the computation overflows, such as if the difference in microseconds between the two ` DATETIME ` values overflows.\n\n**Note:** The behavior of the this function follows the type of arguments passed in. For example, ` DATETIME_DIFF(TIMESTAMP, TIMESTAMP, PART) ` behaves like ` TIMESTAMP_DIFF(TIMESTAMP, TIMESTAMP, PART) ` .\n\n**Return Data Type**\n\n` INT64 `\n\n**Example**\n\n\nSELECT DATETIME \"2010-07-07 10:20:00\" as first_datetime,\nDATETIME \"2008-12-25 15:30:00\" as second_datetime,\nDATETIME_DIFF(DATETIME \"2010-07-07 10:20:00\",\nDATETIME \"2008-12-25 15:30:00\", DAY) as difference;\n\n/*----------------------------+------------------------+------------------------*\n| first_datetime             | second_datetime        | difference             |\n+----------------------------+------------------------+------------------------+\n| 2010-07-07T10:20:00        | 2008-12-25T15:30:00    | 559                    |\n*----------------------------+------------------------+------------------------*/\n\n\nSELECT DATETIME_DIFF(DATETIME '2017-10-15 00:00:00',\nDATETIME '2017-10-14 00:00:00', DAY) as days_diff,\nDATETIME_DIFF(DATETIME '2017-10-15 00:00:00',\nDATETIME '2017-10-14 00:00:00', WEEK) as weeks_diff;\n\n/*-----------+------------*\n| days_diff | weeks_diff |\n+-----------+------------+\n| 1         | 1          |\n*-----------+------------*/\n\nThe example above shows the result of ` DATETIME_DIFF ` for two ` DATETIME ` s that are 24 hours apart. ` DATETIME_DIFF ` with the part ` WEEK ` returns 1 because ` DATETIME_DIFF ` counts the number of part boundaries in this range of ` DATETIME ` s. Each ` WEEK ` begins on Sunday, so there is one part boundary between Saturday, ` 2017-10-14 00:00:00 ` and Sunday, ` 2017-10-15 00:00:00 ` .\n\nThe following example shows the result of ` DATETIME_DIFF ` for two dates in different years. ` DATETIME_DIFF ` with the date part ` YEAR ` returns 3 because it counts the number of Gregorian calendar year boundaries between the two ` DATETIME ` s. ` DATETIME_DIFF ` with the date part ` ISOYEAR ` returns 2 because the second ` DATETIME ` belongs to the ISO year 2015. The first Thursday of the 2015 calendar year was 2015-01-01, so the ISO year 2015 begins on the preceding Monday, 2014-12-29.\n\n\nSELECT DATETIME_DIFF('2017-12-30 00:00:00',\n'2014-12-30 00:00:00', YEAR) AS year_diff,\nDATETIME_DIFF('2017-12-30 00:00:00',\n'2014-12-30 00:00:00', ISOYEAR) AS isoyear_diff;\n\n/*-----------+--------------*\n| year_diff | isoyear_diff |\n+-----------+--------------+\n| 3         | 2            |\n*-----------+--------------*/\n\nThe following example shows the result of ` DATETIME_DIFF ` for two days in succession. The first date falls on a Monday and the second date falls on a Sunday. ` DATETIME_DIFF ` with the date part ` WEEK ` returns 0 because this time part uses weeks that begin on Sunday. ` DATETIME_DIFF ` with the date part ` WEEK(MONDAY) ` returns 1. ` DATETIME_DIFF ` with the date part `\nISOWEEK ` also returns 1 because ISO weeks begin on Monday.\n\n\nSELECT DATETIME_DIFF('2017-12-18', '2017-12-17', WEEK) AS week_diff,\nDATETIME_DIFF('2017-12-18', '2017-12-17', WEEK(MONDAY)) AS week_weekday_diff,\nDATETIME_DIFF('2017-12-18', '2017-12-17', ISOWEEK) AS isoweek_diff;\n\n/*-----------+-------------------+--------------*\n| week_diff | week_weekday_diff | isoweek_diff |\n+-----------+-------------------+--------------+\n| 0         | 1                 | 1            |\n*-----------+-------------------+--------------*/"
            },
            "DATETIME_SUB": {
                "name": "DATETIME_SUB",
                "summary": "Subtracts a specified time interval from a ` DATETIME `\nvalue.",
                "description": "DATETIME_SUB(datetime_expression, INTERVAL int64_expression part)\n\n**Description**\n\nSubtracts ` int64_expression ` units of ` part ` from the ` DATETIME ` .\n\n` DATETIME_SUB ` supports the following values for ` part ` :\n\n* ` MICROSECOND `\n* ` MILLISECOND `\n* ` SECOND `\n* ` MINUTE `\n* ` HOUR `\n* ` DAY `\n* ` WEEK ` . Equivalent to 7 ` DAY ` s.\n* ` MONTH `\n* ` QUARTER `\n* ` YEAR `\n\nSpecial handling is required for ` MONTH ` , ` QUARTER ` , and ` YEAR ` parts when the date is at (or near) the last day of the month. If the resulting month has fewer days than the original ` DATETIME ` 's day, then the result day is the last day of the new month.\n\n**Return Data Type**\n\n` DATETIME `\n\n**Example**\n\n\nSELECT DATETIME \"2008-12-25 15:30:00\" as original_date,\nDATETIME_SUB(DATETIME \"2008-12-25 15:30:00\", INTERVAL 10 MINUTE) as earlier;\n\n/*-----------------------------+------------------------*\n| original_date               | earlier                |\n+-----------------------------+------------------------+\n| 2008-12-25T15:30:00         | 2008-12-25T15:20:00    |\n*-----------------------------+------------------------*/"
            },
            "DATETIME_TRUNC": {
                "name": "DATETIME_TRUNC",
                "summary": "Truncates a ` DATETIME ` value.",
                "description": "DATETIME_TRUNC(datetime_expression, date_time_part)\n\n**Description**\n\nTruncates a ` DATETIME ` value to the granularity of ` date_time_part ` . The\n` DATETIME ` value is always rounded to the beginning of ` date_time_part ` ,\nwhich can be one of the following:\n\n* ` MICROSECOND ` : If used, nothing is truncated from the value.\n* ` MILLISECOND ` : The nearest lessor or equal millisecond.\n* ` SECOND ` : The nearest lessor or equal second.\n* ` MINUTE ` : The nearest lessor or equal minute.\n* ` HOUR ` : The nearest lessor or equal hour.\n* ` DAY ` : The day in the Gregorian calendar year that contains the ` DATETIME ` value.\n* ` WEEK ` : The first day of the week in the week that contains the ` DATETIME ` value. Weeks begin on Sundays. ` WEEK ` is equivalent to ` WEEK(SUNDAY) ` .\n* ` WEEK(WEEKDAY) ` : The first day of the week in the week that contains the ` DATETIME ` value. Weeks begin on ` WEEKDAY ` . ` WEEKDAY ` must be one of the following: ` SUNDAY ` , ` MONDAY ` , ` TUESDAY ` , ` WEDNESDAY ` , ` THURSDAY ` , ` FRIDAY ` , or ` SATURDAY ` .\n* ` ISOWEEK ` : The first day of the [ ISO 8601 week ](https://en.wikipedia.org/wiki/ISO_week_date) in the ISO week that contains the ` DATETIME ` value. The ISO week begins on Monday. The first ISO week of each ISO year contains the first Thursday of the corresponding Gregorian calendar year.\n* ` MONTH ` : The first day of the month in the month that contains the ` DATETIME ` value.\n* ` QUARTER ` : The first day of the quarter in the quarter that contains the ` DATETIME ` value.\n* ` YEAR ` : The first day of the year in the year that contains the ` DATETIME ` value.\n* ` ISOYEAR ` : The first day of the [ ISO 8601 ](https://en.wikipedia.org/wiki/ISO_8601) week-numbering year in the ISO year that contains the ` DATETIME ` value. The ISO year is the Monday of the first week whose Thursday belongs to the corresponding Gregorian calendar year.\n\n**Return Data Type**\n\n` DATETIME `\n\n**Examples**\n\n\nSELECT DATETIME \"2008-12-25 15:30:00\" as original,\nDATETIME_TRUNC(DATETIME \"2008-12-25 15:30:00\", DAY) as truncated;\n\n/*----------------------------+------------------------*\n| original                   | truncated              |\n+----------------------------+------------------------+\n| 2008-12-25T15:30:00        | 2008-12-25T00:00:00    |\n*----------------------------+------------------------*/\n\nIn the following example, the original ` DATETIME ` falls on a Sunday. Because the ` part ` is ` WEEK(MONDAY) ` , ` DATE_TRUNC ` returns the ` DATETIME ` for the preceding Monday.\n\n\nSELECT datetime AS original,\nDATETIME_TRUNC(datetime, WEEK(MONDAY)) AS truncated FROM (SELECT DATETIME(TIMESTAMP \"2017-11-05 00:00:00+00\", \"UTC\") AS datetime);\n\n/*---------------------+---------------------*\n| original            | truncated           |\n+---------------------+---------------------+\n| 2017-11-05T00:00:00 | 2017-10-30T00:00:00 |\n*---------------------+---------------------*/\n\nIn the following example, the original ` datetime_expression ` is in the Gregorian calendar year 2015. However, ` DATETIME_TRUNC ` with the ` ISOYEAR `\ndate part truncates the ` datetime_expression ` to the beginning of the ISO year, not the Gregorian calendar year. The first Thursday of the 2015 calendar year was 2015-01-01, so the ISO year 2015 begins on the preceding Monday,\n2014-12-29. Therefore the ISO year boundary preceding the `\ndatetime_expression ` 2015-06-15 00:00:00 is 2014-12-29.\n\n\nSELECT DATETIME_TRUNC('2015-06-15 00:00:00', ISOYEAR) AS isoyear_boundary,\nEXTRACT(ISOYEAR FROM DATETIME '2015-06-15 00:00:00') AS isoyear_number;\n\n/*---------------------+----------------*\n| isoyear_boundary    | isoyear_number |\n+---------------------+----------------+\n| 2014-12-29T00:00:00 | 2015           |\n*---------------------+----------------*/"
            },
            "EXTRACT": {
                "name": "EXTRACT",
                "summary": "Extracts part of a date and time from a ` DATETIME ` value.",
                "description": "EXTRACT(part FROM datetime_expression)\n\n**Description**\n\nReturns a value that corresponds to the specified ` part ` from a supplied `\ndatetime_expression ` .\n\nAllowed ` part ` values are:\n\n* ` MICROSECOND `\n* ` MILLISECOND `\n* ` SECOND `\n* ` MINUTE `\n* ` HOUR `\n* ` DAYOFWEEK ` : Returns values in the range [1,7] with Sunday as the first day of of the week.\n* ` DAY `\n* ` DAYOFYEAR `\n* ` WEEK ` : Returns the week number of the date in the range [0, 53]. Weeks begin with Sunday, and dates prior to the first Sunday of the year are in week 0.\n* ` WEEK(<WEEKDAY>) ` : Returns the week number of ` datetime_expression ` in the range [0, 53]. Weeks begin on ` WEEKDAY ` . ` datetime ` s prior to the first ` WEEKDAY ` of the year are in week 0. Valid values for ` WEEKDAY ` are ` SUNDAY ` , ` MONDAY ` , ` TUESDAY ` , ` WEDNESDAY ` , ` THURSDAY ` , ` FRIDAY ` , and ` SATURDAY ` .\n* ` ISOWEEK ` : Returns the [ ISO 8601 week ](https://en.wikipedia.org/wiki/ISO_week_date) number of the ` datetime_expression ` . ` ISOWEEK ` s begin on Monday. Return values are in the range [1, 53]. The first ` ISOWEEK ` of each ISO year begins on the Monday before the first Thursday of the Gregorian calendar year.\n* ` MONTH `\n* ` QUARTER `\n* ` YEAR `\n* ` ISOYEAR ` : Returns the [ ISO 8601 ](https://en.wikipedia.org/wiki/ISO_8601) week-numbering year, which is the Gregorian calendar year containing the Thursday of the week to which ` date_expression ` belongs.\n* ` DATE `\n* ` TIME `\n\nReturned values truncate lower order time periods. For example, when extracting seconds, ` EXTRACT ` truncates the millisecond and microsecond values.\n\n**Return Data Type**\n\n` INT64 ` , except in the following cases:\n\n* If ` part ` is ` DATE ` , returns a ` DATE ` object.\n* If ` part ` is ` TIME ` , returns a ` TIME ` object.\n\n**Examples**\n\nIn the following example, ` EXTRACT ` returns a value corresponding to the `\nHOUR ` time part.\n\n\nSELECT EXTRACT(HOUR FROM DATETIME(2008, 12, 25, 15, 30, 00)) as hour;\n\n/*------------------*\n| hour             |\n+------------------+\n| 15               |\n*------------------*/\n\nIn the following example, ` EXTRACT ` returns values corresponding to different time parts from a column of datetimes.\n\n\nWITH Datetimes AS ( SELECT DATETIME '2005-01-03 12:34:56' AS datetime UNION ALL SELECT DATETIME '2007-12-31' UNION ALL SELECT DATETIME '2009-01-01' UNION ALL SELECT DATETIME '2009-12-31' UNION ALL SELECT DATETIME '2017-01-02' UNION ALL SELECT DATETIME '2017-05-26'\n) SELECT datetime,\nEXTRACT(ISOYEAR FROM datetime) AS isoyear,\nEXTRACT(ISOWEEK FROM datetime) AS isoweek,\nEXTRACT(YEAR FROM datetime) AS year,\nEXTRACT(WEEK FROM datetime) AS week FROM Datetimes ORDER BY datetime;\n\n/*---------------------+---------+---------+------+------*\n| datetime            | isoyear | isoweek | year | week |\n+---------------------+---------+---------+------+------+\n| 2005-01-03T12:34:56 | 2005    | 1       | 2005 | 1    |\n| 2007-12-31T00:00:00 | 2008    | 1       | 2007 | 52   |\n| 2009-01-01T00:00:00 | 2009    | 1       | 2009 | 0    |\n| 2009-12-31T00:00:00 | 2009    | 53      | 2009 | 52   |\n| 2017-01-02T00:00:00 | 2017    | 1       | 2017 | 1    |\n| 2017-05-26T00:00:00 | 2017    | 21      | 2017 | 21   |\n*---------------------+---------+---------+------+------*/\n\nIn the following example, ` datetime_expression ` falls on a Sunday. ` EXTRACT\n` calculates the first column using weeks that begin on Sunday, and it calculates the second column using weeks that begin on Monday.\n\n\nWITH table AS (SELECT DATETIME(TIMESTAMP \"2017-11-05 00:00:00+00\", \"UTC\") AS datetime) SELECT datetime,\nEXTRACT(WEEK(SUNDAY) FROM datetime) AS week_sunday,\nEXTRACT(WEEK(MONDAY) FROM datetime) AS week_monday FROM table;\n\n/*---------------------+-------------+---------------*\n| datetime            | week_sunday | week_monday   |\n+---------------------+-------------+---------------+\n| 2017-11-05T00:00:00 | 45          | 44            |\n*---------------------+-------------+---------------*/"
            },
            "FORMAT_DATETIME": {
                "name": "FORMAT_DATETIME",
                "summary": "Formats a ` DATETIME ` value according to a specified format string.",
                "description": "FORMAT_DATETIME(format_string, datetime_expression)\n\n**Description**\n\nFormats a ` DATETIME ` object according to the specified ` format_string ` .\nSee [ Supported Format Elements For DATETIME\n](/bigquery/docs/reference/standard-sql/format-\nelements#format_elements_date_time) for a list of format elements that this function supports.\n\n**Return Data Type**\n\n` STRING `\n\n**Examples**\n\n\nSELECT FORMAT_DATETIME(\"%c\", DATETIME \"2008-12-25 15:30:00\") AS formatted;\n\n/*--------------------------*\n| formatted                |\n+--------------------------+\n| Thu Dec 25 15:30:00 2008 |\n*--------------------------*/\n\n\nSELECT FORMAT_DATETIME(\"%b-%d-%Y\", DATETIME \"2008-12-25 15:30:00\") AS formatted;\n\n/*-------------*\n| formatted   |\n+-------------+\n| Dec-25-2008 |\n*-------------*/\n\n\nSELECT FORMAT_DATETIME(\"%b %Y\", DATETIME \"2008-12-25 15:30:00\") AS formatted;\n\n/*-------------*\n| formatted   |\n+-------------+\n| Dec 2008    |\n*-------------*/"
            },
            "LAST_DAY": {
                "name": "LAST_DAY",
                "summary": "Gets the last day in a specified time period that contains a `\nDATETIME ` value.",
                "description": "LAST_DAY(datetime_expression[, date_part])\n\n**Description**\n\nReturns the last day from a datetime expression that contains the date. This is commonly used to return the last day of the month.\n\nYou can optionally specify the date part for which the last day is returned.\nIf this parameter is not used, the default value is ` MONTH ` . ` LAST_DAY `\nsupports the following values for ` date_part ` :\n\n* ` YEAR `\n* ` QUARTER `\n* ` MONTH `\n* ` WEEK ` . Equivalent to 7 ` DAY ` s.\n* ` WEEK(<WEEKDAY>) ` . ` <WEEKDAY> ` represents the starting day of the week. Valid values are ` SUNDAY ` , ` MONDAY ` , ` TUESDAY ` , ` WEDNESDAY ` , ` THURSDAY ` , ` FRIDAY ` , and ` SATURDAY ` .\n* ` ISOWEEK ` . Uses [ ISO 8601 ](https://en.wikipedia.org/wiki/ISO_week_date) week boundaries. ISO weeks begin on Monday.\n* ` ISOYEAR ` . Uses the [ ISO 8601 ](https://en.wikipedia.org/wiki/ISO_8601) week-numbering year boundary. The ISO year boundary is the Monday of the first week whose Thursday belongs to the corresponding Gregorian calendar year.\n\n**Return Data Type**\n\n` DATE `\n\n**Example**\n\nThese both return the last day of the month:\n\n\nSELECT LAST_DAY(DATETIME '2008-11-25', MONTH) AS last_day\n\n/*------------*\n| last_day   |\n+------------+\n| 2008-11-30 |\n*------------*/\n\n\nSELECT LAST_DAY(DATETIME '2008-11-25') AS last_day\n\n/*------------*\n| last_day   |\n+------------+\n| 2008-11-30 |\n*------------*/\n\nThis returns the last day of the year:\n\n\nSELECT LAST_DAY(DATETIME '2008-11-25 15:30:00', YEAR) AS last_day\n\n/*------------*\n| last_day   |\n+------------+\n| 2008-12-31 |\n*------------*/\n\nThis returns the last day of the week for a week that starts on a Sunday:\n\n\nSELECT LAST_DAY(DATETIME '2008-11-10 15:30:00', WEEK(SUNDAY)) AS last_day\n\n/*------------*\n| last_day   |\n+------------+\n| 2008-11-15 |\n*------------*/\n\nThis returns the last day of the week for a week that starts on a Monday:\n\n\nSELECT LAST_DAY(DATETIME '2008-11-10 15:30:00', WEEK(MONDAY)) AS last_day\n\n/*------------*\n| last_day   |\n+------------+\n| 2008-11-16 |\n*------------*/"
            },
            "PARSE_DATETIME": {
                "name": "PARSE_DATETIME",
                "summary": "Converts a ` STRING ` value to a ` DATETIME ` value.",
                "description": "PARSE_DATETIME(format_string, datetime_string)\n\n**Description**\n\nConverts a  string representation of a datetime  to a ` DATETIME ` object.\n\n` format_string ` contains the [ format elements\n](/bigquery/docs/reference/standard-sql/format-\nelements#format_elements_date_time) that define how ` datetime_string ` is formatted. Each element in ` datetime_string ` must have a corresponding element in ` format_string ` . The location of each element in ` format_string\n` must match the location of each element in ` datetime_string ` .\n\n\n-- This works because elements on both sides match.\nSELECT PARSE_DATETIME(\"%a %b %e %I:%M:%S %Y\", \"Thu Dec 25 07:30:00 2008\");\n\n-- This produces an error because the year element is in different locations.\nSELECT PARSE_DATETIME(\"%a %b %e %Y %I:%M:%S\", \"Thu Dec 25 07:30:00 2008\");\n\n-- This produces an error because one of the year elements is missing.\nSELECT PARSE_DATETIME(\"%a %b %e %I:%M:%S\", \"Thu Dec 25 07:30:00 2008\");\n\n-- This works because %c can find all matching elements in datetime_string.\nSELECT PARSE_DATETIME(\"%c\", \"Thu Dec 25 07:30:00 2008\");\n\nThe format string fully supports most format elements, except for ` %P ` .\n\n` PARSE_DATETIME ` parses ` string ` according to the following rules:\n\n* **Unspecified fields.** Any unspecified field is initialized from ` 1970-01-01 00:00:00.0 ` . For example, if the year is unspecified then it defaults to ` 1970 ` .\n* **Case insensitivity.** Names, such as ` Monday ` and ` February ` , are case insensitive.\n* **Whitespace.** One or more consecutive white spaces in the format string matches zero or more consecutive white spaces in the ` DATETIME ` string. Leading and trailing white spaces in the ` DATETIME ` string are always allowed, even if they are not in the format string.\n* **Format precedence.** When two or more format elements have overlapping information, the last one generally overrides any earlier ones, with some exceptions. For example, both ` %F ` and ` %Y ` affect the year, so the earlier element overrides the later. See the descriptions of ` %s ` , ` %C ` , and ` %y ` in [ Supported Format Elements For DATETIME ](/bigquery/docs/reference/standard-sql/format-elements#format_elements_date_time) .\n* **Format divergence.** ` %p ` can be used with ` am ` , ` AM ` , ` pm ` , and ` PM ` .\n\n**Return Data Type**\n\n` DATETIME `\n\n**Examples**\n\nThe following examples parse a ` STRING ` literal as a ` DATETIME ` .\n\n\nSELECT PARSE_DATETIME('%Y-%m-%d %H:%M:%S', '1998-10-18 13:45:55') AS datetime;\n\n/*---------------------*\n| datetime            |\n+---------------------+\n| 1998-10-18T13:45:55 |\n*---------------------*/\n\n\nSELECT PARSE_DATETIME('%m/%d/%Y %I:%M:%S %p', '8/30/2018 2:23:38 pm') AS datetime;\n\n/*---------------------*\n| datetime            |\n+---------------------+\n| 2018-08-30T14:23:38 |\n*---------------------*/\n\nThe following example parses a ` STRING ` literal containing a date in a natural language format as a ` DATETIME ` .\n\n\nSELECT PARSE_DATETIME('%A, %B %e, %Y','Wednesday, December 19, 2018') AS datetime;\n\n/*---------------------*\n| datetime            |\n+---------------------+\n| 2018-12-19T00:00:00 |\n*---------------------*/"
            }
        }
    },
    {
        "category": "debugging-functions",
        "description": "GoogleSQL for BigQuery supports the following debugging functions.",
        "source": "debugging_functions.txt",
        "functions": {
            "ERROR": {
                "name": "ERROR",
                "summary": "Produces an error with a custom error message.",
                "description": "ERROR(error_message)\n\n**Description**\n\nReturns an error.\n\n**Definitions**\n\n* ` error_message ` : A ` STRING ` value that represents the error message to produce. Any whitespace characters beyond a single space are trimmed from the results.\n\n**Details**\n\n` ERROR ` is treated like any other expression that may result in an error:\nthere is no special guarantee of evaluation order.\n\n**Return Data Type**\n\nGoogleSQL infers the return type in context.\n\n**Examples**\n\nIn the following example, the query returns an error message if the value of the row does not match one of two defined values.\n\n\nSELECT CASE WHEN value = 'foo' THEN 'Value is foo.'\nWHEN value = 'bar' THEN 'Value is bar.'\nELSE ERROR(CONCAT('Found unexpected value: ', value)) END AS new_value FROM ( SELECT 'foo' AS value UNION ALL SELECT 'bar' AS value UNION ALL SELECT 'baz' AS value);\n\n-- Found unexpected value: baz\n\nIn the following example, GoogleSQL may evaluate the ` ERROR ` function before or after the  ` x > 0 ` condition, because GoogleSQL generally provides no ordering guarantees between ` WHERE ` clause conditions and there are no special guarantees for the ` ERROR ` function.\n\n\nSELECT *\nFROM (SELECT -1 AS x) WHERE x > 0 AND ERROR('Example error');\n\nIn the next example, the ` WHERE ` clause evaluates an ` IF ` condition, which ensures that GoogleSQL only evaluates the ` ERROR ` function if the condition fails.\n\n\nSELECT *\nFROM (SELECT -1 AS x) WHERE IF(x > 0, true, ERROR(FORMAT('Error: x must be positive but is %t', x)));\n\n-- Error: x must be positive but is -1"
            }
        }
    },
    {
        "category": "differentially-private-aggregate-functions",
        "description": "GoogleSQL for BigQuery supports differentially private aggregate functions.\nFor an explanation of how aggregate functions work, see [ Aggregate function calls ](/bigquery/docs/reference/standard-sql/aggregate-function-calls) .\n\nYou can only use differentially private aggregate functions with [\ndifferentially private queries ](/bigquery/docs/differential-privacy) in a [\ndifferential privacy clause ](/bigquery/docs/reference/standard-sql/query-\nsyntax#dp_clause) .\n\n**Note:** In this topic, the privacy parameters in the examples are not recommendations. You should work with your privacy or security officer to determine the optimal privacy parameters for your dataset and organization.",
        "source": "aggregate-dp-functions.txt",
        "functions": {
            "AVG": {
                "name": "AVG",
                "summary": "` DIFFERENTIAL_PRIVACY ` -supported ` AVG ` .\n\nGets the differentially-private average of non- ` NULL ` , non- ` NaN ` values in a query with a ` DIFFERENTIAL_PRIVACY ` clause.",
                "description": "WITH DIFFERENTIAL_PRIVACY ...\nAVG( expression,\n[contribution_bounds_per_group => (lower_bound, upper_bound)]\n)\n\n**Description**\n\nReturns the average of non- ` NULL ` , non- ` NaN ` values in the expression.\nThis function first computes the average per privacy unit column, and then computes the final result by averaging these averages.\n\nThis function must be used with the [ ` DIFFERENTIAL_PRIVACY ` clause\n](/bigquery/docs/reference/standard-sql/query-syntax#dp_clause) and can support the following arguments:\n\n* ` expression ` : The input expression. This can be any numeric input type, such as ` INT64 ` .\n* ` contribution_bounds_per_group ` : The  contribution bounds named argument  . Perform clamping per each group separately before performing intermediate grouping on the privacy unit column.\n\n**Return type**\n\n` FLOAT64 `\n\n**Examples**\n\nThe following differentially private query gets the average number of each item requested per professor. Smaller aggregations might not be included. This query references a table called [ ` professors `\n](/bigquery/docs/reference/standard-sql/query-syntax#dp_example_tables) .\n\n\n-- With noise, using the epsilon parameter.\nSELECT WITH DIFFERENTIAL_PRIVACY OPTIONS(epsilon=10, delta=.01, max_groups_contributed=1, privacy_unit_column=id) item,\nAVG(quantity, contribution_bounds_per_group => (0,100)) average_quantity FROM professors GROUP BY item;\n\n-- These results will change each time you run the query.\n-- Smaller aggregations might be removed.\n/*----------+------------------*\n| item     | average_quantity |\n+----------+------------------+\n| pencil   | 38.5038356810269 |\n| pen      | 13.4725028762032 |\n*----------+------------------*/\n\n\n-- Without noise, using the epsilon parameter.\n-- (this un-noised version is for demonstration only) SELECT WITH DIFFERENTIAL_PRIVACY OPTIONS(epsilon=1e20, delta=.01, max_groups_contributed=1, privacy_unit_column=id) item,\nAVG(quantity) average_quantity FROM professors GROUP BY item;\n\n-- These results will not change when you run the query.\n/*----------+------------------*\n| item     | average_quantity |\n+----------+------------------+\n| scissors | 8                |\n| pencil   | 40               |\n| pen      | 18.5             |\n*----------+------------------*/\n\n**Note:** For more information about when and when not to use noise, see [\nRemove noise ](/bigquery/docs/reference/standard-sql/query-\nsyntax#eliminate_noise) ."
            },
            "COUNT": {
                "name": "COUNT",
                "summary": "` DIFFERENTIAL_PRIVACY ` -supported ` COUNT ` .\n\nSignature 1: Gets the differentially-private count of rows in a query with a `\nDIFFERENTIAL_PRIVACY ` clause.\n\nSignature 2: Gets the differentially-private count of rows with a non- ` NULL\n` expression in a query with a ` DIFFERENTIAL_PRIVACY ` clause.",
                "description": "* Signature 1  : Returns the number of rows in a differentially private ` FROM ` clause.\n* Signature 2  : Returns the number of non- ` NULL ` values in an expression.\n\n####  Signature 1\n\n\nWITH DIFFERENTIAL_PRIVACY ...\nCOUNT(\n*,\n[contribution_bounds_per_group => (lower_bound, upper_bound)]\n)\n\n**Description**\n\nReturns the number of rows in the [ differentially private\n](/bigquery/docs/differential-privacy#dp_from) ` FROM ` clause. The final result is an aggregation across a privacy unit column.\n\nThis function must be used with the [ ` DIFFERENTIAL_PRIVACY ` clause\n](/bigquery/docs/reference/standard-sql/query-syntax#dp_clause) and can support the following argument:\n\n* ` contribution_bounds_per_group ` : The  contribution bounds named argument  . Perform clamping per each group separately before performing intermediate grouping on the privacy unit column.\n\n**Return type**\n\n` INT64 `\n\n**Examples**\n\nThe following differentially private query counts the number of requests for each item. This query references a table called [ ` professors `\n](/bigquery/docs/reference/standard-sql/query-syntax#dp_example_tables) .\n\n\n-- With noise, using the epsilon parameter.\nSELECT WITH DIFFERENTIAL_PRIVACY OPTIONS(epsilon=10, delta=.01, max_groups_contributed=1, privacy_unit_column=id) item,\nCOUNT(*, contribution_bounds_per_group=>(0, 100)) times_requested FROM professors GROUP BY item;\n\n-- These results will change each time you run the query.\n-- Smaller aggregations might be removed.\n/*----------+-----------------*\n| item     | times_requested |\n+----------+-----------------+\n| pencil   | 5               |\n| pen      | 2               |\n*----------+-----------------*/\n\n\n-- Without noise, using the epsilon parameter.\n-- (this un-noised version is for demonstration only) SELECT WITH DIFFERENTIAL_PRIVACY OPTIONS(epsilon=1e20, delta=.01, max_groups_contributed=1, privacy_unit_column=id) item,\nCOUNT(*, contribution_bounds_per_group=>(0, 100)) times_requested FROM professors GROUP BY item;\n\n-- These results will not change when you run the query.\n/*----------+-----------------*\n| item     | times_requested |\n+----------+-----------------+\n| scissors | 1               |\n| pencil   | 4               |\n| pen      | 3               |\n*----------+-----------------*/\n\n**Note:** For more information about when and when not to use noise, see [\nRemove noise ](/bigquery/docs/reference/standard-sql/query-\nsyntax#eliminate_noise) .\n\n####  Signature 2\n\n\nWITH DIFFERENTIAL_PRIVACY ...\nCOUNT( expression,\n[contribution_bounds_per_group => (lower_bound, upper_bound)]\n)\n\n**Description**\n\nReturns the number of non- ` NULL ` expression values. The final result is an aggregation across a privacy unit column.\n\nThis function must be used with the [ ` DIFFERENTIAL_PRIVACY ` clause\n](/bigquery/docs/reference/standard-sql/query-syntax#dp_clause) and can support these arguments:\n\n* ` expression ` : The input expression. This expression can be any numeric input type, such as ` INT64 ` .\n* ` contribution_bounds_per_group ` : The  contribution bounds named argument  . Perform clamping per each group separately before performing intermediate grouping on the privacy unit column.\n\n**Return type**\n\n` INT64 `\n\n**Examples**\n\nThe following differentially private query counts the number of requests made for each type of item. This query references a table called [ ` professors `\n](/bigquery/docs/reference/standard-sql/query-syntax#dp_example_tables) .\n\n\n-- With noise, using the epsilon parameter.\nSELECT WITH DIFFERENTIAL_PRIVACY OPTIONS(epsilon=10, delta=.01, max_groups_contributed=1, privacy_unit_column=id) item,\nCOUNT(item, contribution_bounds_per_group => (0,100)) times_requested FROM professors GROUP BY item;\n\n-- These results will change each time you run the query.\n-- Smaller aggregations might be removed.\n/*----------+-----------------*\n| item     | times_requested |\n+----------+-----------------+\n| pencil   | 5               |\n| pen      | 2               |\n*----------+-----------------*/\n\n\n-- Without noise, using the epsilon parameter.\n-- (this un-noised version is for demonstration only) SELECT WITH DIFFERENTIAL_PRIVACY OPTIONS(epsilon=1e20, delta=.01, max_groups_contributed=1, privacy_unit_column=id) item,\nCOUNT(item, contribution_bounds_per_group => (0,100)) times_requested FROM professors GROUP BY item;\n\n-- These results will not change when you run the query.\n/*----------+-----------------*\n| item     | times_requested |\n+----------+-----------------+\n| scissors | 1               |\n| pencil   | 4               |\n| pen      | 3               |\n*----------+-----------------*/\n\n**Note:** For more information about when and when not to use noise, see [\nRemove noise ](/bigquery/docs/reference/standard-sql/query-\nsyntax#eliminate_noise) ."
            },
            "PERCENTILE_CONT": {
                "name": "PERCENTILE_CONT",
                "summary": "` DIFFERENTIAL_PRIVACY ` -supported ` PERCENTILE_CONT `\n.\n\nComputes a differentially-private percentile across privacy unit columns in a query with a ` DIFFERENTIAL_PRIVACY ` clause.",
                "description": "WITH DIFFERENTIAL_PRIVACY ...\nPERCENTILE_CONT( expression,\npercentile,\ncontribution_bounds_per_row => (lower_bound, upper_bound) )\n\n**Description**\n\nTakes an expression and computes a percentile for it. The final result is an aggregation across privacy unit columns.\n\nThis function must be used with the [ ` DIFFERENTIAL_PRIVACY ` clause\n](/bigquery/docs/reference/standard-sql/query-syntax#dp_clause) and can support these arguments:\n\n* ` expression ` : The input expression. This can be most numeric input types, such as ` INT64 ` . ` NULL ` values are always ignored.\n* ` percentile ` : The percentile to compute. The percentile must be a literal in the range ` [0, 1] ` .\n* ` contribution_bounds_per_row ` : The  contribution bounds named argument  . Perform clamping per each row separately before performing intermediate grouping on the privacy unit column.\n\n` NUMERIC ` and ` BIGNUMERIC ` arguments are not allowed. If you need them,\ncast them as the ` FLOAT64 ` data type first.\n\n**Return type**\n\n` FLOAT64 `\n\n**Examples**\n\nThe following differentially private query gets the percentile of items requested. Smaller aggregations might not be included. This query references a view called [ ` professors ` ](/bigquery/docs/reference/standard-sql/query-\nsyntax#dp_example_tables) .\n\n\n-- With noise, using the epsilon parameter.\nSELECT WITH DIFFERENTIAL_PRIVACY OPTIONS(epsilon=10, delta=.01, max_groups_contributed=1, privacy_unit_column=id) item,\nPERCENTILE_CONT(quantity, 0.5, contribution_bounds_per_row => (0,100)) percentile_requested FROM professors GROUP BY item;\n\n-- These results will change each time you run the query.\n-- Smaller aggregations might be removed.\n/*----------+----------------------*\n| item     | percentile_requested |\n+----------+----------------------+\n| pencil   | 72.00011444091797    |\n| scissors | 8.000175476074219    |\n| pen      | 23.001075744628906   |\n*----------+----------------------*/"
            },
            "SUM": {
                "name": "SUM",
                "summary": "` DIFFERENTIAL_PRIVACY ` -supported ` SUM ` .\n\nGets the differentially-private sum of non- ` NULL ` , non- ` NaN ` values in a query with a ` DIFFERENTIAL_PRIVACY ` clause.",
                "description": "WITH DIFFERENTIAL_PRIVACY ...\nSUM( expression,\n[contribution_bounds_per_group => (lower_bound, upper_bound)]\n)\n\n**Description**\n\nReturns the sum of non- ` NULL ` , non- ` NaN ` values in the expression. The final result is an aggregation across privacy unit columns.\n\nThis function must be used with the [ ` DIFFERENTIAL_PRIVACY ` clause\n](/bigquery/docs/reference/standard-sql/query-syntax#dp_clause) and can support these arguments:\n\n* ` expression ` : The input expression. This can be any numeric input type, such as ` INT64 ` . ` NULL ` values are always ignored.\n* ` contribution_bounds_per_group ` : The  contribution bounds named argument  . Perform clamping per each group separately before performing intermediate grouping on the privacy unit column.\n\n**Return type**\n\nOne of the following [ supertypes ](/bigquery/docs/reference/standard-\nsql/conversion_rules#supertypes) :\n\n* ` INT64 `\n* ` FLOAT64 `\n\n**Examples**\n\nThe following differentially private query gets the sum of items requested.\nSmaller aggregations might not be included. This query references a view called [ ` professors ` ](/bigquery/docs/reference/standard-sql/query-\nsyntax#dp_example_tables) .\n\n\n-- With noise, using the epsilon parameter.\nSELECT WITH DIFFERENTIAL_PRIVACY OPTIONS(epsilon=10, delta=.01, max_groups_contributed=1, privacy_unit_column=id) item,\nSUM(quantity, contribution_bounds_per_group => (0,100)) quantity FROM professors GROUP BY item;\n\n-- These results will change each time you run the query.\n-- Smaller aggregations might be removed.\n/*----------+-----------*\n| item     | quantity  |\n+----------+-----------+\n| pencil   | 143       |\n| pen      | 59        |\n*----------+-----------*/\n\n\n-- Without noise, using the epsilon parameter.\n-- (this un-noised version is for demonstration only) SELECT WITH DIFFERENTIAL_PRIVACY OPTIONS(epsilon=1e20, delta=.01, max_groups_contributed=1, privacy_unit_column=id) item,\nSUM(quantity) quantity FROM professors GROUP BY item;\n\n-- These results will not change when you run the query.\n/*----------+----------*\n| item     | quantity |\n+----------+----------+\n| scissors | 8        |\n| pencil   | 144      |\n| pen      | 58       |\n*----------+----------*/\n\n**Note:** For more information about when and when not to use noise, see [ Use differential privacy ](/bigquery/docs/reference/standard-sql/query-\nsyntax#eliminate_noise) ."
            }
        }
    },
    {
        "category": "federated-query-functions",
        "description": "GoogleSQL for BigQuery supports the following federated query functions.",
        "source": "federated_query_functions.txt",
        "functions": {
            "EXTERNAL_QUERY": {
                "name": "EXTERNAL_QUERY",
                "summary": "Executes a query on an external database and returns the results as a temporary table.",
                "description": "EXTERNAL_QUERY('connection_id', '''external_database_query'''[, 'options'])\n\n**Description**\n\nExecutes a query on an external database and returns the results as a temporary table. The external database data type is converted to a [ GoogleSQL data type ](/bigquery/docs/reference/standard-sql/data-types#data-types) in the temporary result table with  these data type mappings  .\n\n* ` external_database_query ` : The query to run on the external database.\n* ` connection_id ` : The ID of the [ connection resource ](/bigquery/docs/cloud-sql-federated-queries#setting_up_database_connections) . The connection resource contains settings for the connection between the external database and BigQuery. If you do not have a default project configured, prepend the project ID to the connection ID in following format:\n\nprojects/PROJECT_ID/locations/LOCATION/connections/CONNECTION_ID\n\nReplace the following:\n\n* PROJECT_ID  : The project ID.\n* LOCATION  : The location of the connection.\n* CONNECTION_ID  : The connection ID.\n\nFor example, ` projects/example-project/locations/us/connections/sql-bq ` .\nFor more information, see [ Create a connection resource\n](/bigquery/docs/working-with-connections#create_a_connection_resource) .\n\n**Caution:** If you have a view that is shared across multiple projects where you use ` EXTERNAL_QUERY ` , always use the fully qualified connection ID (projects/  PROJECT_ID  /locations/  LOCATION  /connections/  CONNECTION_ID ), otherwise the wrong project might be used.\n\n\\+ ` options ` : An optional string of a JSON format map with key value pairs of option name and value (both are case sensitive).\n\n\nFor example::\n``` '{\"default_type_for_decimal_columns\":\"numeric\"}' ```\n\nSupported options:\n\n|Option Name | Description\n|-------- | -------\n|\"default_type_for_decimal_columns\" | Can be \"float64\", \"numeric\", \"bignumeric\" or \"string\". With this option, the MySQL Decimal type or PostgreSQL Numeric type will be mapped to the provided BigQuery type. When this option is not provided, the MySQL Decimal type or PostgreSQL Numeric type will be mapped to BigQuery NUMERIC type.\n|\"query_execution_priority\" | Can be \"low\", \"medium\" or \"high\". Only supported in Spanner. Specifies priority for execution of the query. Execution priority is \"medium\" by default.\n\nAdditional notes:\n\n* The ` EXTERNAL_QUERY ` function is usually used in a ` FROM ` clause.\n* You can use the ` EXTERNAL_QUERY() ` function to access metadata about the external database.\n* ` EXTERNAL_QUERY() ` won't honor the ordering of the external query result, even if your external query includes ` ORDER BY ` .\n\n**Return Data Type**\n\nBigQuery table\n\n**Examples**\n\nSuppose you need the date of the first order for each of your customers to include in a report. This data is not currently in BigQuery but is available in your operational PostgreSQL database in . The following federated query example accomplishes this and includes 3 parts:\n\n1. Run the external query ` SELECT customer_id, MIN(order_date) AS first_order_date FROM orders GROUP BY customer_id ` in the operational PostgreSQL database to get the first order date for each customer through the ` EXTERNAL_QUERY() ` function.\n2. Join external query result table with customers table in BigQuery by ` customer_id ` .\n3. Select customer information and first order date.\n\n\nSELECT c.customer_id, c.name, SUM(t.amount) AS total_revenue, rq.first_order_date FROM customers AS c INNER JOIN transaction_fact AS t ON c.customer_id = t.customer_id LEFT OUTER JOIN EXTERNAL_QUERY(\n'connection_id',\n'''SELECT customer_id, MIN(order_date) AS first_order_date FROM orders GROUP BY customer_id'''\n) AS rq ON rq.customer_id = c.customer_id GROUP BY c.customer_id, c.name, rq.first_order_date;\n\nYou can use the ` EXTERNAL_QUERY() ` function to query information_schema tables to access database metadata, such as list all tables in the database or show table schema. The following example information_schema queries work in both [ MySQL ](https://dev.mysql.com/doc/refman/8.0/en/information-schema-\nintroduction.html) and [ PostgreSQL\n](https://www.postgresql.org/docs/9.1/information-schema.html) .\n\n\n-- List all tables in a database.\nSELECT *\nFROM EXTERNAL_QUERY(\n'connection_id',\n'''SELECT * FROM information_schema.tables'''\n);\n\n\n-- List all columns in a table.\nSELECT *\nFROM EXTERNAL_QUERY(\n'connection_id',\n'''SELECT * FROM information_schema.columns WHERE table_name='x';'''\n);\n\n` EXTERNAL_QUERY() ` won't honor the ordering of the external query result,\neven if your external query includes ` ORDER BY ` . The following example query orders rows by customer ID in the external database, but BigQuery will not output the result rows in that order.\n\n\n-- ORDER BY will not order rows.\nSELECT *\nFROM EXTERNAL_QUERY(\n'connection_id',\n'''SELECT * FROM customers AS c ORDER BY c.customer_id'''\n);\n\n####  Data type mappings\n\nWhen you execute a federated query, the data from the external database are converted to GoogleSQL types. Below are the data type mappings from  MySQL to BigQuery  and  PostgreSQL to BigQuery  .\n\nThings to know about mapping:\n\n* Most MySQL data types can be matched to the same BigQuery data type, with a few exceptions such as ` decimal ` , ` timestamp ` , and ` time ` .\n* PostgreSQL supports many non-standard data types which are not supported in BigQuery, for example ` money ` , ` path ` , ` uuid ` , ` boxer ` , and others.\n* The numeric data types in MySQL and PostgreSQL will be mapped to BigQuery ` NUMERIC ` value by default. The BigQuery ` NUMERIC ` value range is smaller than in MySQL and PostgreSQL. It can also be mapped to [ ` BIGNUMERIC ` ](/bigquery/docs/reference/standard-sql/data-types#numeric_types) , ` FLOAT64 ` , or ` STRING ` with  \"default_type_for_decimal_columns\"  in ` EXTERNAL_QUERY ` options.\n\n**Error handling**\n\nIf your external query contains a data type that is unsupported in BigQuery,\nthe query will fail immediately. You can cast the unsupported data type to a different MySQL / PostgreSQL data type that is supported. See  unsupported data types  for more information on how to cast.\n\n####  MySQL to BigQuery type mapping\n\n**MySQL type** |  **MySQL Description** |  **BigQuery type** |  **Type difference**\n---|---|---|---\n**Integer** |  |  |\nINT  |  4 bytes, 2^32 - 1  |  INT64  |\nTINYINT  |  1 byte, 2^8 - 1  |  INT64  |\nSMALLINT  |  2 bytes, 2^16 - 1  |  INT64  |\nMEDIUMINT  |  3 bytes, 2^24 - 1  |  INT64  |\nBIGINT  |  8 bytes, 2^64 - 1  |  INT64  |\nUNSIGNED BIGINT  |  8 bytes, 2^64 - 1  |  NUMERIC  |\n**Exact numeric** |  |  |\nDECIMAL (M,D)  |  A decimal represents by (M,D) where M is the total number of digits and D is the number of decimals. M <= 65  |  NUMERIC, BIGNUMERIC,\nFLOAT64, or STRING\n\n|  DECIMAL (M,D) will to mapped to NUMERIC by default, or can be mapped to BIGNUMERIC, FLOAT64, or STRING with  default_type_for_decimal_columns  .\n**Approximate numeric** |  |  |\nFLOAT (M,D)  |  4 bytes, M <= 23  |  FLOAT64  |\nDOUBLE (M,D)  |  8 bytes, M <= 53  |  FLOAT64  |\n**Date and time** |  |  |\nTIMESTAMP  |  '1970-01-01 00:00:01'UTC to '2038-01-19 03:14:07' UTC.  |\nTIMESTAMP  |  MySQL TIMESTAMP is retrieved as UTC timezone no matter where user call BigQuery DATETIME  |  '1000-01-01 00:00:00' to '9999-12-31 23:59:59'  |  DATETIME  |\nDATE  |  '1000-01-01' to '9999-12-31'.  |  DATE  |\nTIME  |  Time in 'HH:MM:SS' format\n'-838:59:59' to '838:59:59'.  |  TIME\n|  BigQuery TIME range is smaller, from 00:00:00 to 23:59:59 YEAR  |  |  INT64  |\n**Character and strings** |  |  |\nENUM  |  string object with a value chosen from a list of permitted values  |\nSTRING  |\nCHAR (M)  |  A fixed-length string between 1 and 255 characters  |  STRING  |\nVARCHAR (M)  |  A variable-length string between 1 and 255 characters in length.  |  STRING  |\nTEXT  |  A field with a maximum length of 65535 characters.  |  STRING  |\nTINYTEXT  |  TEXT column with a maximum length of 255 characters.  |  STRING\n|\nMEDIUMTEXT  |  TEXT column with a maximum length of 16777215 characters.  |\nSTRING  |\nLONGTEXT  |  TEXT column with a maximum length of 4294967295 characters.  |\nSTRING  |\n**Binary** |  |  |\nBLOB  |  A binary large object with a maximum length of 65535 characters.  |\nBYTES  |\nMEDIUM_BLOB  |  A BLOB with a maximum length of 16777215 characters.  |  BYTES\n|\nLONG_BLOB  |  A BLOB with a maximum length of 4294967295 characters.  |  BYTES\n|\nTINY_BLOB  |  A BLOB with a maximum length of 255 characters.  |  BYTES  |\nBINARY  |  A fixed-length binary string between 1 and 255 characters.  |\nBYTES  |\nVARBINARY  |  A variable-length binary string between 1 and 255 characters.  |\nBYTES  |\n**Other** |  |  |\nSET  |  when declare SET column, predefine some values. Then INSERT any set of predefined values into this column  |  STRING  |\nGEOMETRY  |  |  GEOGRAPHY  |  NOT YET SUPPORTED BIT  |  |  INT64  |  NOT YET SUPPORTED\n\n####  PostgreSQL to BigQuery type mapping\n\n**Name** |  **Description** |  **BigQuery type** |  **Type difference**\n---|---|---|---\n**Integer** |  |  |\nsmallint  |  2 bytes, -32768 to +32767  |  INT64  |\nsmallserial  |  See smallint  |  INT64  |\ninteger  |  4 bytes, -2147483648 to +2147483647  |  INT64  |\nserial  |  See integer  |  INT64  |\nbigint  |  8 bytes, -9223372036854775808 to 9223372036854775807  |  INT64  |\nbigserial  |  See bigint  |  INT64  |\n**Exact numeric** |  |  |\nnumeric [ (p, s) ]  |  Precision up to 1,000.  |  NUMERIC, BIGNUMERIC,\nFLOAT64, or STRING  |  numeric [ (p, s) ] will to mapped to NUMERIC by default, or can be mapped to BIGNUMERIC, FLOAT64, or STRING with default_type_for_decimal_columns  .\nDecimal [ (p, s) ]  |  See numeric  |  NUMERIC  |  See numeric money  |  8 bytes, 2 digit scale, -92233720368547758.08 to\n+92233720368547758.07  |  NOT SUPPORTED  |\n**Approximate numeric** |  |  |\nreal  |  4 bytes, single precision floating-point number  |  FLOAT64  |\ndouble precision  |  8 bytes, double precision floating-point number  |\nFLOAT64  |\n**Date and time** |  |  |\ndate  |  calendar date (year, month, day)  |  DATE  |\ntime [ (p) ] [ without time zone ]  |  time of day (no time zone)  |  TIME  |\ntime [ (p) ] with time zone  |  time of day, including time zone  |  NOT SUPPORTED  |\ntimestamp [ (p) ] [ without time zone ]  |  date and time (no time zone)  |\nDATETIME  |\ntimestamp [ (p) ] with time zone  |  date and time, including time zone  |\nTIMESTAMP  |  PostgreSQL TIMESTAMP is retrieved as UTC timezone no matter where user call BigQuery interval  |  A time duration  |  NOT SUPPORTED  |\n**Character and strings** |  |  |\ncharacter [ (n) ]  |  fixed-length character string  |  STRING  |\ncharacter varying [ (n) ]  |  variable-length character string  |  STRING  |\ntext  |  variable-length character string  |  STRING  |\n**Binary** |  |  |\nbytea  |  binary data (\"byte array\")  |  BYTES  |\nbit [ (n) ]  |  fixed-length bit string  |  BYTES  |\nbit varying [ (n) ]  |  variable-length bit string  |  BYTES  |\n**Other** |  |  |\nboolean  |  logical Boolean (true/false)  |  BOOL  |\ninet  |  IPv4 or IPv6 host address  |  NOT SUPPORTED  |\npath  |  geometric path on a plane  |  NOT SUPPORTED  |\npg_lsn  |  PostgreSQL Log Sequence Number  |  NOT SUPPORTED  |\npoint  |  geometric point on a plane  |  NOT SUPPORTED  |\npolygon  |  closed geometric path on a plane  |  NOT SUPPORTED  |\ntsquery  |  text search query  |  NOT SUPPORTED  |\ntsvector  |  text search document  |  NOT SUPPORTED  |\ntxid_snapshot  |  user-level transaction ID snapshot  |  NOT SUPPORTED  |\nuuid  |  universally unique identifier  |  NOT SUPPORTED  |\nxml  |  XML data  |  STRING  |\nbox  |  rectangular box on a plane  |  NOT SUPPORTED  |\ncidr  |  IPv4 or IPv6 network address  |  NOT SUPPORTED  |\ncircle  |  circle on a plane  |  NOT SUPPORTED  |\ninterval [ fields ] [ (p) ]  |  time span  |  NOT SUPPORTED  |\njson  |  textual JSON data  |  STRING  |\njsonb  |  binary JSON data, decomposed  |  NOT SUPPORTED  |\nline  |  infinite line on a plane  |  NOT SUPPORTED  |\nlseg  |  line segment on a plane  |  NOT SUPPORTED  |\nmacaddr  |  MAC (Media Access Control) address  |  NOT SUPPORTED  |\nmacaddr8  |  MAC (Media Access Control) address (EUI-64 format)  |  NOT SUPPORTED  |\n\n####  Unsupported MySQL and PostgreSQL data types\n\nIf your external query contains a data type that is unsupported in BigQuery,\nthe query will fail immediately. You can cast the unsupported data type to a different supported MySQL / PostgreSQL data type.\n\n* Unsupported MySQL data type\n* **Error message:** ` Invalid table-valued function external_query Found unsupported MySQL type in BigQuery. at [1:15] `\n* **Unsupported type:** ` GEOMETRY ` , ` BIT `\n* **Resolution:** Cast the unsupported data type to STRING.\n* **Example:** ` SELECT ST_AsText(ST_GeomFromText('POINT(1 1)')); ` This command casts the unsupported data type ` GEOMETRY ` to ` STRING ` .\n* Unsupported PostgreSQL data type\n* **Error message:** ` Invalid table-valued function external_query Postgres type (OID = 790) is not supported now at [1:15] `\n* **Unsupported type:** ` money, time with time zone, inet, path, pg_lsn, point, polygon, tsquery, tsvector, txid_snapshot, uuid, box, cidr, circle, interval, jsonb, line, lseg, macaddr, macaddr8 `\n* **Resolution:** Cast the unsupported data type to STRING.\n* **Example:** ` SELECT CAST('12.34'::float8::numeric::money AS varchar(30)); ` This command casts the unsupported data type ` money ` to ` string ` .\n\n####  Spanner to BigQuery type mapping\n\nWhen you execute a Spanner federated query, the data from Spanner is converted to GoogleSQL types.\n\nSpanner GoogleSQL type  |  Spanner PostgreSQL type  |  BigQuery type\n---|---|---\n` ARRAY ` |  \\-  |  ` ARRAY `\n` BOOL ` |  ` bool ` |  ` BOOL `\n` BYTES ` |  ` bytea ` |  ` BYTES `\n` DATE ` |  ` date ` |  ` DATE `\n` FLOAT64 ` |  ` float8 ` |  ` FLOAT64 `\n` INT64 ` |  ` bigint ` |  ` INT64 `\n` JSON ` |  ` JSONB ` |  ` JSON `\n` NUMERIC ` |  ` numeric ` *  |  ` NUMERIC `\n` STRING ` |  ` varchar ` |  ` STRING `\n` STRUCT ` |  \\-  |  Not supported for Spanner federated queries\n` TIMESTAMP ` |  ` timestamptz ` |  ` TIMESTAMP ` with nanoseconds truncated\n\n* PostgreSQL numeric values with a precision that is greater than the precision that BigQuery supports are rounded. Values that are larger than the maximum value generate an ` Invalid NUMERIC value ` error.\n\nIf your external query contains a data type that is unsupported for federated queries, the query fails immediately. You can cast the unsupported data type to a supported data type.\n\n####  SAP Datasphere to BigQuery type mapping\n\nWhen you execute a [ SAP Datasphere federated query ](/bigquery/docs/sap-\ndatasphere-federated-queries) , the data from SAP Datasphere is converted to the following GoogleSQL types.\n\n**SAP Datasphere type** |  **SAP Datasphere description** |  **BigQuery type**\n---|---|---\n**Integer** |  |\nInteger  |  Standard signed integer.  |  INT64 Integer64  |  Signed 64-bit integer.  |  BIGNUMERIC hana.SMALLINT  |  Signed 16-bit integer supporting the values -32,768 to 32,767.  |  INT64 hana.TINYINT  |  Unsigned 8-bit integer supporting the values 0 to 255.  |\nINT64\n**Exact numeric** |  |\nDecimal (p, s)  |  Precision (p) defines the number of total digits and can be between 1 and 38.\n\nScale (s) defines the number of digits after the decimal point and can be between 0 and p.  |  BIGNUMERIC DecimalFloat  |  Decimal floating-point number with 34 mantissa digits.  |\nBIGNUMERIC hana.SMALLDECIMAL  |  64-bit decimal floating-point number, where (p) can be between 1 and 16 and s can be between -369 and 368.  |  BIGNUMERIC\n**Approximate numeric** |  |\nDouble  |  Double-precision, 64-bit floating-point number.  |  FLOAT64 hana.REAL  |  32-bit binary floating-point number.  |  FLOAT64\n**Date and time** |  |\nDate  |  Default format YYYY-MM-DD.  |  DATE Datetime  |  Default format YYYY-MM-DD HH24:MI:SS.  |  TIMESTAMP Time  |  Default format HH24:MI:SS.  |  TIME Timestamp  |  Default format YYYY-MM-DD HH24:MI:SS.  |  TIMESTAMP\n**Character and strings** |  |\nLargeString  |  Variable length string of up to 2GB.  |  STRING String (n)  |  Variable-length Unicode string of up to 5000 characters.  |\nSTRING\n**Binary** |  |\nBinary (n)  |  Variable length byte string of up to 4000 bytes.  |  BYTES LargeBinary  |  Variable length byte string of up to 2GB.  |  BYTES hana.BINARY (n)  |  Byte string of fixed length (n).  |  STRING\n**Other** |  |\nBoolean  |  TRUE, FALSE and UNKNOWN, where UNKNOWN is a synonym of NULL.  |\nBOOL UUID  |  Universally unique identifier encoded as a 128-bit integer.  |\nSTRING hana.ST_GEOMETRY  |  Spatial data in any form, including 0-dimensional points,\nlines, multi-lines, and polygons.  |  NOT SUPPORTED hana.ST_POINT  |  Spatial data in the form of 0-dimensional points that represents a single location in coordinate space.  |  NOT SUPPORTED"
            }
        }
    },
    {
        "category": "dlp-encryption-functions",
        "description": "**Preview**\n\nThis product or feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of the [ Service Specific Terms\n](/terms/service-terms) . Pre-GA products and features are available \"as is\"\nand might have limited support. For more information, see the [ launch stage descriptions ](/products#product-launch-stages) .\n\n**Note:** To provide feedback or request support for this feature, send an email to [ bigquery-sql-preview-support@google.com ](mailto:bigquery-sql-\npreview-support@google.com) .\n\nGoogleSQL for BigQuery supports the following DLP functions that allow interoperable encryption and decryption between BigQuery and [ Cloud Data Loss Prevention (Cloud DLP) ](https://cloud.google.com/dlp/docs) , using [ AES-SIV\n](https://cloud.google.com/dlp/docs/pseudonymization#aes-siv) .",
        "source": "dlp_functions.txt",
        "functions": {
            "DLP_DETERMINISTIC_ENCRYPT": {
                "name": "DLP_DETERMINISTIC_ENCRYPT",
                "summary": "Encrypts data with a DLP compatible algorithm.",
                "description": "DLP_DETERMINISTIC_ENCRYPT(key, plaintext, context)\n\n\nDLP_DETERMINISTIC_ENCRYPT(key, plaintext, context, surrogate)\n\n**Description**\n\nThis function derives a data encryption key from ` key ` and ` context ` , and then encrypts ` plaintext ` . Optionally, you can use ` surrogate ` to prepend the encryption result.\n\n**Definitions**\n\n* ` key ` : A serialized ` BYTES ` value that is returned by  ` DLP_KEY_CHAIN ` . ` key ` must be set to ` ENABLED ` in Cloud KMS. For information about how to generate a wrapped key, see [ gcloud kms encrypt ](https://cloud.google.com/sdk/gcloud/reference/kms/encrypt) .\n* ` plaintext ` : The ` STRING ` value to encrypt.\n* ` context ` : A user-provided ` STRING ` value that is used with a Cloud KMS key to derive a data encryption key. For more information, see [ CryptoDeterministicConfig:context ](https://cloud.google.com/dlp/docs/reference/rest/v2/projects.deidentifyTemplates#cryptodeterministicconfig) .\n* ` surrogate ` : A ` STRING ` value that you can prepend to output.\n\n**Return data type**\n\n` STRING `\n\n**Example**\n\n\nSELECT DLP_DETERMINISTIC_ENCRYPT( DLP_KEY_CHAIN(\n'gcp-kms://projects/my_project/locations/us-central1/keyRings/keyringtest/cryptoKeys/testkey',\nb'\\123\\044\\290\\876....'),\nplaintext,\n'',\n'test') AS results\n\n/*--------------------------------------*\n| results                              |\n+--------------------------------------+\n| AXDEwUnZsTf/NzxoHaC8AZXcawWuma7L39A= |\n*--------------------------------------*/"
            },
            "DLP_DETERMINISTIC_DECRYPT": {
                "name": "DLP_DETERMINISTIC_DECRYPT",
                "summary": "Decrypts DLP-encrypted data.",
                "description": "DLP_DETERMINISTIC_DECRYPT(key, ciphertext, context)\n\n\nDLP_DETERMINISTIC_DECRYPT(key, ciphertext, context, surrogate)\n\n**Description**\n\nThis function decrypts ` ciphertext ` using an encryption key derived from `\nkey ` and ` context ` . Optionally, you can use ` surrogate ` to prepend the decryption result.\n\n**Definitions**\n\n* ` key ` : A serialized ` BYTES ` value returned by  ` DLP_KEY_CHAIN ` . ` key ` must be set to ` ENABLED ` in Cloud KMS. For information about how to generate a wrapped key, see [ gcloud kms encrypt ](https://cloud.google.com/sdk/gcloud/reference/kms/encrypt) .\n* ` ciphertext ` : The ` STRING ` value to decrypt.\n* ` context ` : A ` STRING ` value that is used with a Cloud KMS key to derive a data encryption key. For more information, see [ CryptoDeterministicConfig:context ](https://cloud.google.com/dlp/docs/reference/rest/v2/projects.deidentifyTemplates#cryptodeterministicconfig) .\n* ` surrogate ` : A ` STRING ` value that you can prepend to output.\n\n**Return data type**\n\n` STRING `\n\n**Example**\n\n\nSELECT DLP_DETERMINISTIC_DECRYPT( DLP_KEY_CHAIN(\n'gcp-kms://projects/myproject/locations/us-central1/keyRings/keyringtest/cryptoKeys/testkey',\nb'\\123\\044\\290\\876....'),\n'your_surrogate(36)AdFnA6r5doSDWxPwW/W4vBaa4iOvDagC8z8=',\n'',\n'your_surrogate') AS results\n\n/*-----------*\n| results   |\n+-----------+\n| plaintext |\n*-----------*/"
            },
            "DLP_KEY_CHAIN": {
                "name": "DLP_KEY_CHAIN",
                "summary": "Gets a data encryption key that is wrapped by Cloud Key Management Service.",
                "description": "DLP_KEY_CHAIN(kms_resource_name, wrapped_key)\n\n**Description**\n\nYou can use this function instead of the ` key ` argument for DLP deterministic encryption functions. This function lets you use the [ AES-SIV encryption functions ](https://cloud.google.com/dlp/docs/pseudonymization#aes-\nsiv) without including ` plaintext ` keys in a query.\n\n**Definitions**\n\n* ` kms_resource_name ` : A ` STRING ` literal that contains the resource path to the Cloud KMS key. ` kms_resource_name ` cannot be ` NULL ` and must reside in the same Cloud region where this function is executed. This argument is used to derive the data encryption key in the ` DLP_DETERMINISTIC_DECRYPT ` and ` DLP_DETERMINISTIC_ENCRYPT ` functions. A Cloud KMS key looks like this:\n\ngcp-kms://projects/my-project/locations/us/keyRings/my-key-ring/cryptoKeys/my-crypto-key\n\n* ` wrapped_key ` : A ` BYTES ` literal that represents a secret text chosen by the user. This secret text can be 16, 24, or 32 bytes. For information about how to generate a wrapped key, see [ gcloud kms encrypt ](https://cloud.google.com/sdk/gcloud/reference/kms/encrypt) .\n\n**Return data type**\n\n` STRUCT `\n\n**Example**\n\n\nSELECT DLP_DETERMINISTIC_ENCRYPT( DLP_KEY_CHAIN(\n'gcp-kms://projects/my_project/locations/us-central1/keyRings/keyringtest/cryptoKeys/testkey',\nb'\\123\\044\\290\\876....'),\nplaintext,\n'',\n'test') AS results\n\n/*--------------------------------------*\n| results                              |\n+--------------------------------------+\n| AXDEwUnZsTf/NzxoHaC8AZXcawWuma7L39A= |\n*--------------------------------------*/"
            }
        }
    },
    {
        "category": "geography-functions",
        "description": "GoogleSQL for BigQuery supports geography functions. Geography functions operate on or generate GoogleSQL ` GEOGRAPHY ` values. The signature of most geography functions starts with ` ST_ ` . GoogleSQL for BigQuery supports the following functions that can be used to analyze geographical data, determine spatial relationships between geographical features, and construct or manipulate ` GEOGRAPHY ` s.\n\nAll GoogleSQL geography functions return ` NULL ` if any input argument is `\nNULL ` .\n\n###  Categories\n\nThe geography functions are grouped into the following categories based on their behavior:\n\nCategory  |  Functions  |  Description\n---|---|---\nConstructors  |  ` ST_GEOGPOINT `\n` ST_MAKELINE `\n` ST_MAKEPOLYGON `\n` ST_MAKEPOLYGONORIENTED ` |  Functions that build new geography values from coordinates or existing geographies.\nParsers  |  ` ST_GEOGFROM `\n` ST_GEOGFROMGEOJSON `\n` ST_GEOGFROMTEXT `\n` ST_GEOGFROMWKB `\n` ST_GEOGPOINTFROMGEOHASH `\n|  Functions that create geographies from an external format such as [ WKT\n](https://en.wikipedia.org/wiki/Well-known_text) and [ GeoJSON\n](https://en.wikipedia.org/wiki/GeoJSON) .\nFormatters  |  ` ST_ASBINARY `\n` ST_ASGEOJSON `\n` ST_ASTEXT `\n` ST_GEOHASH ` |  Functions that export geographies to an external format such as WKT.\nTransformations  |  ` ST_BOUNDARY `\n` ST_BUFFER `\n` ST_BUFFERWITHTOLERANCE `\n` ST_CENTROID `\n` ST_CENTROID_AGG ` (Aggregate)\n` ST_CLOSESTPOINT `\n` ST_CONVEXHULL `\n` ST_DIFFERENCE `\n` ST_EXTERIORRING `\n` ST_INTERIORRINGS `\n` ST_INTERSECTION `\n` ST_LINEINTERPOLATEPOINT `\n` ST_LINESUBSTRING `\n` ST_SIMPLIFY `\n` ST_SNAPTOGRID `\n` ST_UNION `\n` ST_UNION_AGG ` (Aggregate)\n|  Functions that generate a new geography based on input.\nAccessors  |  ` ST_DIMENSION `\n` ST_DUMP `\n` ST_ENDPOINT `\n` ST_GEOMETRYTYPE `\n` ST_ISCLOSED `\n` ST_ISCOLLECTION `\n` ST_ISEMPTY `\n` ST_ISRING `\n` ST_NPOINTS `\n` ST_NUMGEOMETRIES `\n` ST_NUMPOINTS `\n` ST_POINTN `\n` ST_STARTPOINT `\n` ST_X `\n` ST_Y `\n|  Functions that provide access to properties of a geography without side-\neffects.\nPredicates  |  ` ST_CONTAINS `\n` ST_COVEREDBY `\n` ST_COVERS `\n` ST_DISJOINT `\n` ST_DWITHIN `\n` ST_EQUALS `\n` ST_INTERSECTS `\n` ST_INTERSECTSBOX `\n` ST_TOUCHES `\n` ST_WITHIN `\n|  Functions that return ` TRUE ` or ` FALSE ` for some spatial relationship between two geographies or some property of a geography. These functions are commonly used in filter clauses.\nMeasures  |  ` ST_ANGLE `\n` ST_AREA `\n` ST_AZIMUTH `\n` ST_BOUNDINGBOX `\n` ST_DISTANCE `\n` ST_EXTENT ` (Aggregate)\n` ST_HAUSDORFFDISTANCE `\n` ST_LINELOCATEPOINT `\n` ST_LENGTH `\n` ST_MAXDISTANCE `\n` ST_PERIMETER `\n|  Functions that compute measurements of one or more geographies.\nClustering  |  ` ST_CLUSTERDBSCAN ` |  Functions that perform clustering on geographies.\nS2 functions  |  ` S2_CELLIDFROMPOINT `\n` S2_COVERINGCELLIDS `\n|  Functions for working with S2 cell coverings of GEOGRAPHY.",
        "source": "geography_functions.txt",
        "functions": {
            "S2_CELLIDFROMPOINT": {
                "name": "S2_CELLIDFROMPOINT",
                "summary": "Gets the S2 cell ID covering a point ` GEOGRAPHY `\nvalue.",
                "description": "S2_CELLIDFROMPOINT(point_geography[, level => cell_level])\n\n**Description**\n\nReturns the [ S2 cell ID ](https://s2geometry.io/devguide/s2cell_hierarchy) covering a point ` GEOGRAPHY ` .\n\n* The optional ` INT64 ` parameter ` level ` specifies the S2 cell level for the returned cell. Naming this argument is optional.\n\nThis is advanced functionality for interoperability with systems utilizing the\n[ S2 Geometry Library ](https://s2geometry.io/) .\n\n**Constraints**\n\n* Returns the cell ID as a signed ` INT64 ` bit-equivalent to [ unsigned 64-bit integer representation ](https://s2geometry.io/devguide/s2cell_hierarchy) .\n* Can return negative cell IDs.\n* Valid S2 cell levels are 0 to 30.\n* ` level ` defaults to 30 if not explicitly specified.\n* The function only supports a single point GEOGRAPHY. Use the ` SAFE ` prefix if the input can be multipoint, linestring, polygon, or an empty ` GEOGRAPHY ` .\n* To compute the covering of a complex ` GEOGRAPHY ` , use  S2_COVERINGCELLIDS  .\n\n**Return type**\n\n` INT64 `\n\n**Example**\n\n\nWITH data AS ( SELECT 1 AS id, ST_GEOGPOINT(-122, 47) AS geo UNION ALL\n-- empty geography is not supported SELECT 2 AS id, ST_GEOGFROMTEXT('POINT EMPTY') AS geo UNION ALL\n-- only points are supported SELECT 3 AS id, ST_GEOGFROMTEXT('LINESTRING(1 2, 3 4)') AS geo ) SELECT id,\nSAFE.S2_CELLIDFROMPOINT(geo) cell30,\nSAFE.S2_CELLIDFROMPOINT(geo, level => 10) cell10 FROM data;\n\n/*----+---------------------+---------------------*\n| id | cell30              | cell10              |\n+----+---------------------+---------------------+\n| 1  | 6093613931972369317 | 6093613287902019584 |\n| 2  | NULL                | NULL                |\n| 3  | NULL                | NULL                |\n*----+---------------------+---------------------*/"
            },
            "S2_COVERINGCELLIDS": {
                "name": "S2_COVERINGCELLIDS",
                "summary": "Gets an array of S2 cell IDs that cover a `\nGEOGRAPHY ` value.",
                "description": "S2_COVERINGCELLIDS( geography\n[, min_level => cell_level]\n[, max_level => cell_level]\n[, max_cells => max_cells]\n[, buffer => buffer])\n\n**Description**\n\nReturns an array of [ S2 cell IDs\n](https://s2geometry.io/devguide/s2cell_hierarchy) that cover the input `\nGEOGRAPHY ` . The function returns at most ` max_cells ` cells. The optional arguments ` min_level ` and ` max_level ` specify minimum and maximum levels for returned S2 cells. The array size is limited by the optional ` max_cells `\nargument. The optional ` buffer ` argument specifies a buffering factor in meters; the region being covered is expanded from the extent of the input geography by this amount.\n\nThis is advanced functionality for interoperability with systems utilizing the\n[ S2 Geometry Library ](https://s2geometry.io/) .\n\n**Constraints**\n\n* Returns the cell ID as a signed ` INT64 ` bit-equivalent to [ unsigned 64-bit integer representation ](https://s2geometry.io/devguide/s2cell_hierarchy) .\n* Can return negative cell IDs.\n* Valid S2 cell levels are 0 to 30.\n* ` max_cells ` defaults to 8 if not explicitly specified.\n* ` buffer ` should be nonnegative. It defaults to 0.0 meters if not explicitly specified.\n\n**Return type**\n\n` ARRAY<INT64> `\n\n**Example**\n\n\nWITH data AS ( SELECT 1 AS id, ST_GEOGPOINT(-122, 47) AS geo UNION ALL SELECT 2 AS id, ST_GEOGFROMTEXT('POINT EMPTY') AS geo UNION ALL SELECT 3 AS id, ST_GEOGFROMTEXT('LINESTRING(-122.12 47.67, -122.19 47.69)') AS geo ) SELECT id, S2_COVERINGCELLIDS(geo, min_level => 12) cells FROM data;\n\n/*----+--------------------------------------------------------------------------------------*\n| id | cells                                                                                |\n+----+--------------------------------------------------------------------------------------+\n| 1  | [6093613931972369317]                                                                |\n| 2  | []                                                                                   |\n| 3  | [6093384954555662336, 6093390709811838976, 6093390735581642752, 6093390740145045504, |\n|    |  6093390791416217600, 6093390812891054080, 6093390817187069952, 6093496378892222464] |\n*----+--------------------------------------------------------------------------------------*/"
            },
            "ST_ANGLE": {
                "name": "ST_ANGLE",
                "summary": "Takes three point ` GEOGRAPHY ` values, which represent two intersecting lines, and returns the angle between these lines.",
                "description": "ST_ANGLE(point_geography_1, point_geography_2, point_geography_3)\n\n**Description**\n\nTakes three point ` GEOGRAPHY ` values, which represent two intersecting lines. Returns the angle between these lines. Point 2 and point 1 represent the first line and point 2 and point 3 represent the second line. The angle between these lines is in radians, in the range ` [0, 2pi) ` . The angle is measured clockwise from the first line to the second line.\n\n` ST_ANGLE ` has the following edge cases:\n\n* If points 2 and 3 are the same, returns ` NULL ` .\n* If points 2 and 1 are the same, returns ` NULL ` .\n* If points 2 and 3 are exactly antipodal, returns ` NULL ` .\n* If points 2 and 1 are exactly antipodal, returns ` NULL ` .\n* If any of the input geographies are not single points or are the empty geography, then throws an error.\n\n**Return type**\n\n` FLOAT64 `\n\n**Example**\n\n\nWITH geos AS ( SELECT 1 id, ST_GEOGPOINT(1, 0) geo1, ST_GEOGPOINT(0, 0) geo2, ST_GEOGPOINT(0, 1) geo3 UNION ALL SELECT 2 id, ST_GEOGPOINT(0, 0), ST_GEOGPOINT(1, 0), ST_GEOGPOINT(0, 1) UNION ALL SELECT 3 id, ST_GEOGPOINT(1, 0), ST_GEOGPOINT(0, 0), ST_GEOGPOINT(1, 0) UNION ALL SELECT 4 id, ST_GEOGPOINT(1, 0) geo1, ST_GEOGPOINT(0, 0) geo2, ST_GEOGPOINT(0, 0) geo3 UNION ALL SELECT 5 id, ST_GEOGPOINT(0, 0), ST_GEOGPOINT(-30, 0), ST_GEOGPOINT(150, 0) UNION ALL SELECT 6 id, ST_GEOGPOINT(0, 0), NULL, NULL UNION ALL SELECT 7 id, NULL, ST_GEOGPOINT(0, 0), NULL UNION ALL SELECT 8 id, NULL, NULL, ST_GEOGPOINT(0, 0)) SELECT ST_ANGLE(geo1,geo2,geo3) AS angle FROM geos ORDER BY id;\n\n/*---------------------*\n| angle               |\n+---------------------+\n| 4.71238898038469    |\n| 0.78547432161873854 |\n| 0                   |\n| NULL                |\n| NULL                |\n| NULL                |\n| NULL                |\n| NULL                |\n*---------------------*/"
            },
            "ST_AREA": {
                "name": "ST_AREA",
                "summary": "Gets the area covered by the polygons in a ` GEOGRAPHY ` value.",
                "description": "ST_AREA(geography_expression[, use_spheroid])\n\n**Description**\n\nReturns the area in square meters covered by the polygons in the input `\nGEOGRAPHY ` .\n\nIf ` geography_expression ` is a point or a line, returns zero. If `\ngeography_expression ` is a collection, returns the area of the polygons in the collection; if the collection does not contain polygons, returns zero.\n\nThe optional ` use_spheroid ` parameter determines how this function measures distance. If ` use_spheroid ` is ` FALSE ` , the function measures distance on the surface of a perfect sphere.\n\nThe ` use_spheroid ` parameter currently only supports the value ` FALSE ` .\nThe default value of ` use_spheroid ` is ` FALSE ` .\n\n**Return type**\n\n` FLOAT64 `"
            },
            "ST_ASBINARY": {
                "name": "ST_ASBINARY",
                "summary": "Converts a ` GEOGRAPHY ` value to a ` BYTES ` WKB geography value.",
                "description": "ST_ASBINARY(geography_expression)\n\n**Description**\n\nReturns the [ WKB ](https://en.wikipedia.org/wiki/Well-known_text#Well-\nknown_binary) representation of an input ` GEOGRAPHY ` .\n\nSee  ` ST_GEOGFROMWKB ` to construct a ` GEOGRAPHY ` from WKB.\n\n**Return type**\n\n` BYTES `"
            },
            "ST_ASGEOJSON": {
                "name": "ST_ASGEOJSON",
                "summary": "Converts a ` GEOGRAPHY ` value to a ` STRING ` GeoJSON geography value.",
                "description": "ST_ASGEOJSON(geography_expression)\n\n**Description**\n\nReturns the [ RFC 7946 ](https://tools.ietf.org/html/rfc7946) compliant [\nGeoJSON ](https://en.wikipedia.org/wiki/GeoJSON) representation of the input `\nGEOGRAPHY ` .\n\nA GoogleSQL ` GEOGRAPHY ` has spherical geodesic edges, whereas a GeoJSON `\nGeometry ` object explicitly has planar edges. To convert between these two types of edges, GoogleSQL adds additional points to the line where necessary so that the resulting sequence of edges remains within 10 meters of the original edge.\n\nSee  ` ST_GEOGFROMGEOJSON ` to construct a ` GEOGRAPHY ` from GeoJSON.\n\n**Return type**\n\n` STRING `"
            },
            "ST_ASTEXT": {
                "name": "ST_ASTEXT",
                "summary": "Converts a ` GEOGRAPHY ` value to a ` STRING ` WKT geography value.",
                "description": "ST_ASTEXT(geography_expression)\n\n**Description**\n\nReturns the [ WKT ](https://en.wikipedia.org/wiki/Well-known_text) representation of an input ` GEOGRAPHY ` .\n\nSee  ` ST_GEOGFROMTEXT ` to construct a ` GEOGRAPHY ` from WKT.\n\n**Return type**\n\n` STRING `"
            },
            "ST_AZIMUTH": {
                "name": "ST_AZIMUTH",
                "summary": "Gets the azimuth of a line segment formed by two point `\nGEOGRAPHY ` values.",
                "description": "ST_AZIMUTH(point_geography_1, point_geography_2)\n\n**Description**\n\nTakes two point ` GEOGRAPHY ` values, and returns the azimuth of the line segment formed by points 1 and 2. The azimuth is the angle in radians measured between the line from point 1 facing true North to the line segment from point 1 to point 2.\n\nThe positive angle is measured clockwise on the surface of a sphere. For example, the azimuth for a line segment:\n\n* Pointing North is ` 0 `\n* Pointing East is ` PI/2 `\n* Pointing South is ` PI `\n* Pointing West is ` 3PI/2 `\n\n` ST_AZIMUTH ` has the following edge cases:\n\n* If the two input points are the same, returns ` NULL ` .\n* If the two input points are exactly antipodal, returns ` NULL ` .\n* If either of the input geographies are not single points or are the empty geography, throws an error.\n\n**Return type**\n\n` FLOAT64 `\n\n**Example**\n\n\nWITH geos AS ( SELECT 1 id, ST_GEOGPOINT(1, 0) AS geo1, ST_GEOGPOINT(0, 0) AS geo2 UNION ALL SELECT 2, ST_GEOGPOINT(0, 0), ST_GEOGPOINT(1, 0) UNION ALL SELECT 3, ST_GEOGPOINT(0, 0), ST_GEOGPOINT(0, 1) UNION ALL\n-- identical SELECT 4, ST_GEOGPOINT(0, 0), ST_GEOGPOINT(0, 0) UNION ALL\n-- antipode SELECT 5, ST_GEOGPOINT(-30, 0), ST_GEOGPOINT(150, 0) UNION ALL\n-- nulls SELECT 6, ST_GEOGPOINT(0, 0), NULL UNION ALL SELECT 7, NULL, ST_GEOGPOINT(0, 0)) SELECT ST_AZIMUTH(geo1, geo2) AS azimuth FROM geos ORDER BY id;\n\n/*--------------------*\n| azimuth            |\n+--------------------+\n| 4.71238898038469   |\n| 1.5707963267948966 |\n| 0                  |\n| NULL               |\n| NULL               |\n| NULL               |\n| NULL               |\n*--------------------*/"
            },
            "ST_BOUNDARY": {
                "name": "ST_BOUNDARY",
                "summary": "Gets the union of component boundaries in a ` GEOGRAPHY `\nvalue.",
                "description": "ST_BOUNDARY(geography_expression)\n\n**Description**\n\nReturns a single ` GEOGRAPHY ` that contains the union of the boundaries of each component in the given input ` GEOGRAPHY ` .\n\nThe boundary of each component of a ` GEOGRAPHY ` is defined as follows:\n\n* The boundary of a point is empty.\n* The boundary of a linestring consists of the endpoints of the linestring.\n* The boundary of a polygon consists of the linestrings that form the polygon shell and each of the polygon's holes.\n\n**Return type**\n\n` GEOGRAPHY `"
            },
            "ST_BOUNDINGBOX": {
                "name": "ST_BOUNDINGBOX",
                "summary": "Gets the bounding box for a ` GEOGRAPHY ` value.",
                "description": "ST_BOUNDINGBOX(geography_expression)\n\n**Description**\n\nReturns a ` STRUCT ` that represents the bounding box for the specified geography. The bounding box is the minimal rectangle that encloses the geography. The edges of the rectangle follow constant lines of longitude and latitude.\n\nCaveats:\n\n* Returns ` NULL ` if the input is ` NULL ` or an empty geography.\n* The bounding box might cross the antimeridian if this allows for a smaller rectangle. In this case, the bounding box has one of its longitudinal bounds outside of the [-180, 180] range, so that ` xmin ` is smaller than the eastmost value ` xmax ` .\n\n**Return type**\n\n` STRUCT<xmin FLOAT64, ymin FLOAT64, xmax FLOAT64, ymax FLOAT64> ` .\n\nBounding box parts:\n\n* ` xmin ` : The westmost constant longitude line that bounds the rectangle.\n* ` xmax ` : The eastmost constant longitude line that bounds the rectangle.\n* ` ymin ` : The minimum constant latitude line that bounds the rectangle.\n* ` ymax ` : The maximum constant latitude line that bounds the rectangle.\n\n**Example**\n\n\nWITH data AS ( SELECT 1 id, ST_GEOGFROMTEXT('POLYGON((-125 48, -124 46, -117 46, -117 49, -125 48))') g UNION ALL SELECT 2 id, ST_GEOGFROMTEXT('POLYGON((172 53, -130 55, -141 70, 172 53))') g UNION ALL SELECT 3 id, ST_GEOGFROMTEXT('POINT EMPTY') g UNION ALL SELECT 4 id, ST_GEOGFROMTEXT('POLYGON((172 53, -141 70, -130 55, 172 53))', oriented => TRUE) ) SELECT id, ST_BOUNDINGBOX(g) AS box FROM data\n\n/*----+------------------------------------------*\n| id | box                                      |\n+----+------------------------------------------+\n| 1  | {xmin:-125, ymin:46, xmax:-117, ymax:49} |\n| 2  | {xmin:172, ymin:53, xmax:230, ymax:70}   |\n| 3  | NULL                                     |\n| 4  | {xmin:-180, ymin:-90, xmax:180, ymax:90} |\n*----+------------------------------------------*/\n\nSee  ` ST_EXTENT ` for the aggregate version of ` ST_BOUNDINGBOX ` ."
            },
            "ST_BUFFER": {
                "name": "ST_BUFFER",
                "summary": "Gets the buffer around a ` GEOGRAPHY ` value, using a specific number of segments.",
                "description": "ST_BUFFER( geography,\nbuffer_radius\n[, num_seg_quarter_circle => num_segments]\n[, use_spheroid => boolean_expression]\n[, endcap => endcap_style]\n[, side => line_side])\n\n**Description**\n\nReturns a ` GEOGRAPHY ` that represents the buffer around the input `\nGEOGRAPHY ` . This function is similar to  ` ST_BUFFERWITHTOLERANCE ` , but you specify the number of segments instead of providing tolerance to determine how much the resulting geography can deviate from the ideal buffer radius.\n\n* ` geography ` : The input ` GEOGRAPHY ` to encircle with the buffer radius.\n* ` buffer_radius ` : ` FLOAT64 ` that represents the radius of the buffer around the input geography. The radius is in meters. Note that polygons contract when buffered with a negative ` buffer_radius ` . Polygon shells and holes that are contracted to a point are discarded.\n* ` num_seg_quarter_circle ` : (Optional) ` FLOAT64 ` specifies the number of segments that are used to approximate a quarter circle. The default value is ` 8.0 ` . Naming this argument is optional.\n* ` endcap ` : (Optional) ` STRING ` allows you to specify one of two endcap styles: ` ROUND ` and ` FLAT ` . The default value is ` ROUND ` . This option only affects the endcaps of buffered linestrings.\n* ` side ` : (Optional) ` STRING ` allows you to specify one of three possibilities for lines: ` BOTH ` , ` LEFT ` , and ` RIGHT ` . The default is ` BOTH ` . This option only affects how linestrings are buffered.\n* ` use_spheroid ` : (Optional) ` BOOL ` determines how this function measures distance. If ` use_spheroid ` is ` FALSE ` , the function measures distance on the surface of a perfect sphere. The ` use_spheroid ` parameter currently only supports the value ` FALSE ` . The default value of ` use_spheroid ` is ` FALSE ` .\n\n**Return type**\n\nPolygon ` GEOGRAPHY `\n\n**Example**\n\nThe following example shows the result of ` ST_BUFFER ` on a point. A buffered point is an approximated circle. When ` num_seg_quarter_circle = 2 ` , there are two line segments in a quarter circle, and therefore the buffered circle has eight sides and  ` ST_NUMPOINTS ` returns nine vertices. When `\nnum_seg_quarter_circle = 8 ` , there are eight line segments in a quarter circle, and therefore the buffered circle has thirty-two sides and  `\nST_NUMPOINTS ` returns thirty-three vertices.\n\n\nSELECT\n-- num_seg_quarter_circle=2 ST_NUMPOINTS(ST_BUFFER(ST_GEOGFROMTEXT('POINT(1 2)'), 50, 2)) AS eight_sides,\n-- num_seg_quarter_circle=8, since 8 is the default ST_NUMPOINTS(ST_BUFFER(ST_GEOGFROMTEXT('POINT(100 2)'), 50)) AS thirty_two_sides;\n\n/*-------------+------------------*\n| eight_sides | thirty_two_sides |\n+-------------+------------------+\n| 9           | 33               |\n*-------------+------------------*/"
            },
            "ST_BUFFERWITHTOLERANCE": {
                "name": "ST_BUFFERWITHTOLERANCE",
                "summary": "Gets the buffer around a ` GEOGRAPHY ` value,\nusing tolerance.",
                "description": "ST_BUFFERWITHTOLERANCE( geography,\nbuffer_radius,\ntolerance_meters => tolerance\n[, use_spheroid => boolean_expression]\n[, endcap => endcap_style]\n[, side => line_side])\n\nReturns a ` GEOGRAPHY ` that represents the buffer around the input `\nGEOGRAPHY ` . This function is similar to  ` ST_BUFFER ` , but you provide tolerance instead of segments to determine how much the resulting geography can deviate from the ideal buffer radius.\n\n* ` geography ` : The input ` GEOGRAPHY ` to encircle with the buffer radius.\n* ` buffer_radius ` : ` FLOAT64 ` that represents the radius of the buffer around the input geography. The radius is in meters. Note that polygons contract when buffered with a negative ` buffer_radius ` . Polygon shells and holes that are contracted to a point are discarded.\n* ` tolerance_meters ` : ` FLOAT64 ` specifies a tolerance in meters with which the shape is approximated. Tolerance determines how much a polygon can deviate from the ideal radius. Naming this argument is optional.\n* ` endcap ` : (Optional) ` STRING ` allows you to specify one of two endcap styles: ` ROUND ` and ` FLAT ` . The default value is ` ROUND ` . This option only affects the endcaps of buffered linestrings.\n* ` side ` : (Optional) ` STRING ` allows you to specify one of three possible line styles: ` BOTH ` , ` LEFT ` , and ` RIGHT ` . The default is ` BOTH ` . This option only affects the endcaps of buffered linestrings.\n* ` use_spheroid ` : (Optional) ` BOOL ` determines how this function measures distance. If ` use_spheroid ` is ` FALSE ` , the function measures distance on the surface of a perfect sphere. The ` use_spheroid ` parameter currently only supports the value ` FALSE ` . The default value of ` use_spheroid ` is ` FALSE ` .\n\n**Return type**\n\nPolygon ` GEOGRAPHY `\n\n**Example**\n\nThe following example shows the results of ` ST_BUFFERWITHTOLERANCE ` on a point, given two different values for tolerance but with the same buffer radius of ` 100 ` . A buffered point is an approximated circle. When `\ntolerance_meters=25 ` , the tolerance is a large percentage of the buffer radius, and therefore only five segments are used to approximate a circle around the input point. When ` tolerance_meters=1 ` , the tolerance is a much smaller percentage of the buffer radius, and therefore twenty-four edges are used to approximate a circle around the input point.\n\n\nSELECT\n-- tolerance_meters=25, or 25% of the buffer radius.\nST_NumPoints(ST_BUFFERWITHTOLERANCE(ST_GEOGFROMTEXT('POINT(1 2)'), 100, 25)) AS five_sides,\n-- tolerance_meters=1, or 1% of the buffer radius.\nst_NumPoints(ST_BUFFERWITHTOLERANCE(ST_GEOGFROMTEXT('POINT(100 2)'), 100, 1)) AS twenty_four_sides;\n\n/*------------+-------------------*\n| five_sides | twenty_four_sides |\n+------------+-------------------+\n| 6          | 24                |\n*------------+-------------------*/"
            },
            "ST_CENTROID": {
                "name": "ST_CENTROID",
                "summary": "Gets the centroid of a ` GEOGRAPHY ` value.",
                "description": "ST_CENTROID(geography_expression)\n\n**Description**\n\nReturns the _centroid_ of the input ` GEOGRAPHY ` as a single point `\nGEOGRAPHY ` .\n\nThe _centroid_ of a ` GEOGRAPHY ` is the weighted average of the centroids of the highest-dimensional components in the ` GEOGRAPHY ` . The centroid for components in each dimension is defined as follows:\n\n* The centroid of points is the arithmetic mean of the input coordinates.\n* The centroid of linestrings is the centroid of all the edges weighted by length. The centroid of each edge is the geodesic midpoint of the edge.\n* The centroid of a polygon is its center of mass.\n\nIf the input ` GEOGRAPHY ` is empty, an empty ` GEOGRAPHY ` is returned.\n\n**Constraints**\n\nIn the unlikely event that the centroid of a ` GEOGRAPHY ` cannot be defined by a single point on the surface of the Earth, a deterministic but otherwise arbitrary point is returned. This can only happen if the centroid is exactly at the center of the Earth, such as the centroid for a pair of antipodal points, and the likelihood of this happening is vanishingly small.\n\n**Return type**\n\nPoint ` GEOGRAPHY `"
            },
            "ST_CENTROID_AGG": {
                "name": "ST_CENTROID_AGG",
                "summary": "Gets the centroid of a set of ` GEOGRAPHY ` values.",
                "description": "ST_CENTROID_AGG(geography)\n\n**Description**\n\nComputes the centroid of the set of input ` GEOGRAPHY ` s as a single point `\nGEOGRAPHY ` .\n\nThe _centroid_ over the set of input ` GEOGRAPHY ` s is the weighted average of the centroid of each individual ` GEOGRAPHY ` . Only the ` GEOGRAPHY ` s with the highest dimension present in the input contribute to the centroid of the entire set. For example, if the input contains both ` GEOGRAPHY ` s with lines and ` GEOGRAPHY ` s with only points, ` ST_CENTROID_AGG ` returns the weighted average of the ` GEOGRAPHY ` s with lines, since those have maximal dimension. In this example, ` ST_CENTROID_AGG ` ignores ` GEOGRAPHY ` s with only points when calculating the aggregate centroid.\n\n` ST_CENTROID_AGG ` ignores ` NULL ` input ` GEOGRAPHY ` values.\n\nSee  ` ST_CENTROID ` for the non-aggregate version of ` ST_CENTROID_AGG ` and the definition of centroid for an individual ` GEOGRAPHY ` value.\n\n**Return type**\n\nPoint ` GEOGRAPHY `\n\n**Example**\n\nThe following queries compute the aggregate centroid over a set of ` GEOGRAPHY\n` values. The input to the first query contains only points, and therefore each value contribute to the aggregate centroid. Also notice that `\nST_CENTROID_AGG ` is _not_ equivalent to calling ` ST_CENTROID ` on the result of ` ST_UNION_AGG ` ; duplicates are removed by the union, unlike `\nST_CENTROID_AGG ` . The input to the second query has mixed dimensions, and only values with the highest dimension in the set, the lines, affect the aggregate centroid.\n\n\nSELECT ST_CENTROID_AGG(points) AS st_centroid_agg,\nST_CENTROID(ST_UNION_AGG(points)) AS centroid_of_union FROM UNNEST([ST_GEOGPOINT(1, 5),\nST_GEOGPOINT(1, 2),\nST_GEOGPOINT(1, -1),\nST_GEOGPOINT(1, -1)]) points;\n\n/*---------------------------+-------------------*\n| st_centroid_agg           | centroid_of_union |\n+---------------------------+-------------------+\n| POINT(1 1.24961422620969) | POINT(1 2)        |\n*---------------------------+-------------------*/\n\n\nSELECT ST_CENTROID_AGG(points) AS st_centroid_agg FROM UNNEST([ST_GEOGPOINT(50, 26),\nST_GEOGPOINT(34, 33.3),\nST_GEOGFROMTEXT('LINESTRING(0 -1, 0 1)'),\nST_GEOGFROMTEXT('LINESTRING(0 1, 0 3)')]) points;\n\n/*-----------------*\n| st_centroid_agg |\n+-----------------+\n| POINT(0 1)      |\n*-----------------*/"
            },
            "ST_CLOSESTPOINT": {
                "name": "ST_CLOSESTPOINT",
                "summary": "Gets the point on a ` GEOGRAPHY ` value which is closest to any point in a second ` GEOGRAPHY ` value.",
                "description": "ST_CLOSESTPOINT(geography_1, geography_2[, use_spheroid])\n\n**Description**\n\nReturns a ` GEOGRAPHY ` containing a point on ` geography_1 ` with the smallest possible distance to ` geography_2 ` . This implies that the distance between the point returned by ` ST_CLOSESTPOINT ` and ` geography_2 ` is less than or equal to the distance between any other point on ` geography_1 ` and `\ngeography_2 ` .\n\nIf either of the input ` GEOGRAPHY ` s is empty, ` ST_CLOSESTPOINT ` returns `\nNULL ` .\n\nThe optional ` use_spheroid ` parameter determines how this function measures distance. If ` use_spheroid ` is ` FALSE ` , the function measures distance on the surface of a perfect sphere.\n\nThe ` use_spheroid ` parameter currently only supports the value ` FALSE ` .\nThe default value of ` use_spheroid ` is ` FALSE ` .\n\n**Return type**\n\nPoint ` GEOGRAPHY `"
            },
            "ST_CLUSTERDBSCAN": {
                "name": "ST_CLUSTERDBSCAN",
                "summary": "Performs DBSCAN clustering on a group of ` GEOGRAPHY `\nvalues and produces a 0-based cluster number for this row.",
                "description": "ST_CLUSTERDBSCAN(geography_column, epsilon, minimum_geographies) OVER over_clause\n\nover_clause:\n{ named_window | ( [ window_specification ] ) }\n\nwindow_specification:\n[ named_window ]\n[ PARTITION BY partition_expression [, ...] ]\n[ ORDER BY expression [ { ASC | DESC }  ] [, ...] ]\n\n\nPerforms [ DBSCAN clustering ](https://en.wikipedia.org/wiki/DBSCAN) on a column of geographies. Returns a 0-based cluster number.\n\nTo learn more about the ` OVER ` clause and how to use it, see [ Window function calls ](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n**Input parameters**\n\n* ` geography_column ` : A column of ` GEOGRAPHY ` s that is clustered.\n* ` epsilon ` : The epsilon that specifies the radius, measured in meters, around a core value. Non-negative ` FLOAT64 ` value.\n* ` minimum_geographies ` : Specifies the minimum number of geographies in a single cluster. Only dense input forms a cluster, otherwise it is classified as noise. Non-negative ` INT64 ` value.\n\n**Geography types and the DBSCAN algorithm**\n\nThe DBSCAN algorithm identifies high-density clusters of data and marks outliers in low-density areas of noise. Geographies passed in through `\ngeography_column ` are classified in one of three ways by the DBSCAN algorithm:\n\n* Core value: A geography is a core value if it is within ` epsilon ` distance of ` minimum_geographies ` geographies, including itself. The core value starts a new cluster, or is added to the same cluster as a core value within ` epsilon ` distance. Core values are grouped in a cluster together with all other core and border values that are within ` epsilon ` distance.\n* Border value: A geography is a border value if it is within epsilon distance of a core value. It is added to the same cluster as a core value within ` epsilon ` distance. A border value may be within ` epsilon ` distance of more than one cluster. In this case, it may be arbitrarily assigned to either cluster and the function will produce the same result in subsequent calls.\n* Noise: A geography is noise if it is neither a core nor a border value. Noise values are assigned to a ` NULL ` cluster. An empty ` GEOGRAPHY ` is always classified as noise.\n\n**Constraints**\n\n* The argument ` minimum_geographies ` is a non-negative ` INT64 ` and ` epsilon ` is a non-negative ` FLOAT64 ` .\n* An empty geography cannot join any cluster.\n* Multiple clustering assignments could be possible for a border value. If a geography is a border value, ` ST_CLUSTERDBSCAN ` will assign it to an arbitrary valid cluster.\n\n**Return type**\n\n` INT64 ` for each geography in the geography column.\n\n**Examples**\n\nThis example performs DBSCAN clustering with a radius of 100,000 meters with a\n` minimum_geographies ` argument of 1. The geographies being analyzed are a mixture of points, lines, and polygons.\n\n\nWITH Geos as (SELECT 1 as row_id, ST_GEOGFROMTEXT('POINT EMPTY') as geo UNION ALL SELECT 2, ST_GEOGFROMTEXT('MULTIPOINT(1 1, 2 2, 4 4, 5 2)') UNION ALL SELECT 3, ST_GEOGFROMTEXT('POINT(14 15)') UNION ALL SELECT 4, ST_GEOGFROMTEXT('LINESTRING(40 1, 42 34, 44 39)') UNION ALL SELECT 5, ST_GEOGFROMTEXT('POLYGON((40 2, 40 1, 41 2, 40 2))')) SELECT row_id, geo, ST_CLUSTERDBSCAN(geo, 1e5, 1) OVER () AS cluster_num FROM Geos ORDER BY row_id\n\n/*--------+-----------------------------------+-------------*\n| row_id |                geo                | cluster_num |\n+--------+-----------------------------------+-------------+\n| 1      | GEOMETRYCOLLECTION EMPTY          | NULL        |\n| 2      | MULTIPOINT(1 1, 2 2, 5 2, 4 4)    | 0           |\n| 3      | POINT(14 15)                      | 1           |\n| 4      | LINESTRING(40 1, 42 34, 44 39)    | 2           |\n| 5      | POLYGON((40 2, 40 1, 41 2, 40 2)) | 2           |\n*--------+-----------------------------------+-------------*/"
            },
            "ST_CONTAINS": {
                "name": "ST_CONTAINS",
                "summary": "Checks if one ` GEOGRAPHY ` value contains another `\nGEOGRAPHY ` value.",
                "description": "ST_CONTAINS(geography_1, geography_2)\n\n**Description**\n\nReturns ` TRUE ` if no point of ` geography_2 ` is outside ` geography_1 ` ,\nand the interiors intersect; returns ` FALSE ` otherwise.\n\nNOTE: A ` GEOGRAPHY ` _does not_ contain its own boundary. Compare with  `\nST_COVERS ` .\n\n**Return type**\n\n` BOOL `\n\n**Example**\n\nThe following query tests whether the polygon ` POLYGON((1 1, 20 1, 10 20, 1 1)) ` contains each of the three points ` (0, 0) ` , ` (1, 1) ` , and ` (10,\n10) ` , which lie on the exterior, the boundary, and the interior of the polygon respectively.\n\n\nSELECT ST_GEOGPOINT(i, i) AS p,\nST_CONTAINS(ST_GEOGFROMTEXT('POLYGON((1 1, 20 1, 10 20, 1 1))'),\nST_GEOGPOINT(i, i)) AS `contains`\nFROM UNNEST([0, 1, 10]) AS i;\n\n/*--------------+----------*\n| p            | contains |\n+--------------+----------+\n| POINT(0 0)   | FALSE    |\n| POINT(1 1)   | FALSE    |\n| POINT(10 10) | TRUE     |\n*--------------+----------*/"
            },
            "ST_CONVEXHULL": {
                "name": "ST_CONVEXHULL",
                "summary": "Returns the convex hull for a ` GEOGRAPHY ` value.",
                "description": "ST_CONVEXHULL(geography_expression)\n\n**Description**\n\nReturns the convex hull for the input ` GEOGRAPHY ` . The convex hull is the smallest convex ` GEOGRAPHY ` that covers the input. A ` GEOGRAPHY ` is convex if for every pair of points in the ` GEOGRAPHY ` , the geodesic edge connecting the points are also contained in the same ` GEOGRAPHY ` .\n\nIn most cases, the convex hull consists of a single polygon. Notable edge cases include the following:\n\n* The convex hull of a single point is also a point.\n* The convex hull of two or more collinear points is a linestring as long as that linestring is convex.\n* If the input ` GEOGRAPHY ` spans more than a hemisphere, the convex hull is the full globe. This includes any input that contains a pair of antipodal points.\n* ` ST_CONVEXHULL ` returns ` NULL ` if the input is either ` NULL ` or the empty ` GEOGRAPHY ` .\n\n**Return type**\n\n` GEOGRAPHY `\n\n**Examples**\n\nThe convex hull returned by ` ST_CONVEXHULL ` can be a point, linestring, or a polygon, depending on the input.\n\n\nWITH Geographies AS (SELECT ST_GEOGFROMTEXT('POINT(1 1)') AS g UNION ALL SELECT ST_GEOGFROMTEXT('LINESTRING(1 1, 2 2)') AS g UNION ALL SELECT ST_GEOGFROMTEXT('MULTIPOINT(2 11, 4 12, 0 15, 1 9, 1 12)') AS g) SELECT g AS input_geography,\nST_CONVEXHULL(g) AS convex_hull FROM Geographies;\n\n/*-----------------------------------------+--------------------------------------------------------*\n|             input_geography             |                      convex_hull                       |\n+-----------------------------------------+--------------------------------------------------------+\n| POINT(1 1)                              | POINT(0.999999999999943 1)                             |\n| LINESTRING(1 1, 2 2)                    | LINESTRING(2 2, 1.49988573656168 1.5000570914792, 1 1) |\n| MULTIPOINT(1 9, 4 12, 2 11, 1 12, 0 15) | POLYGON((1 9, 4 12, 0 15, 1 9))                        |\n*-----------------------------------------+--------------------------------------------------------*/"
            },
            "ST_COVEREDBY": {
                "name": "ST_COVEREDBY",
                "summary": "Checks if all points of a ` GEOGRAPHY ` value are on the boundary or interior of another ` GEOGRAPHY ` value.",
                "description": "ST_COVEREDBY(geography_1, geography_2)\n\n**Description**\n\nReturns ` FALSE ` if ` geography_1 ` or ` geography_2 ` is empty. Returns `\nTRUE ` if no points of ` geography_1 ` lie in the exterior of ` geography_2 `\n.\n\nGiven two ` GEOGRAPHY ` s ` a ` and ` b ` , ` ST_COVEREDBY(a, b) ` returns the same result as  ` ST_COVERS ` ` (b, a) ` . Note the opposite order of arguments.\n\n**Return type**\n\n` BOOL `"
            },
            "ST_COVERS": {
                "name": "ST_COVERS",
                "summary": "Checks if all points of a ` GEOGRAPHY ` value are on the boundary or interior of another ` GEOGRAPHY ` value.",
                "description": "ST_COVERS(geography_1, geography_2)\n\n**Description**\n\nReturns ` FALSE ` if ` geography_1 ` or ` geography_2 ` is empty. Returns `\nTRUE ` if no points of ` geography_2 ` lie in the exterior of ` geography_1 `\n.\n\n**Return type**\n\n` BOOL `\n\n**Example**\n\nThe following query tests whether the polygon ` POLYGON((1 1, 20 1, 10 20, 1 1)) ` covers each of the three points ` (0, 0) ` , ` (1, 1) ` , and ` (10, 10)\n` , which lie on the exterior, the boundary, and the interior of the polygon respectively.\n\n\nSELECT ST_GEOGPOINT(i, i) AS p,\nST_COVERS(ST_GEOGFROMTEXT('POLYGON((1 1, 20 1, 10 20, 1 1))'),\nST_GEOGPOINT(i, i)) AS `covers`\nFROM UNNEST([0, 1, 10]) AS i;\n\n/*--------------+--------*\n| p            | covers |\n+--------------+--------+\n| POINT(0 0)   | FALSE  |\n| POINT(1 1)   | TRUE   |\n| POINT(10 10) | TRUE   |\n*--------------+--------*/"
            },
            "ST_DIFFERENCE": {
                "name": "ST_DIFFERENCE",
                "summary": "Gets the point set difference between two ` GEOGRAPHY `\nvalues.",
                "description": "ST_DIFFERENCE(geography_1, geography_2)\n\n**Description**\n\nReturns a ` GEOGRAPHY ` that represents the point set difference of `\ngeography_1 ` and ` geography_2 ` . Therefore, the result consists of the part of ` geography_1 ` that does not intersect with ` geography_2 ` .\n\nIf ` geometry_1 ` is completely contained in ` geometry_2 ` , then `\nST_DIFFERENCE ` returns an empty ` GEOGRAPHY ` .\n\n**Constraints**\n\nThe underlying geometric objects that a GoogleSQL ` GEOGRAPHY ` represents correspond to a _closed_ point set. Therefore, ` ST_DIFFERENCE ` is the closure of the point set difference of ` geography_1 ` and ` geography_2 ` .\nThis implies that if ` geography_1 ` and ` geography_2 ` intersect, then a portion of the boundary of ` geography_2 ` could be in the difference.\n\n**Return type**\n\n` GEOGRAPHY `\n\n**Example**\n\nThe following query illustrates the difference between ` geog1 ` , a larger polygon ` POLYGON((0 0, 10 0, 10 10, 0 0)) ` and ` geog1 ` , a smaller polygon\n` POLYGON((4 2, 6 2, 8 6, 4 2)) ` that intersects with ` geog1 ` . The result is ` geog1 ` with a hole where ` geog2 ` intersects with it.\n\n\nSELECT ST_DIFFERENCE( ST_GEOGFROMTEXT('POLYGON((0 0, 10 0, 10 10, 0 0))'),\nST_GEOGFROMTEXT('POLYGON((4 2, 6 2, 8 6, 4 2))') );\n\n/*--------------------------------------------------------*\n| difference_of_geog1_and_geog2                          |\n+--------------------------------------------------------+\n| POLYGON((0 0, 10 0, 10 10, 0 0), (8 6, 6 2, 4 2, 8 6)) |\n*--------------------------------------------------------*/"
            },
            "ST_DIMENSION": {
                "name": "ST_DIMENSION",
                "summary": "Gets the dimension of the highest-dimensional element in a\n` GEOGRAPHY ` value.",
                "description": "ST_DIMENSION(geography_expression)\n\n**Description**\n\nReturns the dimension of the highest-dimensional element in the input `\nGEOGRAPHY ` .\n\nThe dimension of each possible element is as follows:\n\n* The dimension of a point is ` 0 ` .\n* The dimension of a linestring is ` 1 ` .\n* The dimension of a polygon is ` 2 ` .\n\nIf the input ` GEOGRAPHY ` is empty, ` ST_DIMENSION ` returns ` -1 ` .\n\n**Return type**\n\n` INT64 `"
            },
            "ST_DISJOINT": {
                "name": "ST_DISJOINT",
                "summary": "Checks if two ` GEOGRAPHY ` values are disjoint (do not intersect).",
                "description": "ST_DISJOINT(geography_1, geography_2)\n\n**Description**\n\nReturns ` TRUE ` if the intersection of ` geography_1 ` and ` geography_2 ` is empty, that is, no point in ` geography_1 ` also appears in ` geography_2 ` .\n\n` ST_DISJOINT ` is the logical negation of  ` ST_INTERSECTS ` .\n\n**Return type**\n\n` BOOL `"
            },
            "ST_DISTANCE": {
                "name": "ST_DISTANCE",
                "summary": "Gets the shortest distance in meters between two `\nGEOGRAPHY ` values.",
                "description": "ST_DISTANCE(geography_1, geography_2[, use_spheroid])\n\n**Description**\n\nReturns the shortest distance in meters between two non-empty ` GEOGRAPHY ` s.\n\nIf either of the input ` GEOGRAPHY ` s is empty, ` ST_DISTANCE ` returns `\nNULL ` .\n\nThe optional ` use_spheroid ` parameter determines how this function measures distance. If ` use_spheroid ` is ` FALSE ` , the function measures distance on the surface of a perfect sphere. If ` use_spheroid ` is ` TRUE ` , the function measures distance on the surface of the [ WGS84\n](https://en.wikipedia.org/wiki/World_Geodetic_System) spheroid. The default value of ` use_spheroid ` is ` FALSE ` .\n\n**Return type**\n\n` FLOAT64 `"
            },
            "ST_DUMP": {
                "name": "ST_DUMP",
                "summary": "Returns an array of simple ` GEOGRAPHY ` components in a `\nGEOGRAPHY ` value.",
                "description": "ST_DUMP(geography[, dimension])\n\n**Description**\n\nReturns an ` ARRAY ` of simple ` GEOGRAPHY ` s where each element is a component of the input ` GEOGRAPHY ` . A simple ` GEOGRAPHY ` consists of a single point, linestring, or polygon. If the input ` GEOGRAPHY ` is simple,\nthe result is a single element. When the input ` GEOGRAPHY ` is a collection,\n` ST_DUMP ` returns an ` ARRAY ` with one simple ` GEOGRAPHY ` for each component in the collection.\n\nIf ` dimension ` is provided, the function only returns ` GEOGRAPHY ` s of the corresponding dimension. A dimension of -1 is equivalent to omitting `\ndimension ` .\n\n**Return Type**\n\n` ARRAY<GEOGRAPHY> `\n\n**Examples**\n\nThe following example shows how ` ST_DUMP ` returns the simple geographies within a complex geography.\n\n\nWITH example AS ( SELECT ST_GEOGFROMTEXT('POINT(0 0)') AS geography UNION ALL SELECT ST_GEOGFROMTEXT('MULTIPOINT(0 0, 1 1)') AS geography UNION ALL SELECT ST_GEOGFROMTEXT('GEOMETRYCOLLECTION(POINT(0 0), LINESTRING(1 2, 2 1))')) SELECT geography AS original_geography,\nST_DUMP(geography) AS dumped_geographies FROM example\n\n/*-------------------------------------+------------------------------------*\n|         original_geographies        |      dumped_geographies            |\n+-------------------------------------+------------------------------------+\n| POINT(0 0)                          | [POINT(0 0)]                       |\n| MULTIPOINT(0 0, 1 1)                | [POINT(0 0), POINT(1 1)]           |\n| GEOMETRYCOLLECTION(POINT(0 0),      | [POINT(0 0), LINESTRING(1 2, 2 1)] |\n|   LINESTRING(1 2, 2 1))             |                                    |\n*-------------------------------------+------------------------------------*/\n\nThe following example shows how ` ST_DUMP ` with the dimension argument only returns simple geographies of the given dimension.\n\n\nWITH example AS ( SELECT ST_GEOGFROMTEXT('GEOMETRYCOLLECTION(POINT(0 0), LINESTRING(1 2, 2 1))') AS geography) SELECT geography AS original_geography,\nST_DUMP(geography, 1) AS dumped_geographies FROM example\n\n/*-------------------------------------+------------------------------*\n|         original_geographies        |      dumped_geographies      |\n+-------------------------------------+------------------------------+\n| GEOMETRYCOLLECTION(POINT(0 0),      | [LINESTRING(1 2, 2 1)]       |\n|   LINESTRING(1 2, 2 1))             |                              |\n*-------------------------------------+------------------------------*/"
            },
            "ST_DWITHIN": {
                "name": "ST_DWITHIN",
                "summary": "Checks if any points in two ` GEOGRAPHY ` values are within a given distance.",
                "description": "ST_DWITHIN(geography_1, geography_2, distance[, use_spheroid])\n\n**Description**\n\nReturns ` TRUE ` if the distance between at least one point in ` geography_1 `\nand one point in ` geography_2 ` is less than or equal to the distance given by the ` distance ` argument; otherwise, returns ` FALSE ` . If either input `\nGEOGRAPHY ` is empty, ` ST_DWithin ` returns ` FALSE ` . The given ` distance\n` is in meters on the surface of the Earth.\n\nThe optional ` use_spheroid ` parameter determines how this function measures distance. If ` use_spheroid ` is ` FALSE ` , the function measures distance on the surface of a perfect sphere.\n\nThe ` use_spheroid ` parameter currently only supports the value ` FALSE ` .\nThe default value of ` use_spheroid ` is ` FALSE ` .\n\n**Return type**\n\n` BOOL `"
            },
            "ST_ENDPOINT": {
                "name": "ST_ENDPOINT",
                "summary": "Gets the last point of a linestring ` GEOGRAPHY ` value.",
                "description": "ST_ENDPOINT(linestring_geography)\n\n**Description**\n\nReturns the last point of a linestring geography as a point geography. Returns an error if the input is not a linestring or if the input is empty. Use the `\nSAFE ` prefix to obtain ` NULL ` for invalid input instead of an error.\n\n**Return Type**\n\nPoint ` GEOGRAPHY `\n\n**Example**\n\n\nSELECT ST_ENDPOINT(ST_GEOGFROMTEXT('LINESTRING(1 1, 2 1, 3 2, 3 3)')) last\n\n/*--------------*\n| last         |\n+--------------+\n| POINT(3 3)   |\n*--------------*/"
            },
            "ST_EQUALS": {
                "name": "ST_EQUALS",
                "summary": "Checks if two ` GEOGRAPHY ` values represent the same `\nGEOGRAPHY ` value.",
                "description": "ST_EQUALS(geography_1, geography_2)\n\n**Description**\n\nReturns ` TRUE ` if ` geography_1 ` and ` geography_2 ` represent the same\n\n` GEOGRAPHY ` value. More precisely, this means that one of the following conditions holds: \\+ ` ST_COVERS(geography_1, geography_2) = TRUE ` and `\nST_COVERS(geography_2, geography_1) = TRUE ` \\+ Both ` geography_1 ` and `\ngeography_2 ` are empty.\n\nTherefore, two ` GEOGRAPHY ` s may be equal even if the ordering of points or vertices differ, as long as they still represent the same geometric structure.\n\n**Constraints**\n\n` ST_EQUALS ` is not guaranteed to be a transitive function.\n\n**Return type**\n\n` BOOL `"
            },
            "ST_EXTENT": {
                "name": "ST_EXTENT",
                "summary": "Gets the bounding box for a group of ` GEOGRAPHY ` values.",
                "description": "ST_EXTENT(geography_expression)\n\n**Description**\n\nReturns a ` STRUCT ` that represents the bounding box for the set of input `\nGEOGRAPHY ` values. The bounding box is the minimal rectangle that encloses the geography. The edges of the rectangle follow constant lines of longitude and latitude.\n\nCaveats:\n\n* Returns ` NULL ` if all the inputs are ` NULL ` or empty geographies.\n* The bounding box might cross the antimeridian if this allows for a smaller rectangle. In this case, the bounding box has one of its longitudinal bounds outside of the [-180, 180] range, so that ` xmin ` is smaller than the eastmost value ` xmax ` .\n* If the longitude span of the bounding box is larger than or equal to 180 degrees, the function returns the bounding box with the longitude range of [-180, 180].\n\n**Return type**\n\n` STRUCT<xmin FLOAT64, ymin FLOAT64, xmax FLOAT64, ymax FLOAT64> ` .\n\nBounding box parts:\n\n* ` xmin ` : The westmost constant longitude line that bounds the rectangle.\n* ` xmax ` : The eastmost constant longitude line that bounds the rectangle.\n* ` ymin ` : The minimum constant latitude line that bounds the rectangle.\n* ` ymax ` : The maximum constant latitude line that bounds the rectangle.\n\n**Example**\n\n\nWITH data AS ( SELECT 1 id, ST_GEOGFROMTEXT('POLYGON((-125 48, -124 46, -117 46, -117 49, -125 48))') g UNION ALL SELECT 2 id, ST_GEOGFROMTEXT('POLYGON((172 53, -130 55, -141 70, 172 53))') g UNION ALL SELECT 3 id, ST_GEOGFROMTEXT('POINT EMPTY') g ) SELECT ST_EXTENT(g) AS box FROM data\n\n/*----------------------------------------------*\n| box                                          |\n+----------------------------------------------+\n| {xmin:172, ymin:46, xmax:243, ymax:70}       |\n*----------------------------------------------*/\n\n` ST_BOUNDINGBOX ` for the non-aggregate version of ` ST_EXTENT ` ."
            },
            "ST_EXTERIORRING": {
                "name": "ST_EXTERIORRING",
                "summary": "Returns a linestring ` GEOGRAPHY ` value that corresponds to the outermost ring of a polygon ` GEOGRAPHY ` value.",
                "description": "ST_EXTERIORRING(polygon_geography)\n\n**Description**\n\nReturns a linestring geography that corresponds to the outermost ring of a polygon geography.\n\n* If the input geography is a polygon, gets the outermost ring of the polygon geography and returns the corresponding linestring.\n* If the input is the full ` GEOGRAPHY ` , returns an empty geography.\n* Returns an error if the input is not a single polygon.\n\nUse the ` SAFE ` prefix to return ` NULL ` for invalid input instead of an error.\n\n**Return type**\n\n* Linestring ` GEOGRAPHY `\n* Empty ` GEOGRAPHY `\n\n**Examples**\n\n\nWITH geo as (SELECT ST_GEOGFROMTEXT('POLYGON((0 0, 1 4, 2 2, 0 0))') AS g UNION ALL SELECT ST_GEOGFROMTEXT('''POLYGON((1 1, 1 10, 5 10, 5 1, 1 1),\n(2 2, 3 4, 2 4, 2 2))''') as g) SELECT ST_EXTERIORRING(g) AS ring FROM geo;\n\n/*---------------------------------------*\n| ring                                  |\n+---------------------------------------+\n| LINESTRING(2 2, 1 4, 0 0, 2 2)        |\n| LINESTRING(5 1, 5 10, 1 10, 1 1, 5 1) |\n*---------------------------------------*/"
            },
            "ST_GEOGFROM": {
                "name": "ST_GEOGFROM",
                "summary": "Converts a ` STRING ` or ` BYTES ` value into a ` GEOGRAPHY\n` value.",
                "description": "ST_GEOGFROM(expression)\n\n**Description**\n\nConverts an expression for a ` STRING ` or ` BYTES ` value into a ` GEOGRAPHY\n` value.\n\nIf ` expression ` represents a ` STRING ` value, it must be a valid `\nGEOGRAPHY ` representation in one of the following formats:\n\n* WKT format. To learn more about this format and the requirements to use it, see  ST_GEOGFROMTEXT  .\n* WKB in hexadecimal text format. To learn more about this format and the requirements to use it, see  ST_GEOGFROMWKB  .\n* GeoJSON format. To learn more about this format and the requirements to use it, see  ST_GEOGFROMGEOJSON  .\n\nIf ` expression ` represents a ` BYTES ` value, it must be a valid ` GEOGRAPHY\n` binary expression in WKB format. To learn more about this format and the requirements to use it, see  ST_GEOGFROMWKB  .\n\nIf ` expression ` is ` NULL ` , the output is ` NULL ` .\n\n**Return type**\n\n` GEOGRAPHY `\n\n**Examples**\n\nThis takes a WKT-formatted string and returns a ` GEOGRAPHY ` polygon:\n\n\nSELECT ST_GEOGFROM('POLYGON((0 0, 0 2, 2 2, 2 0, 0 0))') AS WKT_format;\n\n/*------------------------------------*\n| WKT_format                         |\n+------------------------------------+\n| POLYGON((2 0, 2 2, 0 2, 0 0, 2 0)) |\n*------------------------------------*/\n\nThis takes a WKB-formatted hexadecimal-encoded string and returns a `\nGEOGRAPHY ` point:\n\n\nSELECT ST_GEOGFROM(FROM_HEX('010100000000000000000000400000000000001040')) AS WKB_format;\n\n/*----------------*\n| WKB_format     |\n+----------------+\n| POINT(2 4)     |\n*----------------*/\n\nThis takes WKB-formatted bytes and returns a ` GEOGRAPHY ` point:\n\n\nSELECT ST_GEOGFROM('010100000000000000000000400000000000001040') AS WKB_format;\n\n/*----------------*\n| WKB_format     |\n+----------------+\n| POINT(2 4)     |\n*----------------*/\n\nThis takes a GeoJSON-formatted string and returns a ` GEOGRAPHY ` polygon:\n\n\nSELECT ST_GEOGFROM(\n'{ \"type\": \"Polygon\", \"coordinates\": [ [ [2, 0], [2, 2], [1, 2], [0, 2], [0, 0], [2, 0] ] ] }'\n) AS GEOJSON_format;\n\n/*-----------------------------------------*\n| GEOJSON_format                          |\n+-----------------------------------------+\n| POLYGON((2 0, 2 2, 1 2, 0 2, 0 0, 2 0)) |\n*-----------------------------------------*/"
            },
            "ST_GEOGFROMGEOJSON": {
                "name": "ST_GEOGFROMGEOJSON",
                "summary": "Converts a ` STRING ` GeoJSON geometry value into a\n` GEOGRAPHY ` value.",
                "description": "ST_GEOGFROMGEOJSON(geojson_string [, make_valid => constant_expression])\n\n**Description**\n\nReturns a ` GEOGRAPHY ` value that corresponds to the input [ GeoJSON\n](https://en.wikipedia.org/wiki/GeoJSON) representation.\n\n` ST_GEOGFROMGEOJSON ` accepts input that is [ RFC 7946\n](https://tools.ietf.org/html/rfc7946) compliant.\n\nIf the parameter ` make_valid ` is set to ` TRUE ` , the function attempts to repair polygons that don't conform to [ Open Geospatial Consortium\n](https://www.ogc.org/standards/sfa) semantics. This parameter uses named argument syntax, and should be specified using ` make_valid => argument_value\n` syntax.\n\nA GoogleSQL ` GEOGRAPHY ` has spherical geodesic edges, whereas a GeoJSON `\nGeometry ` object explicitly has planar edges. To convert between these two types of edges, GoogleSQL adds additional points to the line where necessary so that the resulting sequence of edges remains within 10 meters of the original edge.\n\nSee  ` ST_ASGEOJSON ` to format a ` GEOGRAPHY ` as GeoJSON.\n\n**Constraints**\n\nThe JSON input is subject to the following constraints:\n\n* ` ST_GEOGFROMGEOJSON ` only accepts JSON geometry fragments and cannot be used to ingest a whole JSON document.\n* The input JSON fragment must consist of a GeoJSON geometry type, which includes ` Point ` , ` MultiPoint ` , ` LineString ` , ` MultiLineString ` , ` Polygon ` , ` MultiPolygon ` , and ` GeometryCollection ` . Any other GeoJSON type such as ` Feature ` or ` FeatureCollection ` will result in an error.\n* A position in the ` coordinates ` member of a GeoJSON geometry type must consist of exactly two elements. The first is the longitude and the second is the latitude. Therefore, ` ST_GEOGFROMGEOJSON ` does not support the optional third element for a position in the ` coordinates ` member.\n\n**Return type**\n\n` GEOGRAPHY `"
            },
            "ST_GEOGFROMTEXT": {
                "name": "ST_GEOGFROMTEXT",
                "summary": "Converts a ` STRING ` WKT geometry value into a `\nGEOGRAPHY ` value.",
                "description": "ST_GEOGFROMTEXT( wkt_string\n[ , oriented => value ]\n[ , planar => value ]\n[ , make_valid => value ]\n)\n\n**Description**\n\nConverts a ` STRING ` [ WKT ](https://en.wikipedia.org/wiki/Well-known_text) geometry value into a ` GEOGRAPHY ` value.\n\nTo format ` GEOGRAPHY ` value as WKT, use  ` ST_ASTEXT ` .\n\n**Definitions**\n\n* ` wkt_string ` : A ` STRING ` value that contains the [ WKT ](https://en.wikipedia.org/wiki/Well-known_text) format.\n* ` oriented ` : A named argument with a ` BOOL ` literal.\n\n* If the value is ` TRUE ` , any polygons in the input are assumed to be oriented as follows: when traveling along the boundary of the polygon in the order of the input vertices, the interior of the polygon is on the left. This allows WKT to represent polygons larger than a hemisphere. See also  ` ST_MAKEPOLYGONORIENTED ` , which is similar to ` ST_GEOGFROMTEXT ` with ` oriented=TRUE ` .\n\n* If the value is ` FALSE ` or omitted, this function returns the polygon with the smaller area.\n\n* ` planar ` : A named argument with a ` BOOL ` literal. If the value is ` TRUE ` , the edges of the linestrings and polygons are assumed to use planar map semantics, rather than GoogleSQL default spherical geodesics semantics. For more information about the differences between spherical geodesics and planar lines, see [ Coordinate systems and edges ](/bigquery/docs/gis-data#coordinate_systems_and_edges) .\n\n* ` make_valid ` : A named argument with a ` BOOL ` literal. If the value is ` TRUE ` , the function attempts to repair polygons that don't conform to [ Open Geospatial Consortium ](https://www.ogc.org/standards/sfa) semantics.\n\n**Details**\n\n* The function does not support three-dimensional geometries that have a ` Z ` suffix, nor does it support linear referencing system geometries with an ` M ` suffix.\n* ` oriented ` and ` planar ` can't be ` TRUE ` at the same time.\n* ` oriented ` and ` make_valid ` can't be ` TRUE ` at the same time.\n\n**Example**\n\nThe following query reads the WKT string ` POLYGON((0 0, 0 2, 2 2, 0 2, 0 0))\n` both as a non-oriented polygon and as an oriented polygon, and checks whether each result contains the point ` (1, 1) ` .\n\n\nWITH polygon AS (SELECT 'POLYGON((0 0, 0 2, 2 2, 2 0, 0 0))' AS p) SELECT ST_CONTAINS(ST_GEOGFROMTEXT(p), ST_GEOGPOINT(1, 1)) AS fromtext_default,\nST_CONTAINS(ST_GEOGFROMTEXT(p, oriented => FALSE), ST_GEOGPOINT(1, 1)) AS non_oriented,\nST_CONTAINS(ST_GEOGFROMTEXT(p, oriented => TRUE),  ST_GEOGPOINT(1, 1)) AS oriented FROM polygon;\n\n/*-------------------+---------------+-----------*\n| fromtext_default  | non_oriented  | oriented  |\n+-------------------+---------------+-----------+\n| TRUE              | TRUE          | FALSE     |\n*-------------------+---------------+-----------*/\n\nThe following query converts a WKT string with an invalid polygon to `\nGEOGRAPHY ` . The WKT string violates two properties of a valid polygon - the loop describing the polygon is not closed, and it contains self-intersection.\nWith the ` make_valid ` option, ` ST_GEOGFROMTEXT ` successfully converts it to a multipolygon shape.\n\n\nWITH data AS ( SELECT 'POLYGON((0 -1, 2 1, 2 -1, 0 1))' wkt) SELECT SAFE.ST_GEOGFROMTEXT(wkt) as geom,\nSAFE.ST_GEOGFROMTEXT(wkt, make_valid => TRUE) as valid_geom FROM data\n\n/*------+-----------------------------------------------------------------*\n| geom | valid_geom                                                      |\n+------+-----------------------------------------------------------------+\n| NULL | MULTIPOLYGON(((0 -1, 1 0, 0 1, 0 -1)), ((1 0, 2 -1, 2 1, 1 0))) |\n*------+-----------------------------------------------------------------*/"
            },
            "ST_GEOGFROMWKB": {
                "name": "ST_GEOGFROMWKB",
                "summary": "Converts a ` BYTES ` or hexadecimal-text ` STRING ` WKT geometry value into a ` GEOGRAPHY ` value.",
                "description": "ST_GEOGFROMWKB( wkb_bytes_expression\n[ , oriented => value ]\n[ , planar => value ]\n[ , make_valid => value ]\n)\n\n\nST_GEOGFROMWKB( wkb_hex_string_expression\n[ , oriented => value ]\n[ , planar => value ]\n[ , make_valid => value ]\n)\n\n**Description**\n\nConverts an expression from a hexadecimal-text ` STRING ` or ` BYTES ` value into a ` GEOGRAPHY ` value. The expression must be in [ WKB\n](https://en.wikipedia.org/wiki/Well-known_text#Well-known_binary) format.\n\nTo format ` GEOGRAPHY ` as WKB, use  ` ST_ASBINARY ` .\n\n**Definitions**\n\n* ` wkb_bytes_expression ` : A ` BYTES ` value that contains the [ WKB ](https://en.wikipedia.org/wiki/Well-known_text#Well-known_binary) format.\n* ` wkb_hex_string_expression ` : A ` STRING ` value that contains the hexadecimal-encoded [ WKB ](https://en.wikipedia.org/wiki/Well-known_text#Well-known_binary) format.\n* ` oriented ` : A named argument with a ` BOOL ` literal.\n\n* If the value is ` TRUE ` , any polygons in the input are assumed to be oriented as follows: when traveling along the boundary of the polygon in the order of the input vertices, the interior of the polygon is on the left. This allows WKB to represent polygons larger than a hemisphere. See also  ` ST_MAKEPOLYGONORIENTED ` , which is similar to ` ST_GEOGFROMWKB ` with ` oriented=TRUE ` .\n\n* If the value is ` FALSE ` or omitted, this function returns the polygon with the smaller area.\n\n* ` planar ` : A named argument with a ` BOOL ` literal. If the value is ` TRUE ` , the edges of the linestrings and polygons are assumed to use planar map semantics, rather than GoogleSQL default spherical geodesics semantics. For more information about the differences between spherical geodesics and planar lines, see [ Coordinate systems and edges ](/bigquery/docs/gis-data#coordinate_systems_and_edges) .\n\n* ` make_valid ` : A named argument with a ` BOOL ` literal. If the value is ` TRUE ` , the function attempts to repair polygons that don't conform to [ Open Geospatial Consortium ](https://www.ogc.org/standards/sfa) semantics.\n\n**Details**\n\n* The function does not support three-dimensional geometries that have a ` Z ` suffix, nor does it support linear referencing system geometries with an ` M ` suffix.\n* ` oriented ` and ` planar ` can't be ` TRUE ` at the same time.\n* ` oriented ` and ` make_valid ` can't be ` TRUE ` at the same time.\n\n**Return type**\n\n` GEOGRAPHY `\n\n**Example**\n\nThe following query reads the hex-encoded WKB data containing ` LINESTRING(1 1, 3 2) ` and uses it with planar and geodesic semantics. When planar is used,\nthe function approximates the planar input line using line that contains a chain of geodesic segments.\n\n\nWITH wkb_data AS ( SELECT '010200000002000000feffffffffffef3f000000000000f03f01000000000008400000000000000040' geo ) SELECT ST_GeogFromWkb(geo, planar=>TRUE) AS from_planar,\nST_GeogFromWkb(geo, planar=>FALSE) AS from_geodesic,\nFROM wkb_data\n\n/*---------------------------------------+----------------------*\n| from_planar                           | from_geodesic        |\n+---------------------------------------+----------------------+\n| LINESTRING(1 1, 2 1.5, 2.5 1.75, 3 2) | LINESTRING(1 1, 3 2) |\n*---------------------------------------+----------------------*/"
            },
            "ST_GEOGPOINT": {
                "name": "ST_GEOGPOINT",
                "summary": "Creates a point ` GEOGRAPHY ` value for a given longitude and latitude.",
                "description": "ST_GEOGPOINT(longitude, latitude)\n\n**Description**\n\nCreates a ` GEOGRAPHY ` with a single point. ` ST_GEOGPOINT ` creates a point from the specified ` FLOAT64 ` longitude (in degrees, negative west of the Prime Meridian, positive east) and latitude (in degrees, positive north of the Equator, negative south) parameters and returns that point in a ` GEOGRAPHY `\nvalue.\n\nNOTE: Some systems present latitude first; take care with argument order.\n\n**Constraints**\n\n* Longitudes outside the range [-180, 180] are allowed; ` ST_GEOGPOINT ` uses the input longitude modulo 360 to obtain a longitude within [-180, 180].\n* Latitudes must be in the range [-90, 90]. Latitudes outside this range will result in an error.\n\n**Return type**\n\nPoint ` GEOGRAPHY `"
            },
            "ST_GEOGPOINTFROMGEOHASH": {
                "name": "ST_GEOGPOINTFROMGEOHASH",
                "summary": "Gets a point ` GEOGRAPHY ` value that is in the middle of a bounding box defined in a ` STRING ` GeoHash value.",
                "description": "ST_GEOGPOINTFROMGEOHASH(geohash)\n\n**Description**\n\nReturns a ` GEOGRAPHY ` value that corresponds to a point in the middle of a bounding box defined in the [ GeoHash ](https://en.wikipedia.org/wiki/Geohash) .\n\n**Return type**\n\nPoint ` GEOGRAPHY `"
            },
            "ST_GEOHASH": {
                "name": "ST_GEOHASH",
                "summary": "Converts a point ` GEOGRAPHY ` value to a ` STRING ` GeoHash value.",
                "description": "ST_GEOHASH(geography_expression[, maxchars])\n\n**Description**\n\nTakes a single-point ` GEOGRAPHY ` and returns a [ GeoHash\n](https://en.wikipedia.org/wiki/Geohash) representation of that ` GEOGRAPHY `\nobject.\n\n* ` geography_expression ` : Represents a ` GEOGRAPHY ` object. Only a ` GEOGRAPHY ` object that represents a single point is supported. If ` ST_GEOHASH ` is used over an empty ` GEOGRAPHY ` object, returns ` NULL ` .\n* ` maxchars ` : This optional ` INT64 ` parameter specifies the maximum number of characters the hash will contain. Fewer characters corresponds to lower precision (or, described differently, to a bigger bounding box). ` maxchars ` defaults to 20 if not explicitly specified. A valid ` maxchars ` value is 1 to 20. Any value below or above is considered unspecified and the default of 20 is used.\n\n**Return type**\n\n` STRING `\n\n**Example**\n\nReturns a GeoHash of the Seattle Center with 10 characters of precision.\n\n\nSELECT ST_GEOHASH(ST_GEOGPOINT(-122.35, 47.62), 10) geohash\n\n/*--------------*\n| geohash      |\n+--------------+\n| c22yzugqw7   |\n*--------------*/"
            },
            "ST_GEOMETRYTYPE": {
                "name": "ST_GEOMETRYTYPE",
                "summary": "Gets the Open Geospatial Consortium (OGC) geometry type for a ` GEOGRAPHY ` value.",
                "description": "ST_GEOMETRYTYPE(geography_expression)\n\n**Description**\n\nReturns the [ Open Geospatial Consortium ](https://www.ogc.org/standards/sfa) (OGC) geometry type that describes the input ` GEOGRAPHY ` . The OGC geometry type matches the types that are used in [ WKT\n](https://en.wikipedia.org/wiki/Well-known_text) and [ GeoJSON\n](https://en.wikipedia.org/wiki/GeoJSON) formats and printed for  ST_ASTEXT and  ST_ASGEOJSON  . ` ST_GEOMETRYTYPE ` returns the OGC geometry type with the \"ST_\" prefix.\n\n` ST_GEOMETRYTYPE ` returns the following given the type on the input:\n\n* Single point geography: Returns ` ST_Point ` .\n* Collection of only points: Returns ` ST_MultiPoint ` .\n* Single linestring geography: Returns ` ST_LineString ` .\n* Collection of only linestrings: Returns ` ST_MultiLineString ` .\n* Single polygon geography: Returns ` ST_Polygon ` .\n* Collection of only polygons: Returns ` ST_MultiPolygon ` .\n* Collection with elements of different dimensions, or the input is the empty geography: Returns ` ST_GeometryCollection ` .\n\n**Return type**\n\n` STRING `\n\n**Example**\n\nThe following example shows how ` ST_GEOMETRYTYPE ` takes geographies and returns the names of their OGC geometry types.\n\n\nWITH example AS( SELECT ST_GEOGFROMTEXT('POINT(0 1)') AS geography UNION ALL SELECT ST_GEOGFROMTEXT('MULTILINESTRING((2 2, 3 4), (5 6, 7 7))') UNION ALL SELECT ST_GEOGFROMTEXT('GEOMETRYCOLLECTION(MULTIPOINT(-1 2, 0 12), LINESTRING(-2 4, 0 6))') UNION ALL SELECT ST_GEOGFROMTEXT('GEOMETRYCOLLECTION EMPTY')) SELECT geography AS WKT,\nST_GEOMETRYTYPE(geography) AS geometry_type_name FROM example;\n\n/*-------------------------------------------------------------------+-----------------------*\n| WKT                                                               | geometry_type_name    |\n+-------------------------------------------------------------------+-----------------------+\n| POINT(0 1)                                                        | ST_Point              |\n| MULTILINESTRING((2 2, 3 4), (5 6, 7 7))                           | ST_MultiLineString    |\n| GEOMETRYCOLLECTION(MULTIPOINT(-1 2, 0 12), LINESTRING(-2 4, 0 6)) | ST_GeometryCollection |\n| GEOMETRYCOLLECTION EMPTY                                          | ST_GeometryCollection |\n*-------------------------------------------------------------------+-----------------------*/"
            },
            "ST_HAUSDORFFDISTANCE": {
                "name": "ST_HAUSDORFFDISTANCE",
                "summary": "Gets the discrete Hausdorff distance between two geometries.",
                "description": "ST_HAUSDORFFDISTANCE(geography_1, geography_2)\n\n\nST_HAUSDORFFDISTANCE(geography_1, geography_2, directed=>{ TRUE | FALSE })\n\n**Description**\n\nGets the discrete [ Hausdorff distance\n](http://en.wikipedia.org/wiki/Hausdorff_distance) , which is the greatest of all the distances from a discrete point in one geography to the closest discrete point in another geography.\n\n**Definitions**\n\n* ` geography_1 ` : A ` GEOGRAPHY ` value that represents the first geography.\n* ` geography_2 ` : A ` GEOGRAPHY ` value that represents the second geography.\n* ` directed ` : Optional, required named argument that represents the type of computation to use on the input geographies. If this argument is not specified, ` directed=>FALSE ` is used by default.\n\n* ` FALSE ` : The largest Hausdorff distance found in ( ` geography_1 ` , ` geography_2 ` ) and ( ` geography_2 ` , ` geography_1 ` ).\n\n* ` TRUE ` (default): The Hausdorff distance for ( ` geography_1 ` , ` geography_2 ` ).\n\n**Details**\n\nIf an input geography is ` NULL ` , the function returns ` NULL ` .\n\n**Return type**\n\n` FLOAT64 `\n\n**Example**\n\nThe following query gets the Hausdorff distance between ` geo1 ` and ` geo2 `\n:\n\n\nWITH data AS ( SELECT ST_GEOGFROMTEXT('LINESTRING(20 70, 70 60, 10 70, 70 70)') AS geo1,\nST_GEOGFROMTEXT('LINESTRING(20 90, 30 90, 60 10, 90 10)') AS geo2 ) SELECT ST_HAUSDORFFDISTANCE(geo1, geo2, directed=>TRUE) AS distance FROM data;\n\n/*--------------------+\n| distance           |\n+--------------------+\n| 1688933.9832041925 |\n+--------------------*/\n\nThe following query gets the Hausdorff distance between ` geo2 ` and ` geo1 `\n:\n\n\nWITH data AS ( SELECT ST_GEOGFROMTEXT('LINESTRING(20 70, 70 60, 10 70, 70 70)') AS geo1,\nST_GEOGFROMTEXT('LINESTRING(20 90, 30 90, 60 10, 90 10)') AS geo2 ) SELECT ST_HAUSDORFFDISTANCE(geo2, geo1, directed=>TRUE) AS distance FROM data;\n\n/*--------------------+\n| distance           |\n+--------------------+\n| 5802892.745488612  |\n+--------------------*/\n\nThe following query gets the largest Hausdorff distance between ( ` geo1 ` and\n` geo2 ` ) and ( ` geo2 ` and ` geo1 ` ):\n\n\nWITH data AS ( SELECT ST_GEOGFROMTEXT('LINESTRING(20 70, 70 60, 10 70, 70 70)') AS geo1,\nST_GEOGFROMTEXT('LINESTRING(20 90, 30 90, 60 10, 90 10)') AS geo2 ) SELECT ST_HAUSDORFFDISTANCE(geo1, geo2, directed=>FALSE) AS distance FROM data;\n\n/*--------------------+\n| distance           |\n+--------------------+\n| 5802892.745488612  |\n+--------------------*/\n\nThe following query produces the same results as the previous query because `\nST_HAUSDORFFDISTANCE ` uses ` directed=>FALSE ` by default.\n\n\nWITH data AS ( SELECT ST_GEOGFROMTEXT('LINESTRING(20 70, 70 60, 10 70, 70 70)') AS geo1,\nST_GEOGFROMTEXT('LINESTRING(20 90, 30 90, 60 10, 90 10)') AS geo2 ) SELECT ST_HAUSDORFFDISTANCE(geo1, geo2) AS distance FROM data;"
            },
            "ST_INTERIORRINGS": {
                "name": "ST_INTERIORRINGS",
                "summary": "Gets the interior rings of a polygon ` GEOGRAPHY `\nvalue.",
                "description": "ST_INTERIORRINGS(polygon_geography)\n\n**Description**\n\nReturns an array of linestring geographies that corresponds to the interior rings of a polygon geography. Each interior ring is the border of a hole within the input polygon.\n\n* If the input geography is a polygon, excludes the outermost ring of the polygon geography and returns the linestrings corresponding to the interior rings.\n* If the input is the full ` GEOGRAPHY ` , returns an empty array.\n* If the input polygon has no holes, returns an empty array.\n* Returns an error if the input is not a single polygon.\n\nUse the ` SAFE ` prefix to return ` NULL ` for invalid input instead of an error.\n\n**Return type**\n\n` ARRAY<LineString GEOGRAPHY> `\n\n**Examples**\n\n\nWITH geo AS ( SELECT ST_GEOGFROMTEXT('POLYGON((0 0, 1 1, 1 2, 0 0))') AS g UNION ALL SELECT ST_GEOGFROMTEXT('POLYGON((1 1, 1 10, 5 10, 5 1, 1 1), (2 2, 3 4, 2 4, 2 2))') UNION ALL SELECT ST_GEOGFROMTEXT('POLYGON((1 1, 1 10, 5 10, 5 1, 1 1), (2 2.5, 3.5 3, 2.5 2, 2 2.5), (3.5 7, 4 6, 3 3, 3.5 7))') UNION ALL SELECT ST_GEOGFROMTEXT('fullglobe') UNION ALL SELECT NULL) SELECT ST_INTERIORRINGS(g) AS rings FROM geo;\n\n/*----------------------------------------------------------------------------*\n| rings                                                                      |\n+----------------------------------------------------------------------------+\n| []                                                                         |\n| [LINESTRING(2 2, 3 4, 2 4, 2 2)]                                           |\n| [LINESTRING(2.5 2, 3.5 3, 2 2.5, 2.5 2), LINESTRING(3 3, 4 6, 3.5 7, 3 3)] |\n| []                                                                         |\n| NULL                                                                       |\n*----------------------------------------------------------------------------*/"
            },
            "ST_INTERSECTION": {
                "name": "ST_INTERSECTION",
                "summary": "Gets the point set intersection of two ` GEOGRAPHY `\nvalues.",
                "description": "ST_INTERSECTION(geography_1, geography_2)\n\n**Description**\n\nReturns a ` GEOGRAPHY ` that represents the point set intersection of the two input ` GEOGRAPHY ` s. Thus, every point in the intersection appears in both `\ngeography_1 ` and ` geography_2 ` .\n\nIf the two input ` GEOGRAPHY ` s are disjoint, that is, there are no points that appear in both input ` geometry_1 ` and ` geometry_2 ` , then an empty `\nGEOGRAPHY ` is returned.\n\nSee  ST_INTERSECTS  ,  ST_DISJOINT  for related predicate functions.\n\n**Return type**\n\n` GEOGRAPHY `"
            },
            "ST_INTERSECTS": {
                "name": "ST_INTERSECTS",
                "summary": "Checks if at least one point appears in two ` GEOGRAPHY `\nvalues.",
                "description": "ST_INTERSECTS(geography_1, geography_2)\n\n**Description**\n\nReturns ` TRUE ` if the point set intersection of ` geography_1 ` and `\ngeography_2 ` is non-empty. Thus, this function returns ` TRUE ` if there is at least one point that appears in both input ` GEOGRAPHY ` s.\n\nIf ` ST_INTERSECTS ` returns ` TRUE ` , it implies that  ` ST_DISJOINT `\nreturns ` FALSE ` .\n\n**Return type**\n\n` BOOL `"
            },
            "ST_INTERSECTSBOX": {
                "name": "ST_INTERSECTSBOX",
                "summary": "Checks if a ` GEOGRAPHY ` value intersects a rectangle.",
                "description": "ST_INTERSECTSBOX(geography, lng1, lat1, lng2, lat2)\n\n**Description**\n\nReturns ` TRUE ` if ` geography ` intersects the rectangle between ` [lng1,\nlng2] ` and ` [lat1, lat2] ` . The edges of the rectangle follow constant lines of longitude and latitude. ` lng1 ` and ` lng2 ` specify the westmost and eastmost constant longitude lines that bound the rectangle, and ` lat1 `\nand ` lat2 ` specify the minimum and maximum constant latitude lines that bound the rectangle.\n\nSpecify all longitude and latitude arguments in degrees.\n\n**Constraints**\n\nThe input arguments are subject to the following constraints:\n\n* Latitudes should be in the ` [-90, 90] ` degree range.\n* Longitudes should follow either of the following rules:\n* Both longitudes are in the ` [-180, 180] ` degree range.\n* One of the longitudes is in the ` [-180, 180] ` degree range, and ` lng2 - lng1 ` is in the ` [0, 360] ` interval.\n\n**Return type**\n\n` BOOL `\n\n**Example**\n\n\nSELECT p, ST_INTERSECTSBOX(p, -90, 0, 90, 20) AS box1,\nST_INTERSECTSBOX(p, 90, 0, -90, 20) AS box2 FROM UNNEST([ST_GEOGPOINT(10, 10), ST_GEOGPOINT(170, 10),\nST_GEOGPOINT(30, 30)]) p\n\n/*----------------+--------------+--------------*\n| p              | box1         | box2         |\n+----------------+--------------+--------------+\n| POINT(10 10)   | TRUE         | FALSE        |\n| POINT(170 10)  | FALSE        | TRUE         |\n| POINT(30 30)   | FALSE        | FALSE        |\n*----------------+--------------+--------------*/"
            },
            "ST_ISCLOSED": {
                "name": "ST_ISCLOSED",
                "summary": "Checks if all components in a ` GEOGRAPHY ` value are closed.",
                "description": "ST_ISCLOSED(geography_expression)\n\n**Description**\n\nReturns ` TRUE ` for a non-empty Geography, where each element in the Geography has an empty boundary. The boundary for each element can be defined with  ` ST_BOUNDARY ` .\n\n* A point is closed.\n* A linestring is closed if the start and end points of the linestring are the same.\n* A polygon is closed only if it is a full polygon.\n* A collection is closed if and only if every element in the collection is closed.\n\nAn empty ` GEOGRAPHY ` is not closed.\n\n**Return type**\n\n` BOOL `\n\n**Example**\n\n\nWITH example AS( SELECT ST_GEOGFROMTEXT('POINT(5 0)') AS geography UNION ALL SELECT ST_GEOGFROMTEXT('LINESTRING(0 1, 4 3, 2 6, 0 1)') AS geography UNION ALL SELECT ST_GEOGFROMTEXT('LINESTRING(2 6, 1 3, 3 9)') AS geography UNION ALL SELECT ST_GEOGFROMTEXT('GEOMETRYCOLLECTION(POINT(0 0), LINESTRING(1 2, 2 1))') AS geography UNION ALL SELECT ST_GEOGFROMTEXT('GEOMETRYCOLLECTION EMPTY')) SELECT geography,\nST_ISCLOSED(geography) AS is_closed,\nFROM example;\n\n/*------------------------------------------------------+-----------*\n| geography                                            | is_closed |\n+------------------------------------------------------+-----------+\n| POINT(5 0)                                           | TRUE      |\n| LINESTRING(0 1, 4 3, 2 6, 0 1)                       | TRUE      |\n| LINESTRING(2 6, 1 3, 3 9)                            | FALSE     |\n| GEOMETRYCOLLECTION(POINT(0 0), LINESTRING(1 2, 2 1)) | FALSE     |\n| GEOMETRYCOLLECTION EMPTY                             | FALSE     |\n*------------------------------------------------------+-----------*/"
            },
            "ST_ISCOLLECTION": {
                "name": "ST_ISCOLLECTION",
                "summary": "Checks if the total number of points, linestrings, and polygons is greater than one in a ` GEOGRAPHY ` value.",
                "description": "ST_ISCOLLECTION(geography_expression)\n\n**Description**\n\nReturns ` TRUE ` if the total number of points, linestrings, and polygons is greater than one.\n\nAn empty ` GEOGRAPHY ` is not a collection.\n\n**Return type**\n\n` BOOL `"
            },
            "ST_ISEMPTY": {
                "name": "ST_ISEMPTY",
                "summary": "Checks if a ` GEOGRAPHY ` value is empty.",
                "description": "ST_ISEMPTY(geography_expression)\n\n**Description**\n\nReturns ` TRUE ` if the given ` GEOGRAPHY ` is empty; that is, the ` GEOGRAPHY\n` does not contain any points, lines, or polygons.\n\nNOTE: An empty ` GEOGRAPHY ` is not associated with a particular geometry shape. For example, the results of expressions ` ST_GEOGFROMTEXT('POINT EMPTY') ` and ` ST_GEOGFROMTEXT('GEOMETRYCOLLECTION EMPTY') ` are identical.\n\n**Return type**\n\n` BOOL `"
            },
            "ST_ISRING": {
                "name": "ST_ISRING",
                "summary": "Checks if a ` GEOGRAPHY ` value is a closed, simple linestring.",
                "description": "ST_ISRING(geography_expression)\n\n**Description**\n\nReturns ` TRUE ` if the input ` GEOGRAPHY ` is a linestring and if the linestring is both  ` ST_ISCLOSED ` and simple. A linestring is considered simple if it does not pass through the same point twice (with the exception of the start and endpoint, which may overlap to form a ring).\n\nAn empty ` GEOGRAPHY ` is not a ring.\n\n**Return type**\n\n` BOOL `"
            },
            "ST_LENGTH": {
                "name": "ST_LENGTH",
                "summary": "Gets the total length of lines in a ` GEOGRAPHY ` value.",
                "description": "ST_LENGTH(geography_expression[, use_spheroid])\n\n**Description**\n\nReturns the total length in meters of the lines in the input ` GEOGRAPHY ` .\n\nIf ` geography_expression ` is a point or a polygon, returns zero. If `\ngeography_expression ` is a collection, returns the length of the lines in the collection; if the collection does not contain lines, returns zero.\n\nThe optional ` use_spheroid ` parameter determines how this function measures distance. If ` use_spheroid ` is ` FALSE ` , the function measures distance on the surface of a perfect sphere.\n\nThe ` use_spheroid ` parameter currently only supports the value ` FALSE ` .\nThe default value of ` use_spheroid ` is ` FALSE ` .\n\n**Return type**\n\n` FLOAT64 `"
            },
            "ST_LINEINTERPOLATEPOINT": {
                "name": "ST_LINEINTERPOLATEPOINT",
                "summary": "Gets a point at a specific fraction in a linestring ` GEOGRAPHY ` value.",
                "description": "ST_LINEINTERPOLATEPOINT(linestring_geography, fraction)\n\n**Description**\n\nGets a point at a specific fraction in a linestring ` GEOGRAPHY ` value.\n\n**Definitions**\n\n* ` linestring_geography ` : A linestring ` GEOGRAPHY ` on which the target point is located.\n* ` fraction ` : A ` FLOAT64 ` value that represents a fraction along the linestring ` GEOGRAPHY ` where the target point is located. This should be an inclusive value between ` 0 ` (start of the linestring) and ` 1 ` (end of the linestring).\n\n**Details**\n\n* Returns ` NULL ` if any input argument is ` NULL ` .\n* Returns an empty geography if ` linestring_geography ` is an empty geography.\n* Returns an error if ` linestring_geography ` is not a linestring or an empty geography, or if ` fraction ` is outside the ` [0, 1] ` range.\n\n**Return Type**\n\n` GEOGRAPHY `\n\n**Example**\n\nThe following query returns a few points on a linestring. Notice that the midpoint of the linestring ` LINESTRING(1 1, 5 5) ` is slightly different from\n` POINT(3 3) ` because the ` GEOGRAPHY ` type uses geodesic line segments.\n\n\nWITH fractions AS ( SELECT 0 AS fraction UNION ALL SELECT 0.5 UNION ALL SELECT 1 UNION ALL SELECT NULL ) SELECT fraction,\nST_LINEINTERPOLATEPOINT(ST_GEOGFROMTEXT('LINESTRING(1 1, 5 5)'), fraction) AS point FROM fractions\n\n/*-------------+-------------------------------------------*\n| fraction    | point                                     |\n+-------------+-------------------------------------------+\n| 0           | POINT(1 1)                                |\n| 0.5         | POINT(2.99633827268976 3.00182528336078)  |\n| 1           | POINT(5 5)                                |\n| NULL        | NULL                                      |\n*-------------+-------------------------------------------*/"
            },
            "ST_LINELOCATEPOINT": {
                "name": "ST_LINELOCATEPOINT",
                "summary": "Gets a section of a linestring ` GEOGRAPHY ` value between the start point and a point ` GEOGRAPHY ` value.",
                "description": "ST_LINELOCATEPOINT(linestring_geography, point_geography)\n\n**Description**\n\nGets a section of a linestring between the start point and a selected point (a point on the linestring closest to the ` point_geography ` argument). Returns the percentage that this section represents in the linestring.\n\nDetails:\n\n* To select a point on the linestring ` GEOGRAPHY ` ( ` linestring_geography ` ), this function takes a point ` GEOGRAPHY ` ( ` point_geography ` ) and finds the  closest point  to it on the linestring.\n* If two points on ` linestring_geography ` are an equal distance away from ` point_geography ` , it is not guaranteed which one will be selected.\n* The return value is an inclusive value between 0 and 1 (0-100%).\n* If the selected point is the start point on the linestring, function returns 0 (0%).\n* If the selected point is the end point on the linestring, function returns 1 (100%).\n\n` NULL ` and error handling:\n\n* Returns ` NULL ` if any input argument is ` NULL ` .\n* Returns an error if ` linestring_geography ` is not a linestring or if ` point_geography ` is not a point. Use the ` SAFE ` prefix to obtain ` NULL ` for invalid input instead of an error.\n\n**Return Type**\n\n` FLOAT64 `\n\n**Examples**\n\n\nWITH geos AS ( SELECT ST_GEOGPOINT(0, 0) AS point UNION ALL SELECT ST_GEOGPOINT(1, 0) UNION ALL SELECT ST_GEOGPOINT(1, 1) UNION ALL SELECT ST_GEOGPOINT(2, 2) UNION ALL SELECT ST_GEOGPOINT(3, 3) UNION ALL SELECT ST_GEOGPOINT(4, 4) UNION ALL SELECT ST_GEOGPOINT(5, 5) UNION ALL SELECT ST_GEOGPOINT(6, 5) UNION ALL SELECT NULL ) SELECT point AS input_point,\nST_LINELOCATEPOINT(ST_GEOGFROMTEXT('LINESTRING(1 1, 5 5)'), point) AS percentage_from_beginning FROM geos\n\n/*-------------+---------------------------*\n| input_point | percentage_from_beginning |\n+-------------+---------------------------+\n| POINT(0 0)  | 0                         |\n| POINT(1 0)  | 0                         |\n| POINT(1 1)  | 0                         |\n| POINT(2 2)  | 0.25015214685147907       |\n| POINT(3 3)  | 0.5002284283637185        |\n| POINT(4 4)  | 0.7501905913884388        |\n| POINT(5 5)  | 1                         |\n| POINT(6 5)  | 1                         |\n| NULL        | NULL                      |\n*-------------+---------------------------*/"
            },
            "ST_LINESUBSTRING": {
                "name": "ST_LINESUBSTRING",
                "summary": "Gets a segment of a single linestring at a specific starting and ending fraction.",
                "description": "ST_LINESUBSTRING(linestring_geography, start_fraction, end_fraction);\n\n**Description**\n\nGets a segment of a linestring at a specific starting and ending fraction.\n\n**Definitions**\n\n* ` linestring_geography ` : The LineString ` GEOGRAPHY ` value that represents the linestring from which to extract a segment.\n* ` start_fraction ` : ` FLOAT64 ` value that represents the starting fraction of the total length of ` linestring_geography ` . This must be an inclusive value between 0 and 1 (0-100%).\n* ` end_fraction ` : ` FLOAT64 ` value that represents the ending fraction of the total length of ` linestring_geography ` . This must be an inclusive value between 0 and 1 (0-100%).\n\n**Details**\n\n` end_fraction ` must be greater than or equal to ` start_fraction ` .\n\nIf ` start_fraction ` and ` end_fraction ` are equal, a linestring with only one point is produced.\n\n**Return type**\n\n* LineString ` GEOGRAPHY ` if the resulting geography has more than one point.\n* Point ` GEOGRAPHY ` if the resulting geography has only one point.\n\n**Example**\n\nThe following query returns the second half of the linestring:\n\n\nWITH data AS ( SELECT ST_GEOGFROMTEXT('LINESTRING(20 70, 70 60, 10 70, 70 70)') AS geo1 ) SELECT ST_LINESUBSTRING(geo1, 0.5, 1) AS segment FROM data;\n\n/*-------------------------------------------------------------+\n| segment                                                     |\n+-------------------------------------------------------------+\n| LINESTRING(49.4760661523471 67.2419539103851, 10 70, 70 70) |\n+-------------------------------------------------------------*/\n\nThe following query returns a linestring that only contains one point:\n\n\nWITH data AS ( SELECT ST_GEOGFROMTEXT('LINESTRING(20 70, 70 60, 10 70, 70 70)') AS geo1 ) SELECT ST_LINESUBSTRING(geo1, 0.5, 0.5) AS segment FROM data;\n\n/*------------------------------------------+\n| segment                                  |\n+------------------------------------------+\n| POINT(49.4760661523471 67.2419539103851) |\n+------------------------------------------*/"
            },
            "ST_MAKELINE": {
                "name": "ST_MAKELINE",
                "summary": "Creates a linestring ` GEOGRAPHY ` value by concatenating the point and linestring vertices of ` GEOGRAPHY ` values.",
                "description": "ST_MAKELINE(geography_1, geography_2)\n\n\nST_MAKELINE(array_of_geography)\n\n**Description**\n\nCreates a ` GEOGRAPHY ` with a single linestring by concatenating the point or line vertices of each of the input ` GEOGRAPHY ` s in the order they are given.\n\n` ST_MAKELINE ` comes in two variants. For the first variant, input must be two ` GEOGRAPHY ` s. For the second, input must be an ` ARRAY ` of type `\nGEOGRAPHY ` . In either variant, each input ` GEOGRAPHY ` must consist of one of the following values:\n\n* Exactly one point.\n* Exactly one linestring.\n\nFor the first variant of ` ST_MAKELINE ` , if either input ` GEOGRAPHY ` is `\nNULL ` , ` ST_MAKELINE ` returns ` NULL ` . For the second variant, if input `\nARRAY ` or any element in the input ` ARRAY ` is ` NULL ` , ` ST_MAKELINE `\nreturns ` NULL ` .\n\n**Constraints**\n\nEvery edge must span strictly less than 180 degrees.\n\nNOTE: The GoogleSQL snapping process may discard sufficiently short edges and snap the two endpoints together. For instance, if two input ` GEOGRAPHY ` s each contain a point and the two points are separated by a distance less than the snap radius, the points will be snapped together. In such a case the result will be a ` GEOGRAPHY ` with exactly one point.\n\n**Return type**\n\nLineString ` GEOGRAPHY `"
            },
            "ST_MAKEPOLYGON": {
                "name": "ST_MAKEPOLYGON",
                "summary": "Constructs a polygon ` GEOGRAPHY ` value by combining a polygon shell with polygon holes.",
                "description": "ST_MAKEPOLYGON(polygon_shell[, array_of_polygon_holes])\n\n**Description**\n\nCreates a ` GEOGRAPHY ` containing a single polygon from linestring inputs,\nwhere each input linestring is used to construct a polygon ring.\n\n` ST_MAKEPOLYGON ` comes in two variants. For the first variant, the input linestring is provided by a single ` GEOGRAPHY ` containing exactly one linestring. For the second variant, the input consists of a single ` GEOGRAPHY\n` and an array of ` GEOGRAPHY ` s, each containing exactly one linestring.\n\nThe first ` GEOGRAPHY ` in either variant is used to construct the polygon shell. Additional ` GEOGRAPHY ` s provided in the input ` ARRAY ` specify a polygon hole. For every input ` GEOGRAPHY ` containing exactly one linestring,\nthe following must be true:\n\n* The linestring must consist of at least three distinct vertices.\n* The linestring must be closed: that is, the first and last vertex have to be the same. If the first and last vertex differ, the function constructs a final edge from the first vertex to the last.\n\nFor the first variant of ` ST_MAKEPOLYGON ` , if either input ` GEOGRAPHY ` is\n` NULL ` , ` ST_MAKEPOLYGON ` returns ` NULL ` . For the second variant, if input ` ARRAY ` or any element in the ` ARRAY ` is ` NULL ` , ` ST_MAKEPOLYGON\n` returns ` NULL ` .\n\nNOTE: ` ST_MAKEPOLYGON ` accepts an empty ` GEOGRAPHY ` as input. `\nST_MAKEPOLYGON ` interprets an empty ` GEOGRAPHY ` as having an empty linestring, which will create a full loop: that is, a polygon that covers the entire Earth.\n\n**Constraints**\n\nTogether, the input rings must form a valid polygon:\n\n* The polygon shell must cover each of the polygon holes.\n* There can be only one polygon shell (which has to be the first input ring). This implies that polygon holes cannot be nested.\n* Polygon rings may only intersect in a vertex on the boundary of both rings.\n\nEvery edge must span strictly less than 180 degrees.\n\nEach polygon ring divides the sphere into two regions. The first input linesting to ` ST_MAKEPOLYGON ` forms the polygon shell, and the interior is chosen to be the smaller of the two regions. Each subsequent input linestring specifies a polygon hole, so the interior of the polygon is already well-\ndefined. In order to define a polygon shell such that the interior of the polygon is the larger of the two regions, see  ` ST_MAKEPOLYGONORIENTED ` .\n\nNOTE: The GoogleSQL snapping process may discard sufficiently short edges and snap the two endpoints together. Hence, when vertices are snapped together, it is possible that a polygon hole that is sufficiently small may disappear, or the output ` GEOGRAPHY ` may contain only a line or a point.\n\n**Return type**\n\n` GEOGRAPHY `"
            },
            "ST_MAKEPOLYGONORIENTED": {
                "name": "ST_MAKEPOLYGONORIENTED",
                "summary": "Constructs a polygon ` GEOGRAPHY ` value, using an array of linestring ` GEOGRAPHY ` values. The vertex ordering of each linestring determines the orientation of each polygon ring.",
                "description": "ST_MAKEPOLYGONORIENTED(array_of_geography)\n\n**Description**\n\nLike ` ST_MAKEPOLYGON ` , but the vertex ordering of each input linestring determines the orientation of each polygon ring. The orientation of a polygon ring defines the interior of the polygon as follows: if someone walks along the boundary of the polygon in the order of the input vertices, the interior of the polygon is on the left. This applies for each polygon ring provided.\n\nThis variant of the polygon constructor is more flexible since `\nST_MAKEPOLYGONORIENTED ` can construct a polygon such that the interior is on either side of the polygon ring. However, proper orientation of polygon rings is critical in order to construct the desired polygon.\n\nIf the input ` ARRAY ` or any element in the ` ARRAY ` is ` NULL ` , `\nST_MAKEPOLYGONORIENTED ` returns ` NULL ` .\n\nNOTE: The input argument for ` ST_MAKEPOLYGONORIENTED ` may contain an empty `\nGEOGRAPHY ` . ` ST_MAKEPOLYGONORIENTED ` interprets an empty ` GEOGRAPHY ` as having an empty linestring, which will create a full loop: that is, a polygon that covers the entire Earth.\n\n**Constraints**\n\nTogether, the input rings must form a valid polygon:\n\n* The polygon shell must cover each of the polygon holes.\n* There must be only one polygon shell, which must to be the first input ring. This implies that polygon holes cannot be nested.\n* Polygon rings may only intersect in a vertex on the boundary of both rings.\n\nEvery edge must span strictly less than 180 degrees.\n\n` ST_MAKEPOLYGONORIENTED ` relies on the ordering of the input vertices of each linestring to determine the orientation of the polygon. This applies to the polygon shell and any polygon holes. ` ST_MAKEPOLYGONORIENTED ` expects all polygon holes to have the opposite orientation of the shell. See  `\nST_MAKEPOLYGON ` for an alternate polygon constructor, and other constraints on building a valid polygon.\n\nNOTE: Due to the GoogleSQL snapping process, edges with a sufficiently short length will be discarded and the two endpoints will be snapped to a single point. Therefore, it is possible that vertices in a linestring may be snapped together such that one or more edge disappears. Hence, it is possible that a polygon hole that is sufficiently small may disappear, or the resulting `\nGEOGRAPHY ` may contain only a line or a point.\n\n**Return type**\n\n` GEOGRAPHY `"
            },
            "ST_MAXDISTANCE": {
                "name": "ST_MAXDISTANCE",
                "summary": "Gets the longest distance between two non-empty `\nGEOGRAPHY ` values.",
                "description": "ST_MAXDISTANCE(geography_1, geography_2[, use_spheroid])\n\nReturns the longest distance in meters between two non-empty ` GEOGRAPHY ` s;\nthat is, the distance between two vertices where the first vertex is in the first ` GEOGRAPHY ` , and the second vertex is in the second ` GEOGRAPHY ` .\nIf ` geography_1 ` and ` geography_2 ` are the same ` GEOGRAPHY ` , the function returns the distance between the two most distant vertices in that `\nGEOGRAPHY ` .\n\nIf either of the input ` GEOGRAPHY ` s is empty, ` ST_MAXDISTANCE ` returns `\nNULL ` .\n\nThe optional ` use_spheroid ` parameter determines how this function measures distance. If ` use_spheroid ` is ` FALSE ` , the function measures distance on the surface of a perfect sphere.\n\nThe ` use_spheroid ` parameter currently only supports the value ` FALSE ` .\nThe default value of ` use_spheroid ` is ` FALSE ` .\n\n**Return type**\n\n` FLOAT64 `"
            },
            "ST_NPOINTS": {
                "name": "ST_NPOINTS",
                "summary": "An alias of ` ST_NUMPOINTS ` .",
                "description": "ST_NPOINTS(geography_expression)\n\n**Description**\n\nAn alias of  ST_NUMPOINTS  ."
            },
            "ST_NUMGEOMETRIES": {
                "name": "ST_NUMGEOMETRIES",
                "summary": "Gets the number of geometries in a ` GEOGRAPHY `\nvalue.",
                "description": "ST_NUMGEOMETRIES(geography_expression)\n\n**Description**\n\nReturns the number of geometries in the input ` GEOGRAPHY ` . For a single point, linestring, or polygon, ` ST_NUMGEOMETRIES ` returns ` 1 ` . For any collection of geometries, ` ST_NUMGEOMETRIES ` returns the number of geometries making up the collection. ` ST_NUMGEOMETRIES ` returns ` 0 ` if the input is the empty ` GEOGRAPHY ` .\n\n**Return type**\n\n` INT64 `\n\n**Example**\n\nThe following example computes ` ST_NUMGEOMETRIES ` for a single point geography, two collections, and an empty geography.\n\n\nWITH example AS( SELECT ST_GEOGFROMTEXT('POINT(5 0)') AS geography UNION ALL SELECT ST_GEOGFROMTEXT('MULTIPOINT(0 1, 4 3, 2 6)') AS geography UNION ALL SELECT ST_GEOGFROMTEXT('GEOMETRYCOLLECTION(POINT(0 0), LINESTRING(1 2, 2 1))') AS geography UNION ALL SELECT ST_GEOGFROMTEXT('GEOMETRYCOLLECTION EMPTY')) SELECT geography,\nST_NUMGEOMETRIES(geography) AS num_geometries,\nFROM example;\n\n/*------------------------------------------------------+----------------*\n| geography                                            | num_geometries |\n+------------------------------------------------------+----------------+\n| POINT(5 0)                                           | 1              |\n| MULTIPOINT(0 1, 4 3, 2 6)                            | 3              |\n| GEOMETRYCOLLECTION(POINT(0 0), LINESTRING(1 2, 2 1)) | 2              |\n| GEOMETRYCOLLECTION EMPTY                             | 0              |\n*------------------------------------------------------+----------------*/"
            },
            "ST_NUMPOINTS": {
                "name": "ST_NUMPOINTS",
                "summary": "Gets the number of vertices in the a ` GEOGRAPHY ` value.",
                "description": "ST_NUMPOINTS(geography_expression)\n\n**Description**\n\nReturns the number of vertices in the input ` GEOGRAPHY ` . This includes the number of points, the number of linestring vertices, and the number of polygon vertices.\n\nNOTE: The first and last vertex of a polygon ring are counted as distinct vertices.\n\n**Return type**\n\n` INT64 `"
            },
            "ST_PERIMETER": {
                "name": "ST_PERIMETER",
                "summary": "Gets the length of the boundary of the polygons in a `\nGEOGRAPHY ` value.",
                "description": "ST_PERIMETER(geography_expression[, use_spheroid])\n\n**Description**\n\nReturns the length in meters of the boundary of the polygons in the input `\nGEOGRAPHY ` .\n\nIf ` geography_expression ` is a point or a line, returns zero. If `\ngeography_expression ` is a collection, returns the perimeter of the polygons in the collection; if the collection does not contain polygons, returns zero.\n\nThe optional ` use_spheroid ` parameter determines how this function measures distance. If ` use_spheroid ` is ` FALSE ` , the function measures distance on the surface of a perfect sphere.\n\nThe ` use_spheroid ` parameter currently only supports the value ` FALSE ` .\nThe default value of ` use_spheroid ` is ` FALSE ` .\n\n**Return type**\n\n` FLOAT64 `"
            },
            "ST_POINTN": {
                "name": "ST_POINTN",
                "summary": "Gets the point at a specific index of a linestring `\nGEOGRAPHY ` value.",
                "description": "ST_POINTN(linestring_geography, index)\n\n**Description**\n\nReturns the Nth point of a linestring geography as a point geography, where N is the index. The index is 1-based. Negative values are counted backwards from the end of the linestring, so that -1 is the last point. Returns an error if the input is not a linestring, if the input is empty, or if there is no vertex at the given index. Use the ` SAFE ` prefix to obtain ` NULL ` for invalid input instead of an error.\n\n**Return Type**\n\nPoint ` GEOGRAPHY `\n\n**Example**\n\nThe following example uses ` ST_POINTN ` ,  ` ST_STARTPOINT ` and  `\nST_ENDPOINT ` to extract points from a linestring.\n\n\nWITH linestring AS ( SELECT ST_GEOGFROMTEXT('LINESTRING(1 1, 2 1, 3 2, 3 3)') g ) SELECT ST_POINTN(g, 1) AS first, ST_POINTN(g, -1) AS last,\nST_POINTN(g, 2) AS second, ST_POINTN(g, -2) AS second_to_last FROM linestring;\n\n/*--------------+--------------+--------------+----------------*\n| first        | last         | second       | second_to_last |\n+--------------+--------------+--------------+----------------+\n| POINT(1 1)   | POINT(3 3)   | POINT(2 1)   | POINT(3 2)     |\n*--------------+--------------+--------------+----------------*/"
            },
            "ST_SIMPLIFY": {
                "name": "ST_SIMPLIFY",
                "summary": "Converts a ` GEOGRAPHY ` value into a simplified `\nGEOGRAPHY ` value, using tolerance.",
                "description": "ST_SIMPLIFY(geography, tolerance_meters)\n\n**Description**\n\nReturns a simplified version of ` geography ` , the given input ` GEOGRAPHY `\n. The input ` GEOGRAPHY ` is simplified by replacing nearly straight chains of short edges with a single long edge. The input ` geography ` will not change by more than the tolerance specified by ` tolerance_meters ` . Thus,\nsimplified edges are guaranteed to pass within ` tolerance_meters ` of the\n_original_ positions of all vertices that were removed from that edge. The given ` tolerance_meters ` is in meters on the surface of the Earth.\n\nNote that ` ST_SIMPLIFY ` preserves topological relationships, which means that no new crossing edges will be created and the output will be valid. For a large enough tolerance, adjacent shapes may collapse into a single object, or a shape could be simplified to a shape with a smaller dimension.\n\n**Constraints**\n\nFor ` ST_SIMPLIFY ` to have any effect, ` tolerance_meters ` must be non-zero.\n\n` ST_SIMPLIFY ` returns an error if the tolerance specified by `\ntolerance_meters ` is one of the following:\n\n* A negative tolerance.\n* Greater than ~7800 kilometers.\n\n**Return type**\n\n` GEOGRAPHY `\n\n**Examples**\n\nThe following example shows how ` ST_SIMPLIFY ` simplifies the input line `\nGEOGRAPHY ` by removing intermediate vertices.\n\n\nWITH example AS (SELECT ST_GEOGFROMTEXT('LINESTRING(0 0, 0.05 0, 0.1 0, 0.15 0, 2 0)') AS line) SELECT line AS original_line,\nST_SIMPLIFY(line, 1) AS simplified_line FROM example;\n\n/*---------------------------------------------+----------------------*\n|                original_line                |   simplified_line    |\n+---------------------------------------------+----------------------+\n| LINESTRING(0 0, 0.05 0, 0.1 0, 0.15 0, 2 0) | LINESTRING(0 0, 2 0) |\n*---------------------------------------------+----------------------*/\n\nThe following example illustrates how the result of ` ST_SIMPLIFY ` can have a lower dimension than the original shape.\n\n\nWITH example AS (SELECT ST_GEOGFROMTEXT('POLYGON((0 0, 0.1 0, 0.1 0.1, 0 0))') AS polygon,\nt AS tolerance FROM UNNEST([1000, 10000, 100000]) AS t) SELECT polygon AS original_triangle,\ntolerance AS tolerance_meters,\nST_SIMPLIFY(polygon, tolerance) AS simplified_result FROM example\n\n/*-------------------------------------+------------------+-------------------------------------*\n|          original_triangle          | tolerance_meters |          simplified_result          |\n+-------------------------------------+------------------+-------------------------------------+\n| POLYGON((0 0, 0.1 0, 0.1 0.1, 0 0)) |             1000 | POLYGON((0 0, 0.1 0, 0.1 0.1, 0 0)) |\n| POLYGON((0 0, 0.1 0, 0.1 0.1, 0 0)) |            10000 |            LINESTRING(0 0, 0.1 0.1) |\n| POLYGON((0 0, 0.1 0, 0.1 0.1, 0 0)) |           100000 |                          POINT(0 0) |\n*-------------------------------------+------------------+-------------------------------------*/"
            },
            "ST_SNAPTOGRID": {
                "name": "ST_SNAPTOGRID",
                "summary": "Produces a ` GEOGRAPHY ` value, where each vertex has been snapped to a longitude/latitude grid.",
                "description": "ST_SNAPTOGRID(geography_expression, grid_size)\n\n**Description**\n\nReturns the input ` GEOGRAPHY ` , where each vertex has been snapped to a longitude/latitude grid. The grid size is determined by the ` grid_size `\nparameter which is given in degrees.\n\n**Constraints**\n\nArbitrary grid sizes are not supported. The ` grid_size ` parameter is rounded so that it is of the form ` 10^n ` , where ` -10 < n < 0 ` .\n\n**Return type**\n\n` GEOGRAPHY `"
            },
            "ST_STARTPOINT": {
                "name": "ST_STARTPOINT",
                "summary": "Gets the first point of a linestring ` GEOGRAPHY ` value.",
                "description": "ST_STARTPOINT(linestring_geography)\n\n**Description**\n\nReturns the first point of a linestring geography as a point geography.\nReturns an error if the input is not a linestring or if the input is empty.\nUse the ` SAFE ` prefix to obtain ` NULL ` for invalid input instead of an error.\n\n**Return Type**\n\nPoint ` GEOGRAPHY `\n\n**Example**\n\n\nSELECT ST_STARTPOINT(ST_GEOGFROMTEXT('LINESTRING(1 1, 2 1, 3 2, 3 3)')) first\n\n/*--------------*\n| first        |\n+--------------+\n| POINT(1 1)   |\n*--------------*/"
            },
            "ST_TOUCHES": {
                "name": "ST_TOUCHES",
                "summary": "Checks if two ` GEOGRAPHY ` values intersect and their interiors have no elements in common.",
                "description": "ST_TOUCHES(geography_1, geography_2)\n\n**Description**\n\nReturns ` TRUE ` provided the following two conditions are satisfied:\n\n1. ` geography_1 ` intersects ` geography_2 ` .\n2. The interior of ` geography_1 ` and the interior of ` geography_2 ` are disjoint.\n\n**Return type**\n\n` BOOL `"
            },
            "ST_UNION": {
                "name": "ST_UNION",
                "summary": "Gets the point set union of multiple ` GEOGRAPHY ` values.",
                "description": "ST_UNION(geography_1, geography_2)\n\n\nST_UNION(array_of_geography)\n\n**Description**\n\nReturns a ` GEOGRAPHY ` that represents the point set union of all input `\nGEOGRAPHY ` s.\n\n` ST_UNION ` comes in two variants. For the first variant, input must be two `\nGEOGRAPHY ` s. For the second, the input is an ` ARRAY ` of type ` GEOGRAPHY `\n.\n\nFor the first variant of ` ST_UNION ` , if an input ` GEOGRAPHY ` is ` NULL `\n, ` ST_UNION ` returns ` NULL ` . For the second variant, if the input ` ARRAY\n` value is ` NULL ` , ` ST_UNION ` returns ` NULL ` . For a non- ` NULL `\ninput ` ARRAY ` , the union is computed and ` NULL ` elements are ignored so that they do not affect the output.\n\nSee  ` ST_UNION_AGG ` for the aggregate version of ` ST_UNION ` .\n\n**Return type**\n\n` GEOGRAPHY `\n\n**Example**\n\n\nSELECT ST_UNION( ST_GEOGFROMTEXT('LINESTRING(-122.12 47.67, -122.19 47.69)'),\nST_GEOGFROMTEXT('LINESTRING(-122.12 47.67, -100.19 47.69)') ) AS results\n\n/*---------------------------------------------------------*\n| results                                                 |\n+---------------------------------------------------------+\n| LINESTRING(-100.19 47.69, -122.12 47.67, -122.19 47.69) |\n*---------------------------------------------------------*/"
            },
            "ST_UNION_AGG": {
                "name": "ST_UNION_AGG",
                "summary": "Aggregates over ` GEOGRAPHY ` values and gets their point set union.",
                "description": "ST_UNION_AGG(geography)\n\n**Description**\n\nReturns a ` GEOGRAPHY ` that represents the point set union of all input `\nGEOGRAPHY ` s.\n\n` ST_UNION_AGG ` ignores ` NULL ` input ` GEOGRAPHY ` values.\n\nSee  ` ST_UNION ` for the non-aggregate version of ` ST_UNION_AGG ` .\n\n**Return type**\n\n` GEOGRAPHY `\n\n**Example**\n\n\nSELECT ST_UNION_AGG(items) AS results FROM UNNEST([\nST_GEOGFROMTEXT('LINESTRING(-122.12 47.67, -122.19 47.69)'),\nST_GEOGFROMTEXT('LINESTRING(-122.12 47.67, -100.19 47.69)'),\nST_GEOGFROMTEXT('LINESTRING(-122.12 47.67, -122.19 47.69)')]) as items;\n\n/*---------------------------------------------------------*\n| results                                                 |\n+---------------------------------------------------------+\n| LINESTRING(-100.19 47.69, -122.12 47.67, -122.19 47.69) |\n*---------------------------------------------------------*/"
            },
            "ST_WITHIN": {
                "name": "ST_WITHIN",
                "summary": "Checks if one ` GEOGRAPHY ` value contains another `\nGEOGRAPHY ` value.",
                "description": "ST_WITHIN(geography_1, geography_2)\n\n**Description**\n\nReturns ` TRUE ` if no point of ` geography_1 ` is outside of ` geography_2 `\nand the interiors of ` geography_1 ` and ` geography_2 ` intersect.\n\nGiven two geographies ` a ` and ` b ` , ` ST_WITHIN(a, b) ` returns the same result as  ` ST_CONTAINS ` ` (b, a) ` . Note the opposite order of arguments.\n\n**Return type**\n\n` BOOL `"
            },
            "ST_X": {
                "name": "ST_X",
                "summary": "Gets the longitude from a point ` GEOGRAPHY ` value.",
                "description": "ST_X(point_geography_expression)\n\n**Description**\n\nReturns the longitude in degrees of the single-point input ` GEOGRAPHY ` .\n\nFor any input ` GEOGRAPHY ` that is not a single point, including an empty `\nGEOGRAPHY ` , ` ST_X ` returns an error. Use the ` SAFE. ` prefix to obtain `\nNULL ` .\n\n**Return type**\n\n` FLOAT64 `\n\n**Example**\n\nThe following example uses ` ST_X ` and ` ST_Y ` to extract coordinates from single-point geographies.\n\n\nWITH points AS (SELECT ST_GEOGPOINT(i, i + 1) AS p FROM UNNEST([0, 5, 12]) AS i) SELECT p,\nST_X(p) as longitude,\nST_Y(p) as latitude FROM points;\n\n/*--------------+-----------+----------*\n| p            | longitude | latitude |\n+--------------+-----------+----------+\n| POINT(0 1)   | 0.0       | 1.0      |\n| POINT(5 6)   | 5.0       | 6.0      |\n| POINT(12 13) | 12.0      | 13.0     |\n*--------------+-----------+----------*/"
            },
            "ST_Y": {
                "name": "ST_Y",
                "summary": "Gets the latitude from a point ` GEOGRAPHY ` value.",
                "description": "ST_Y(point_geography_expression)\n\n**Description**\n\nReturns the latitude in degrees of the single-point input ` GEOGRAPHY ` .\n\nFor any input ` GEOGRAPHY ` that is not a single point, including an empty `\nGEOGRAPHY ` , ` ST_Y ` returns an error. Use the ` SAFE. ` prefix to return `\nNULL ` instead.\n\n**Return type**\n\n` FLOAT64 `\n\n**Example**\n\nSee  ` ST_X ` for example usage."
            }
        }
    },
    {
        "category": "hash-functions",
        "description": "GoogleSQL for BigQuery supports the following hash functions.",
        "source": "hash_functions.txt",
        "functions": {
            "FARM_FINGERPRINT": {
                "name": "FARM_FINGERPRINT",
                "summary": "Computes the fingerprint of a ` STRING ` or ` BYTES `\nvalue, using the FarmHash Fingerprint64 algorithm.",
                "description": "FARM_FINGERPRINT(value)\n\n**Description**\n\nComputes the fingerprint of the ` STRING ` or ` BYTES ` input using the `\nFingerprint64 ` function from the [ open-source FarmHash library\n](https://github.com/google/farmhash) . The output of this function for a particular input will never change.\n\n**Return type**\n\nINT64\n\n**Examples**\n\n\nWITH example AS ( SELECT 1 AS x, \"foo\" AS y, true AS z UNION ALL SELECT 2 AS x, \"apple\" AS y, false AS z UNION ALL SELECT 3 AS x, \"\" AS y, true AS z ) SELECT\n*,\nFARM_FINGERPRINT(CONCAT(CAST(x AS STRING), y, CAST(z AS STRING))) AS row_fingerprint FROM example;\n/*---+-------+-------+----------------------*\n| x | y     | z     | row_fingerprint      |\n+---+-------+-------+----------------------+\n| 1 | foo   | true  | -1541654101129638711 |\n| 2 | apple | false | 2794438866806483259  |\n| 3 |       | true  | -4880158226897771312 |\n*---+-------+-------+----------------------*/"
            },
            "MD5": {
                "name": "MD5",
                "summary": "Computes the hash of a ` STRING ` or ` BYTES ` value, using the MD5 algorithm.",
                "description": "MD5(input)\n\n**Description**\n\nComputes the hash of the input using the [ MD5 algorithm\n](https://en.wikipedia.org/wiki/MD5) . The input can either be ` STRING ` or `\nBYTES ` . The string version treats the input as an array of bytes.\n\nThis function returns 16 bytes.\n\n**Warning:** MD5 is no longer considered secure. For increased security use another hashing function.\n\n**Return type**\n\n` BYTES `\n\n**Example**\n\n\nSELECT MD5(\"Hello World\") as md5;\n\n-- Note that the result of MD5 is of type BYTES, displayed as a base64-encoded string.\n/*--------------------------*\n| md5                      |\n+--------------------------+\n| sQqNsWTgdUEFt6mb5y4/5Q== |\n*--------------------------*/"
            },
            "SHA1": {
                "name": "SHA1",
                "summary": "Computes the hash of a ` STRING ` or ` BYTES ` value, using the SHA-1 algorithm.",
                "description": "SHA1(input)\n\n**Description**\n\nComputes the hash of the input using the [ SHA-1 algorithm\n](https://en.wikipedia.org/wiki/SHA-1) . The input can either be ` STRING ` or\n` BYTES ` . The string version treats the input as an array of bytes.\n\nThis function returns 20 bytes.\n\n**Warning:** SHA1 is no longer considered secure. For increased security, use another hashing function.\n\n**Return type**\n\n` BYTES `\n\n**Example**\n\n\nSELECT SHA1(\"Hello World\") as sha1;\n\n-- Note that the result of SHA1 is of type BYTES, displayed as a base64-encoded string.\n/*------------------------------*\n| sha1                         |\n+------------------------------+\n| Ck1VqNd45QIvq3AZd8XYQLvEhtA= |\n*------------------------------*/"
            },
            "SHA256": {
                "name": "SHA256",
                "summary": "Computes the hash of a ` STRING ` or ` BYTES ` value, using the SHA-256 algorithm.",
                "description": "SHA256(input)\n\n**Description**\n\nComputes the hash of the input using the [ SHA-256 algorithm\n](https://en.wikipedia.org/wiki/SHA-2) . The input can either be ` STRING ` or\n` BYTES ` . The string version treats the input as an array of bytes.\n\nThis function returns 32 bytes.\n\n**Return type**\n\n` BYTES `\n\n**Example**\n\n\nSELECT SHA256(\"Hello World\") as sha256;"
            },
            "SHA512": {
                "name": "SHA512",
                "summary": "Computes the hash of a ` STRING ` or ` BYTES ` value, using the SHA-512 algorithm.",
                "description": "SHA512(input)\n\n**Description**\n\nComputes the hash of the input using the [ SHA-512 algorithm\n](https://en.wikipedia.org/wiki/SHA-2) . The input can either be ` STRING ` or\n` BYTES ` . The string version treats the input as an array of bytes.\n\nThis function returns 64 bytes.\n\n**Return type**\n\n` BYTES `\n\n**Example**\n\n\nSELECT SHA512(\"Hello World\") as sha512;"
            }
        }
    },
    {
        "category": "hyperloglog-functions",
        "description": "The [ HyperLogLog++ algorithm (HLL++) ](/bigquery/docs/sketches#sketches_hll) estimates [ cardinality ](https://en.wikipedia.org/wiki/Cardinality) from [\nsketches ](/bigquery/docs/sketches#sketches_hll) .\n\nHLL++ functions are approximate aggregate functions. Approximate aggregation typically requires less memory than exact aggregation functions, like [ `\nCOUNT(DISTINCT) ` ](/bigquery/docs/reference/standard-\nsql/aggregate_functions#count) , but also introduces statistical error. This makes HLL++ functions appropriate for large data streams for which linear memory usage is impractical, as well as for data that is already approximate.\n\nIf you do not need materialized sketches, you can alternatively use an [\napproximate aggregate function with system-defined precision\n](/bigquery/docs/reference/standard-sql/approximate_aggregate_functions) ,\nsuch as [ ` APPROX_COUNT_DISTINCT ` ](/bigquery/docs/reference/standard-\nsql/approximate_aggregate_functions#approx-count-distinct) . However, `\nAPPROX_COUNT_DISTINCT ` does not allow partial aggregations, re-aggregations,\nand custom precision.\n\nGoogleSQL for BigQuery supports the following HLL++ functions:",
        "source": "hll_functions.txt",
        "functions": {
            "HLL_COUNT.EXTRACT": {
                "name": "HLL_COUNT.EXTRACT",
                "summary": "Extracts a cardinality estimate of an HLL++ sketch.",
                "description": "HLL_COUNT.EXTRACT(sketch)\n\n**Description**\n\nA scalar function that extracts a cardinality estimate of a single [ HLL++\n](https://research.google.com/pubs/pub40671.html) sketch.\n\nIf ` sketch ` is ` NULL ` , this function returns a cardinality estimate of `\n0 ` .\n\n**Supported input types**\n\n` BYTES `\n\n**Return type**\n\n` INT64 `\n\n**Example**\n\nThe following query returns the number of distinct users for each country who have at least one invoice.\n\n\nSELECT country,\nHLL_COUNT.EXTRACT(HLL_sketch) AS distinct_customers_with_open_invoice FROM ( SELECT country,\nHLL_COUNT.INIT(customer_id) AS hll_sketch FROM UNNEST( ARRAY<STRUCT<country STRING, customer_id STRING, invoice_id STRING>>[\n('UA', 'customer_id_1', 'invoice_id_11'),\n('BR', 'customer_id_3', 'invoice_id_31'),\n('CZ', 'customer_id_2', 'invoice_id_22'),\n('CZ', 'customer_id_2', 'invoice_id_23'),\n('BR', 'customer_id_3', 'invoice_id_31'),\n('UA', 'customer_id_2', 'invoice_id_24')]) GROUP BY country );\n\n/*---------+--------------------------------------*\n| country | distinct_customers_with_open_invoice |\n+---------+--------------------------------------+\n| UA      |                                    2 |\n| BR      |                                    1 |\n| CZ      |                                    1 |\n*---------+--------------------------------------*/"
            },
            "HLL_COUNT.INIT": {
                "name": "HLL_COUNT.INIT",
                "summary": "Aggregates values of the same underlying type into a new HLL++ sketch.",
                "description": "HLL_COUNT.INIT(input [, precision])\n\n**Description**\n\nAn aggregate function that takes one or more ` input ` values and aggregates them into a [ HLL++ ](https://research.google.com/pubs/pub40671.html) sketch.\nEach sketch is represented using the ` BYTES ` data type. You can then merge sketches using ` HLL_COUNT.MERGE ` or ` HLL_COUNT.MERGE_PARTIAL ` . If no merging is needed, you can extract the final count of distinct values from the sketch using ` HLL_COUNT.EXTRACT ` .\n\nThis function supports an optional parameter, ` precision ` . This parameter defines the accuracy of the estimate at the cost of additional memory required to process the sketches or store them on disk. The range for this value is `\n10 ` to ` 24 ` . The default value is ` 15 ` . For more information about precision, see [ Precision for sketches\n](/bigquery/docs/sketches#precision_hll) .\n\nIf the input is ` NULL ` , this function returns ` NULL ` .\n\nFor more information, see [ HyperLogLog in Practice: Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm\n](https://research.google.com/pubs/pub40671.html) .\n\n**Supported input types**\n\n* ` INT64 `\n* ` NUMERIC `\n* ` BIGNUMERIC `\n* ` STRING `\n* ` BYTES `\n\n**Return type**\n\n` BYTES `\n\n**Example**\n\nThe following query creates HLL++ sketches that count the number of distinct users with at least one invoice per country.\n\n\nSELECT country,\nHLL_COUNT.INIT(customer_id, 10) AS hll_sketch FROM UNNEST( ARRAY<STRUCT<country STRING, customer_id STRING, invoice_id STRING>>[\n('UA', 'customer_id_1', 'invoice_id_11'),\n('CZ', 'customer_id_2', 'invoice_id_22'),\n('CZ', 'customer_id_2', 'invoice_id_23'),\n('BR', 'customer_id_3', 'invoice_id_31'),\n('UA', 'customer_id_2', 'invoice_id_24')]) GROUP BY country;\n\n/*---------+------------------------------------------------------------------------------------*\n| country | hll_sketch                                                                         |\n+---------+------------------------------------------------------------------------------------+\n| UA      | \"\\010p\\020\\002\\030\\002 \\013\\202\\007\\r\\020\\002\\030\\n \\0172\\005\\371\\344\\001\\315\\010\" |\n| CZ      | \"\\010p\\020\\002\\030\\002 \\013\\202\\007\\013\\020\\001\\030\\n \\0172\\003\\371\\344\\001\"       |\n| BR      | \"\\010p\\020\\001\\030\\002 \\013\\202\\007\\013\\020\\001\\030\\n \\0172\\003\\202\\341\\001\"       |\n*---------+------------------------------------------------------------------------------------*/"
            },
            "HLL_COUNT.MERGE": {
                "name": "HLL_COUNT.MERGE",
                "summary": "Merges HLL++ sketches of the same underlying type into a new sketch, and then gets the cardinality of the new sketch.",
                "description": "HLL_COUNT.MERGE(sketch)\n\n**Description**\n\nAn aggregate function that returns the cardinality of several [ HLL++\n](https://research.google.com/pubs/pub40671.html) sketches by computing their union.\n\nEach ` sketch ` must be initialized on the same type. Attempts to merge sketches for different types results in an error. For example, you cannot merge a sketch initialized from ` INT64 ` data with one initialized from `\nSTRING ` data.\n\nIf the merged sketches were initialized with different precisions, the precision will be downgraded to the lowest precision involved in the merge.\n\nThis function ignores ` NULL ` values when merging sketches. If the merge happens over zero rows or only over ` NULL ` values, the function returns ` 0\n` .\n\n**Supported input types**\n\n` BYTES `\n\n**Return type**\n\n` INT64 `\n\n**Example**\n\nThe following query counts the number of distinct users across all countries who have at least one invoice.\n\n\nSELECT HLL_COUNT.MERGE(hll_sketch) AS distinct_customers_with_open_invoice FROM ( SELECT country,\nHLL_COUNT.INIT(customer_id) AS hll_sketch FROM UNNEST( ARRAY<STRUCT<country STRING, customer_id STRING, invoice_id STRING>>[\n('UA', 'customer_id_1', 'invoice_id_11'),\n('BR', 'customer_id_3', 'invoice_id_31'),\n('CZ', 'customer_id_2', 'invoice_id_22'),\n('CZ', 'customer_id_2', 'invoice_id_23'),\n('BR', 'customer_id_3', 'invoice_id_31'),\n('UA', 'customer_id_2', 'invoice_id_24')]) GROUP BY country );\n\n/*--------------------------------------*\n| distinct_customers_with_open_invoice |\n+--------------------------------------+\n|                                    3 |\n*--------------------------------------*/"
            },
            "HLL_COUNT.MERGE_PARTIAL": {
                "name": "HLL_COUNT.MERGE_PARTIAL",
                "summary": "Merges HLL++ sketches of the same underlying type into a new sketch.",
                "description": "HLL_COUNT.MERGE_PARTIAL(sketch)\n\n**Description**\n\nAn aggregate function that takes one or more [ HLL++\n](https://research.google.com/pubs/pub40671.html) ` sketch ` inputs and merges them into a new sketch.\n\nEach ` sketch ` must be initialized on the same type. Attempts to merge sketches for different types results in an error. For example, you cannot merge a sketch initialized from ` INT64 ` data with one initialized from `\nSTRING ` data.\n\nIf the merged sketches were initialized with different precisions, the precision will be downgraded to the lowest precision involved in the merge.\nFor example, if ` MERGE_PARTIAL ` encounters sketches of precision 14 and 15,\nthe returned new sketch will have precision 14.\n\nThis function returns ` NULL ` if there is no input or all inputs are ` NULL `\n.\n\n**Supported input types**\n\n` BYTES `\n\n**Return type**\n\n` BYTES `\n\n**Example**\n\nThe following query returns an HLL++ sketch that counts the number of distinct users who have at least one invoice across all countries.\n\n\nSELECT HLL_COUNT.MERGE_PARTIAL(HLL_sketch) AS distinct_customers_with_open_invoice FROM ( SELECT country,\nHLL_COUNT.INIT(customer_id) AS hll_sketch FROM UNNEST( ARRAY<STRUCT<country STRING, customer_id STRING, invoice_id STRING>>[\n('UA', 'customer_id_1', 'invoice_id_11'),\n('BR', 'customer_id_3', 'invoice_id_31'),\n('CZ', 'customer_id_2', 'invoice_id_22'),\n('CZ', 'customer_id_2', 'invoice_id_23'),\n('BR', 'customer_id_3', 'invoice_id_31'),\n('UA', 'customer_id_2', 'invoice_id_24')]) GROUP BY country );\n\n/*----------------------------------------------------------------------------------------------*\n| distinct_customers_with_open_invoice                                                         |\n+----------------------------------------------------------------------------------------------+\n| \"\\010p\\020\\006\\030\\002 \\013\\202\\007\\020\\020\\003\\030\\017 \\0242\\010\\320\\2408\\352}\\244\\223\\002\" |\n*----------------------------------------------------------------------------------------------*/"
            }
        }
    },
    {
        "category": "interval-functions",
        "description": "GoogleSQL for BigQuery supports the following interval functions.",
        "source": "interval_functions.txt",
        "functions": {
            "EXTRACT": {
                "name": "EXTRACT",
                "summary": "Extracts part of an ` INTERVAL ` value.",
                "description": "EXTRACT(part FROM interval_expression)\n\n**Description**\n\nReturns the value corresponding to the specified date part. The ` part ` must be one of ` YEAR ` , ` MONTH ` , ` DAY ` , ` HOUR ` , ` MINUTE ` , ` SECOND `\n, ` MILLISECOND ` or ` MICROSECOND ` .\n\n**Return Data Type**\n\n` INTERVAL `\n\n**Examples**\n\nIn the following example, different parts of two intervals are extracted.\n\n\nSELECT EXTRACT(YEAR FROM i) AS year,\nEXTRACT(MONTH FROM i) AS month,\nEXTRACT(DAY FROM i) AS day,\nEXTRACT(HOUR FROM i) AS hour,\nEXTRACT(MINUTE FROM i) AS minute,\nEXTRACT(SECOND FROM i) AS second,\nEXTRACT(MILLISECOND FROM i) AS milli,\nEXTRACT(MICROSECOND FROM i) AS micro FROM UNNEST([INTERVAL '1-2 3 4:5:6.789999' YEAR TO SECOND,\nINTERVAL '0-13 370 48:61:61' YEAR TO SECOND]) AS i\n\n/*------+-------+-----+------+--------+--------+-------+--------*\n| year | month | day | hour | minute | second | milli | micro  |\n+------+-------+-----+------+--------+--------+-------+--------+\n| 1    | 2     | 3   | 4    | 5      | 6      | 789   | 789999 |\n| 1    | 1     | 370 | 49   | 2      | 1      | 0     | 0      |\n*------+-------+-----+------+--------+--------+-------+--------*/\n\nWhen a negative sign precedes the time part in an interval, the negative sign distributes over the hours, minutes, and seconds. For example:\n\n\nSELECT EXTRACT(HOUR FROM i) AS hour,\nEXTRACT(MINUTE FROM i) AS minute FROM UNNEST([INTERVAL '10 -12:30' DAY TO MINUTE]) AS i\n\n/*------+--------*\n| hour | minute |\n+------+--------+\n| -12  | -30    |\n*------+--------*/\n\nWhen a negative sign precedes the year and month part in an interval, the negative sign distributes over the years and months. For example:\n\n\nSELECT EXTRACT(YEAR FROM i) AS year,\nEXTRACT(MONTH FROM i) AS month FROM UNNEST([INTERVAL '-22-6 10 -12:30' YEAR TO MINUTE]) AS i\n\n/*------+--------*\n| year | month  |\n+------+--------+\n| -22  | -6     |\n*------+--------*/"
            },
            "JUSTIFY_DAYS": {
                "name": "JUSTIFY_DAYS",
                "summary": "Normalizes the day part of an ` INTERVAL ` value.",
                "description": "JUSTIFY_DAYS(interval_expression)\n\n**Description**\n\nNormalizes the day part of the interval to the range from -29 to 29 by incrementing/decrementing the month or year part of the interval.\n\n**Return Data Type**\n\n` INTERVAL `\n\n**Example**\n\n\nSELECT JUSTIFY_DAYS(INTERVAL 29 DAY) AS i1,\nJUSTIFY_DAYS(INTERVAL -30 DAY) AS i2,\nJUSTIFY_DAYS(INTERVAL 31 DAY) AS i3,\nJUSTIFY_DAYS(INTERVAL -65 DAY) AS i4,\nJUSTIFY_DAYS(INTERVAL 370 DAY) AS i5\n\n/*--------------+--------------+-------------+---------------+--------------*\n| i1           | i2           | i3          | i4            | i5           |\n+--------------+--------------+-------------+---------------+--------------+\n| 0-0 29 0:0:0 | -0-1 0 0:0:0 | 0-1 1 0:0:0 | -0-2 -5 0:0:0 | 1-0 10 0:0:0 |\n*--------------+--------------+-------------+---------------+--------------*/"
            },
            "JUSTIFY_HOURS": {
                "name": "JUSTIFY_HOURS",
                "summary": "Normalizes the time part of an ` INTERVAL ` value.",
                "description": "JUSTIFY_HOURS(interval_expression)\n\n**Description**\n\nNormalizes the time part of the interval to the range from -23:59:59.999999 to 23:59:59.999999 by incrementing/decrementing the day part of the interval.\n\n**Return Data Type**\n\n` INTERVAL `\n\n**Example**\n\n\nSELECT JUSTIFY_HOURS(INTERVAL 23 HOUR) AS i1,\nJUSTIFY_HOURS(INTERVAL -24 HOUR) AS i2,\nJUSTIFY_HOURS(INTERVAL 47 HOUR) AS i3,\nJUSTIFY_HOURS(INTERVAL -12345 MINUTE) AS i4\n\n/*--------------+--------------+--------------+-----------------*\n| i1           | i2           | i3           | i4              |\n+--------------+--------------+--------------+-----------------+\n| 0-0 0 23:0:0 | 0-0 -1 0:0:0 | 0-0 1 23:0:0 | 0-0 -8 -13:45:0 |\n*--------------+--------------+--------------+-----------------*/"
            },
            "JUSTIFY_INTERVAL": {
                "name": "JUSTIFY_INTERVAL",
                "summary": "Normalizes the day and time parts of an ` INTERVAL `\nvalue.",
                "description": "JUSTIFY_INTERVAL(interval_expression)\n\n**Description**\n\nNormalizes the days and time parts of the interval.\n\n**Return Data Type**\n\n` INTERVAL `\n\n**Example**\n\n\nSELECT JUSTIFY_INTERVAL(INTERVAL '29 49:00:00' DAY TO SECOND) AS i\n\n/*-------------*\n| i           |\n+-------------+\n| 0-1 1 1:0:0 |\n*-------------*/"
            },
            "MAKE_INTERVAL": {
                "name": "MAKE_INTERVAL",
                "summary": "Constructs an ` INTERVAL ` value.",
                "description": "MAKE_INTERVAL([year][, month][, day][, hour][, minute][, second])\n\n**Description**\n\nConstructs an [ ` INTERVAL ` ](/bigquery/docs/reference/standard-sql/data-\ntypes#interval_type) object using ` INT64 ` values representing the year,\nmonth, day, hour, minute, and second. All arguments are optional, ` 0 ` by default, and can be [ named arguments ](/bigquery/docs/reference/standard-\nsql/functions-reference#named_arguments) .\n\n**Return Data Type**\n\n` INTERVAL `\n\n**Example**\n\n\nSELECT MAKE_INTERVAL(1, 6, 15) AS i1,\nMAKE_INTERVAL(hour => 10, second => 20) AS i2,\nMAKE_INTERVAL(1, minute => 5, day => 2) AS i3\n\n/*--------------+---------------+-------------*\n| i1           | i2            | i3          |\n+--------------+---------------+-------------+\n| 1-6 15 0:0:0 | 0-0 0 10:0:20 | 1-0 2 0:5:0 |\n*--------------+---------------+-------------*/"
            }
        }
    },
    {
        "category": "json-functions",
        "description": "GoogleSQL for BigQuery supports the following functions, which can retrieve and transform JSON data.\n\n###  Categories\n\nThe JSON functions are grouped into the following categories based on their behavior:\n\nCategory  |  Functions  |  Description\n---|---|---\nStandard extractors  |  ` JSON_QUERY `\n` JSON_VALUE `\n` JSON_QUERY_ARRAY `\n` JSON_VALUE_ARRAY `\n|  Functions that extract JSON data.\nLegacy extractors  |  ` JSON_EXTRACT `\n` JSON_EXTRACT_SCALAR `\n` JSON_EXTRACT_ARRAY `\n` JSON_EXTRACT_STRING_ARRAY `\n|  Functions that extract JSON data.\nWhile these functions are supported by GoogleSQL, we recommend using the standard extractor functions  .\nLax converters  |  ` LAX_BOOL `\n` LAX_FLOAT64 `\n` LAX_INT64 `\n` LAX_STRING `\n|  Functions that flexibly convert a JSON value to an SQL value without returning errors.\nConverters  |  ` BOOL `\n` FLOAT64 `\n` INT64 `\n` STRING `\n|  Functions that convert a JSON value to a SQL value.\nOther converters  |  ` PARSE_JSON `\n` TO_JSON `\n` TO_JSON_STRING `\n|  Other conversion functions from or to JSON.\nConstructors  |  ` JSON_ARRAY `\n` JSON_OBJECT `\n|  Functions that create JSON.\nMutators  |  ` JSON_ARRAY_APPEND `\n` JSON_ARRAY_INSERT `\n` JSON_REMOVE `\n` JSON_SET `\n` JSON_STRIP_NULLS `\n|  Functions that mutate existing JSON.\nAccessors  |  ` JSON_TYPE `\n|  Functions that provide access to JSON properties.",
        "source": "json_functions.txt",
        "functions": {
            "BOOL": {
                "name": "BOOL",
                "summary": "Converts a JSON boolean to a SQL ` BOOL ` value.",
                "description": "BOOL(json_expr)\n\n**Description**\n\nConverts a JSON boolean to a SQL ` BOOL ` value.\n\nArguments:\n\n* ` json_expr ` : JSON. For example:\n\nJSON 'true'\n\nIf the JSON value is not a boolean, an error is produced. If the expression is SQL ` NULL ` , the function returns SQL ` NULL ` .\n\n**Return type**\n\n` BOOL `\n\n**Examples**\n\n\nSELECT BOOL(JSON 'true') AS vacancy;\n\n/*---------*\n| vacancy |\n+---------+\n| true    |\n*---------*/\n\n\nSELECT BOOL(JSON_QUERY(JSON '{\"hotel class\": \"5-star\", \"vacancy\": true}', \"$.vacancy\")) AS vacancy;\n\n/*---------*\n| vacancy |\n+---------+\n| true    |\n*---------*/\n\nThe following examples show how invalid requests are handled:\n\n\n-- An error is thrown if JSON is not of type bool.\nSELECT BOOL(JSON '123') AS result; -- Throws an error SELECT BOOL(JSON 'null') AS result; -- Throws an error SELECT SAFE.BOOL(JSON '123') AS result; -- Returns a SQL NULL"
            },
            "FLOAT64": {
                "name": "FLOAT64",
                "summary": "Converts a JSON number to a SQL ` FLOAT64 ` value.",
                "description": "FLOAT64(json_expr[, wide_number_mode=>{ 'exact' | 'round' }])\n\n**Description**\n\nConverts a JSON number to a SQL ` FLOAT64 ` value.\n\nArguments:\n\n* ` json_expr ` : JSON. For example:\n\nJSON '9.8'\n\nIf the JSON value is not a number, an error is produced. If the expression is a SQL ` NULL ` , the function returns SQL ` NULL ` .\n\n* ` wide_number_mode ` : Optional mandatory-named argument, which defines what happens with a number that cannot be represented as a ` FLOAT64 ` without loss of precision. This argument accepts one of the two case-sensitive values:\n\n* ` exact ` : The function fails if the result cannot be represented as a ` FLOAT64 ` without loss of precision.\n* ` round ` (default): The numeric value stored in JSON will be rounded to ` FLOAT64 ` . If such rounding is not possible, the function fails.\n\n**Return type**\n\n` FLOAT64 `\n\n**Examples**\n\n\nSELECT FLOAT64(JSON '9.8') AS velocity;\n\n/*----------*\n| velocity |\n+----------+\n| 9.8      |\n*----------*/\n\n\nSELECT FLOAT64(JSON_QUERY(JSON '{\"vo2_max\": 39.1, \"age\": 18}', \"$.vo2_max\")) AS vo2_max;\n\n/*---------*\n| vo2_max |\n+---------+\n| 39.1    |\n*---------*/\n\n\nSELECT FLOAT64(JSON '18446744073709551615', wide_number_mode=>'round') as result;\n\n/*------------------------*\n| result                 |\n+------------------------+\n| 1.8446744073709552e+19 |\n*------------------------*/\n\n\nSELECT FLOAT64(JSON '18446744073709551615') as result;\n\n/*------------------------*\n| result                 |\n+------------------------+\n| 1.8446744073709552e+19 |\n*------------------------*/\n\nThe following examples show how invalid requests are handled:\n\n\n-- An error is thrown if JSON is not of type FLOAT64.\nSELECT FLOAT64(JSON '\"strawberry\"') AS result;\nSELECT FLOAT64(JSON 'null') AS result;\n\n-- An error is thrown because `wide_number_mode` is case-sensitive and not \"exact\" or \"round\".\nSELECT FLOAT64(JSON '123.4', wide_number_mode=>'EXACT') as result;\nSELECT FLOAT64(JSON '123.4', wide_number_mode=>'exac') as result;\n\n-- An error is thrown because the number cannot be converted to DOUBLE without loss of precision SELECT FLOAT64(JSON '18446744073709551615', wide_number_mode=>'exact') as result;\n\n-- Returns a SQL NULL SELECT SAFE.FLOAT64(JSON '\"strawberry\"') AS result;"
            },
            "INT64": {
                "name": "INT64",
                "summary": "Converts a JSON number to a SQL ` INT64 ` value.",
                "description": "INT64(json_expr)\n\n**Description**\n\nConverts a JSON number to a SQL ` INT64 ` value.\n\nArguments:\n\n* ` json_expr ` : JSON. For example:\n\nJSON '999'\n\nIf the JSON value is not a number, or the JSON number is not in the SQL `\nINT64 ` domain, an error is produced. If the expression is SQL ` NULL ` , the function returns SQL ` NULL ` .\n\n**Return type**\n\n` INT64 `\n\n**Examples**\n\n\nSELECT INT64(JSON '2005') AS flight_number;\n\n/*---------------*\n| flight_number |\n+---------------+\n| 2005          |\n*---------------*/\n\n\nSELECT INT64(JSON_QUERY(JSON '{\"gate\": \"A4\", \"flight_number\": 2005}', \"$.flight_number\")) AS flight_number;\n\n/*---------------*\n| flight_number |\n+---------------+\n| 2005          |\n*---------------*/\n\n\nSELECT INT64(JSON '10.0') AS score;\n\n/*-------*\n| score |\n+-------+\n| 10    |\n*-------*/\n\nThe following examples show how invalid requests are handled:\n\n\n-- An error is thrown if JSON is not a number or cannot be converted to a 64-bit integer.\nSELECT INT64(JSON '10.1') AS result;  -- Throws an error SELECT INT64(JSON '\"strawberry\"') AS result; -- Throws an error SELECT INT64(JSON 'null') AS result; -- Throws an error SELECT SAFE.INT64(JSON '\"strawberry\"') AS result;  -- Returns a SQL NULL"
            },
            "JSON_ARRAY": {
                "name": "JSON_ARRAY",
                "summary": "Creates a JSON array.",
                "description": "JSON_ARRAY([value][, ...])\n\n**Description**\n\nCreates a JSON array from zero or more SQL values.\n\nArguments:\n\n* ` value ` : A  JSON encoding-supported  value to add to a JSON array.\n\n**Return type**\n\n` JSON `\n\n**Examples**\n\nYou can create an empty JSON array. For example:\n\n\nSELECT JSON_ARRAY() AS json_data\n\n/*-----------*\n| json_data |\n+-----------+\n| []        |\n*-----------*/\n\nThe following query creates a JSON array with one value in it:\n\n\nSELECT JSON_ARRAY(10) AS json_data\n\n/*-----------*\n| json_data |\n+-----------+\n| [10]      |\n*-----------*/\n\nYou can create a JSON array with an empty JSON array in it. For example:\n\n\nSELECT JSON_ARRAY([]) AS json_data\n\n/*-----------*\n| json_data |\n+-----------+\n| [[]]      |\n*-----------*/\n\n\nSELECT JSON_ARRAY(10, 'foo', NULL) AS json_data\n\n/*-----------------*\n| json_data       |\n+-----------------+\n| [10,\"foo\",null] |\n*-----------------*/\n\n\nSELECT JSON_ARRAY(STRUCT(10 AS a, 'foo' AS b)) AS json_data\n\n/*----------------------*\n| json_data            |\n+----------------------+\n| [{\"a\":10,\"b\":\"foo\"}] |\n*----------------------*/\n\n\nSELECT JSON_ARRAY(10, ['foo', 'bar'], [20, 30]) AS json_data\n\n/*----------------------------*\n| json_data                  |\n+----------------------------+\n| [10,[\"foo\",\"bar\"],[20,30]] |\n*----------------------------*/\n\n\nSELECT JSON_ARRAY(10, [JSON '20', JSON '\"foo\"']) AS json_data\n\n/*-----------------*\n| json_data       |\n+-----------------+\n| [10,[20,\"foo\"]] |\n*-----------------*/"
            },
            "JSON_ARRAY_APPEND": {
                "name": "JSON_ARRAY_APPEND",
                "summary": "Appends JSON data to the end of a JSON array.",
                "description": "JSON_ARRAY_APPEND( json_expr,\njson_path_value_pair[, ...]\n[, append_each_element=>{ TRUE | FALSE }]\n)\n\njson_path_value_pair:\njson_path, value\n\nAppends JSON data to the end of a JSON array.\n\nArguments:\n\n* ` json_expr ` : JSON. For example:\n\nJSON '[\"a\", \"b\", \"c\"]'\n\n* ` json_path_value_pair ` : A value and the  JSONPath  for that value. This includes:\n\n* ` json_path ` : Append ` value ` at this  JSONPath  in ` json_expr ` .\n\n* ` value ` : A  JSON encoding-supported  value to append.\n\n* ` append_each_element ` : An optional, mandatory named argument.\n\n* If ` TRUE ` (default), and ` value ` is a SQL array, appends each element individually.\n\n* If ` FALSE, ` and ` value ` is a SQL array, appends the array as one element.\n\nDetails:\n\n* Path value pairs are evaluated left to right. The JSON produced by evaluating one pair becomes the JSON against which the next pair is evaluated.\n* The operation is ignored if the path points to a JSON non-array value that is not a JSON null.\n* If ` json_path ` points to a JSON null, the JSON null is replaced by a JSON array that contains ` value ` .\n* If the path exists but has an incompatible type at any given path token, the path value pair operation is ignored.\n* The function applies all path value pair append operations even if an individual path value pair operation is invalid. For invalid operations, the operation is ignored and the function continues to process the rest of the path value pairs.\n* If any ` json_path ` is an invalid  JSONPath  , an error is produced.\n* If ` json_expr ` is SQL ` NULL ` , the function returns SQL ` NULL ` .\n* If ` append_each_element ` is SQL ` NULL ` , the function returns ` json_expr ` .\n* If ` json_path ` is SQL ` NULL ` , the ` json_path_value_pair ` operation is ignored.\n\n**Return type**\n\n` JSON `\n\n**Examples**\n\nIn the following example, path ` $ ` is matched and appends ` 1 ` .\n\n\nSELECT JSON_ARRAY_APPEND(JSON '[\"a\", \"b\", \"c\"]', '$', 1) AS json_data\n\n/*-----------------*\n| json_data       |\n+-----------------+\n| [\"a\",\"b\",\"c\",1] |\n*-----------------*/\n\nIn the following example, ` append_each_element ` defaults to ` TRUE ` , so `\n[1, 2] ` is appended as individual elements.\n\n\nSELECT JSON_ARRAY_APPEND(JSON '[\"a\", \"b\", \"c\"]', '$', [1, 2]) AS json_data\n\n/*-------------------*\n| json_data         |\n+-------------------+\n| [\"a\",\"b\",\"c\",1,2] |\n*-------------------*/\n\nIn the following example, ` append_each_element ` is ` FALSE ` , so ` [1, 2] `\nis appended as one element.\n\n\nSELECT JSON_ARRAY_APPEND( JSON '[\"a\", \"b\", \"c\"]',\n'$', [1, 2],\nappend_each_element=>FALSE) AS json_data\n\n/*---------------------*\n| json_data           |\n+---------------------+\n| [\"a\",\"b\",\"c\",[1,2]] |\n*---------------------*/\n\nIn the following example, ` append_each_element ` is ` FALSE ` , so ` [1, 2] `\nand ` [3, 4] ` are each appended as one element.\n\n\nSELECT JSON_ARRAY_APPEND( JSON '[\"a\", [\"b\"], \"c\"]',\n'$[1]', [1, 2],\n'$[1][1]', [3, 4],\nappend_each_element=>FALSE) AS json_data\n\n/*-----------------------------*\n| json_data                   |\n+-----------------------------+\n| [\"a\",[\"b\",[1,2,[3,4]]],\"c\"] |\n*-----------------------------*/\n\nIn the following example, the first path ` $[1] ` appends ` [1, 2] ` as single elements, and then the second path ` $[1][1] ` is not a valid path to an array, so the second operation is ignored.\n\n\nSELECT JSON_ARRAY_APPEND( JSON '[\"a\", [\"b\"], \"c\"]',\n'$[1]', [1, 2],\n'$[1][1]', [3, 4]) AS json_data\n\n/*---------------------*\n| json_data           |\n+---------------------+\n| [\"a\",[\"b\",1,2],\"c\"] |\n*---------------------*/\n\nIn the following example, path ` $.a ` is matched and appends ` 2 ` .\n\n\nSELECT JSON_ARRAY_APPEND(JSON '{\"a\": [1]}', '$.a', 2) AS json_data\n\n/*-------------*\n| json_data   |\n+-------------+\n| {\"a\":[1,2]} |\n*-------------*/\n\nIn the following example, a value is appended into a JSON null.\n\n\nSELECT JSON_ARRAY_APPEND(JSON '{\"a\": null}', '$.a', 10)\n\n/*------------*\n| json_data  |\n+------------+\n| {\"a\":[10]} |\n*------------*/\n\nIn the following example, path ` $.a ` is not an array, so the operation is ignored.\n\n\nSELECT JSON_ARRAY_APPEND(JSON '{\"a\": 1}', '$.a', 2) AS json_data\n\n/*-----------*\n| json_data |\n+-----------+\n| {\"a\":1}   |\n*-----------*/\n\nIn the following example, path ` $.b ` does not exist, so the operation is ignored.\n\n\nSELECT JSON_ARRAY_APPEND(JSON '{\"a\": 1}', '$.b', 2) AS json_data\n\n/*-----------*\n| json_data |\n+-----------+\n| {\"a\":1}   |\n*-----------*/"
            },
            "JSON_ARRAY_INSERT": {
                "name": "JSON_ARRAY_INSERT",
                "summary": "Inserts JSON data into a JSON array.",
                "description": "JSON_ARRAY_INSERT( json_expr,\njson_path_value_pair[, ...]\n[, insert_each_element=>{ TRUE | FALSE }]\n)\n\njson_path_value_pair:\njson_path, value\n\nProduces a new JSON value that is created by inserting JSON data into a JSON array.\n\nArguments:\n\n* ` json_expr ` : JSON. For example:\n\nJSON '[\"a\", \"b\", \"c\"]'\n\n* ` json_path_value_pair ` : A value and the  JSONPath  for that value. This includes:\n\n* ` json_path ` : Insert ` value ` at this  JSONPath  in ` json_expr ` .\n\n* ` value ` : A  JSON encoding-supported  value to insert.\n\n* ` insert_each_element ` : An optional, mandatory named argument.\n\n* If ` TRUE ` (default), and ` value ` is a SQL array, inserts each element individually.\n\n* If ` FALSE, ` and ` value ` is a SQL array, inserts the array as one element.\n\nDetails:\n\n* Path value pairs are evaluated left to right. The JSON produced by evaluating one pair becomes the JSON against which the next pair is evaluated.\n* The operation is ignored if the path points to a JSON non-array value that is not a JSON null.\n* If ` json_path ` points to a JSON null, the JSON null is replaced by a JSON array of the appropriate size and padded on the left with JSON nulls.\n* If the path exists but has an incompatible type at any given path token, the path value pair operator is ignored.\n* The function applies all path value pair append operations even if an individual path value pair operation is invalid. For invalid operations, the operation is ignored and the function continues to process the rest of the path value pairs.\n* If the array index in ` json_path ` is larger than the size of the array, the function extends the length of the array to the index, fills in the array with JSON nulls, then adds ` value ` at the index.\n* If any ` json_path ` is an invalid  JSONPath  , an error is produced.\n* If ` json_expr ` is SQL ` NULL ` , the function returns SQL ` NULL ` .\n* If ` insert_each_element ` is SQL ` NULL ` , the function returns ` json_expr ` .\n* If ` json_path ` is SQL ` NULL ` , the ` json_path_value_pair ` operation is ignored.\n\n**Return type**\n\n` JSON `\n\n**Examples**\n\nIn the following example, path ` $[1] ` is matched and inserts ` 1 ` .\n\n\nSELECT JSON_ARRAY_INSERT(JSON '[\"a\", [\"b\", \"c\"], \"d\"]', '$[1]', 1) AS json_data\n\n/*-----------------------*\n| json_data             |\n+-----------------------+\n| [\"a\",1,[\"b\",\"c\"],\"d\"] |\n*-----------------------*/\n\nIn the following example, path ` $[1][0] ` is matched and inserts ` 1 ` .\n\n\nSELECT JSON_ARRAY_INSERT(JSON '[\"a\", [\"b\", \"c\"], \"d\"]', '$[1][0]', 1) AS json_data\n\n/*-----------------------*\n| json_data             |\n+-----------------------+\n| [\"a\",[1,\"b\",\"c\"],\"d\"] |\n*-----------------------*/\n\nIn the following example, ` insert_each_element ` defaults to ` TRUE ` , so `\n[1, 2] ` is inserted as individual elements.\n\n\nSELECT JSON_ARRAY_INSERT(JSON '[\"a\", \"b\", \"c\"]', '$[1]', [1, 2]) AS json_data\n\n/*-------------------*\n| json_data         |\n+-------------------+\n| [\"a\",1,2,\"b\",\"c\"] |\n*-------------------*/\n\nIn the following example, ` insert_each_element ` is ` FALSE ` , so ` [1, 2] `\nis inserted as one element.\n\n\nSELECT JSON_ARRAY_INSERT( JSON '[\"a\", \"b\", \"c\"]',\n'$[1]', [1, 2],\ninsert_each_element=>FALSE) AS json_data\n\n/*---------------------*\n| json_data           |\n+---------------------+\n| [\"a\",[1,2],\"b\",\"c\"] |\n*---------------------*/\n\nIn the following example, path ` $[7] ` is larger than the length of the matched array, so the array is extended with JSON nulls and ` \"e\" ` is inserted at the end of the array.\n\n\nSELECT JSON_ARRAY_INSERT(JSON '[\"a\", \"b\", \"c\", \"d\"]', '$[7]', \"e\") AS json_data\n\n/*--------------------------------------*\n| json_data                            |\n+--------------------------------------+\n| [\"a\",\"b\",\"c\",\"d\",null,null,null,\"e\"] |\n*--------------------------------------*/\n\nIn the following example, path ` $.a ` is an object, so the operation is ignored.\n\n\nSELECT JSON_ARRAY_INSERT(JSON '{\"a\": {}}', '$.a[0]', 2) AS json_data\n\n/*-----------*\n| json_data |\n+-----------+\n| {\"a\":{}}  |\n*-----------*/\n\nIn the following example, path ` $ ` does not specify a valid array position,\nso the operation is ignored.\n\n\nSELECT JSON_ARRAY_INSERT(JSON '[1, 2]', '$', 3) AS json_data\n\n/*-----------*\n| json_data |\n+-----------+\n| [1,2]     |\n*-----------*/\n\nIn the following example, a value is inserted into a JSON null.\n\n\nSELECT JSON_ARRAY_INSERT(JSON '{\"a\": null}', '$.a[2]', 10) AS json_data\n\n/*----------------------*\n| json_data            |\n+----------------------+\n| {\"a\":[null,null,10]} |\n*----------------------*/\n\nIn the following example, the operation is ignored because you can't insert data into a JSON number.\n\n\nSELECT JSON_ARRAY_INSERT(JSON '1', '$[0]', 'r1') AS json_data\n\n/*-----------*\n| json_data |\n+-----------+\n| 1         |\n*-----------*/"
            },
            "JSON_EXTRACT": {
                "name": "JSON_EXTRACT",
                "summary": "(Deprecated) Extracts a JSON value and converts it to a SQL JSON-formatted ` STRING ` or ` JSON ` value.",
                "description": "JSON_EXTRACT(json_string_expr, json_path)\n\n\nJSON_EXTRACT(json_expr, json_path)\n\n**Description**\n\nExtracts a JSON value and converts it to a SQL JSON-formatted ` STRING ` or `\nJSON ` value. This function uses single quotes and brackets to escape invalid JSONPath  characters in JSON keys. For example: ` ['a.b'] ` .\n\nArguments:\n\n* ` json_string_expr ` : A JSON-formatted string. For example:\n\n'{\"class\": {\"students\": [{\"name\": \"Jane\"}]}}'\n\nExtracts a SQL ` NULL ` when a JSON-formatted string ` null ` is encountered.\nFor example:\n\nSELECT JSON_EXTRACT(\"null\", \"$\") -- Returns a SQL NULL\n\n* ` json_expr ` : JSON. For example:\n\nJSON '{\"class\": {\"students\": [{\"name\": \"Jane\"}]}}'\n\nExtracts a JSON ` null ` when a JSON ` null ` is encountered.\n\nSELECT JSON_EXTRACT(JSON 'null', \"$\") -- Returns a JSON 'null'\n\n* ` json_path ` : The  JSONPath  . This identifies the data that you want to obtain from the input.\n\nThere are differences between the JSON-formatted string and JSON input types.\nFor details, see  Differences between the JSON and JSON-formatted STRING types .\n\n**Return type**\n\n* ` json_string_expr ` : A JSON-formatted ` STRING `\n* ` json_expr ` : ` JSON `\n\n**Examples**\n\nIn the following example, JSON data is extracted and returned as JSON.\n\n\nSELECT JSON_EXTRACT(JSON '{\"class\": {\"students\": [{\"id\": 5}, {\"id\": 12}]}}', '$.class') AS json_data;\n\n/*-----------------------------------*\n| json_data                         |\n+-----------------------------------+\n| {\"students\":[{\"id\":5},{\"id\":12}]} |\n*-----------------------------------*/\n\nIn the following examples, JSON data is extracted and returned as JSON-\nformatted strings.\n\n\nSELECT JSON_EXTRACT(json_text, '$') AS json_text_string FROM UNNEST([\n'{\"class\": {\"students\": [{\"name\": \"Jane\"}]}}',\n'{\"class\": {\"students\": []}}',\n'{\"class\": {\"students\": [{\"name\": \"John\"}, {\"name\": \"Jamie\"}]}}'\n]) AS json_text;\n\n/*-----------------------------------------------------------*\n| json_text_string                                          |\n+-----------------------------------------------------------+\n| {\"class\":{\"students\":[{\"name\":\"Jane\"}]}}                  |\n| {\"class\":{\"students\":[]}}                                 |\n| {\"class\":{\"students\":[{\"name\":\"John\"},{\"name\":\"Jamie\"}]}} |\n*-----------------------------------------------------------*/\n\n\nSELECT JSON_EXTRACT(json_text, '$.class.students[0]') AS first_student FROM UNNEST([\n'{\"class\": {\"students\": [{\"name\": \"Jane\"}]}}',\n'{\"class\": {\"students\": []}}',\n'{\"class\": {\"students\": [{\"name\": \"John\"}, {\"name\": \"Jamie\"}]}}'\n]) AS json_text;\n\n/*-----------------*\n| first_student   |\n+-----------------+\n| {\"name\":\"Jane\"} |\n| NULL            |\n| {\"name\":\"John\"} |\n*-----------------*/\n\n\nSELECT JSON_EXTRACT(json_text, '$.class.students[1].name') AS second_student_name FROM UNNEST([\n'{\"class\": {\"students\": [{\"name\": \"Jane\"}]}}',\n'{\"class\": {\"students\": []}}',\n'{\"class\": {\"students\": [{\"name\": \"John\"}, {\"name\": null}]}}',\n'{\"class\": {\"students\": [{\"name\": \"John\"}, {\"name\": \"Jamie\"}]}}'\n]) AS json_text;\n\n/*----------------*\n| second_student |\n+----------------+\n| NULL           |\n| NULL           |\n| NULL           |\n| \"Jamie\"        |\n*----------------*/\n\n\nSELECT JSON_EXTRACT(json_text, \"$.class['students']\") AS student_names FROM UNNEST([\n'{\"class\": {\"students\": [{\"name\": \"Jane\"}]}}',\n'{\"class\": {\"students\": []}}',\n'{\"class\": {\"students\": [{\"name\": \"John\"}, {\"name\": \"Jamie\"}]}}'\n]) AS json_text;\n\n/*------------------------------------*\n| student_names                      |\n+------------------------------------+\n| [{\"name\":\"Jane\"}]                  |\n| []                                 |\n| [{\"name\":\"John\"},{\"name\":\"Jamie\"}] |\n*------------------------------------*/\n\n\nSELECT JSON_EXTRACT('{\"a\": null}', \"$.a\"); -- Returns a SQL NULL SELECT JSON_EXTRACT('{\"a\": null}', \"$.b\"); -- Returns a SQL NULL\n\n\nSELECT JSON_EXTRACT(JSON '{\"a\": null}', \"$.a\"); -- Returns a JSON 'null'\nSELECT JSON_EXTRACT(JSON '{\"a\": null}', \"$.b\"); -- Returns a SQL NULL"
            },
            "JSON_EXTRACT_ARRAY": {
                "name": "JSON_EXTRACT_ARRAY",
                "summary": "(Deprecated) Extracts a JSON array and converts it to a SQL ` ARRAY<JSON-formatted STRING> ` or ` ARRAY<JSON> ` value.",
                "description": "JSON_EXTRACT_ARRAY(json_string_expr[, json_path])\n\n\nJSON_EXTRACT_ARRAY(json_expr[, json_path])\n\n**Description**\n\nExtracts a JSON array and converts it to a SQL ` ARRAY<JSON-formatted STRING>\n` or ` ARRAY<JSON> ` value. This function uses single quotes and brackets to escape invalid  JSONPath  characters in JSON keys. For example: ` ['a.b'] ` .\n\nArguments:\n\n* ` json_string_expr ` : A JSON-formatted string. For example:\n\n'[\"a\", \"b\", {\"key\": \"c\"}]'\n\n* ` json_expr ` : JSON. For example:\n\nJSON '[\"a\", \"b\", {\"key\": \"c\"}]'\n\n* ` json_path ` : The  JSONPath  . This identifies the data that you want to obtain from the input. If this optional parameter is not provided, then the JSONPath ` $ ` symbol is applied, which means that all of the data is analyzed.\n\nThere are differences between the JSON-formatted string and JSON input types.\nFor details, see  Differences between the JSON and JSON-formatted STRING types .\n\n**Return type**\n\n* ` json_string_expr ` : ` ARRAY<JSON-formatted STRING> `\n* ` json_expr ` : ` ARRAY<JSON> `\n\n**Examples**\n\nThis extracts items in JSON to an array of ` JSON ` values:\n\n\nSELECT JSON_EXTRACT_ARRAY( JSON '{\"fruits\":[\"apples\",\"oranges\",\"grapes\"]}','$.fruits'\n) AS json_array;\n\n/*---------------------------------*\n| json_array                      |\n+---------------------------------+\n| [\"apples\", \"oranges\", \"grapes\"] |\n*---------------------------------*/\n\nThis extracts the items in a JSON-formatted string to a string array:\n\n\nSELECT JSON_EXTRACT_ARRAY('[1,2,3]') AS string_array;\n\n/*--------------*\n| string_array |\n+--------------+\n| [1, 2, 3]    |\n*--------------*/\n\nThis extracts a string array and converts it to an integer array:\n\n\nSELECT ARRAY( SELECT CAST(integer_element AS INT64) FROM UNNEST( JSON_EXTRACT_ARRAY('[1,2,3]','$') ) AS integer_element ) AS integer_array;\n\n/*---------------*\n| integer_array |\n+---------------+\n| [1, 2, 3]     |\n*---------------*/\n\nThis extracts string values in a JSON-formatted string to an array:\n\n\n-- Doesn't strip the double quotes SELECT JSON_EXTRACT_ARRAY('[\"apples\", \"oranges\", \"grapes\"]', '$') AS string_array;\n\n/*---------------------------------*\n| string_array                    |\n+---------------------------------+\n| [\"apples\", \"oranges\", \"grapes\"] |\n*---------------------------------*/\n\n-- Strips the double quotes SELECT ARRAY( SELECT JSON_EXTRACT_SCALAR(string_element, '$') FROM UNNEST(JSON_EXTRACT_ARRAY('[\"apples\",\"oranges\",\"grapes\"]','$')) AS string_element ) AS string_array;\n\n/*---------------------------*\n| string_array              |\n+---------------------------+\n| [apples, oranges, grapes] |\n*---------------------------*/\n\nThis extracts only the items in the ` fruit ` property to an array:\n\n\nSELECT JSON_EXTRACT_ARRAY(\n'{\"fruit\": [{\"apples\": 5, \"oranges\": 10}, {\"apples\": 2, \"oranges\": 4}], \"vegetables\": [{\"lettuce\": 7, \"kale\": 8}]}',\n'$.fruit'\n) AS string_array;\n\n/*-------------------------------------------------------*\n| string_array                                          |\n+-------------------------------------------------------+\n| [{\"apples\":5,\"oranges\":10}, {\"apples\":2,\"oranges\":4}] |\n*-------------------------------------------------------*/\n\nThese are equivalent:\n\n\nSELECT JSON_EXTRACT_ARRAY('{\"fruits\": [\"apples\", \"oranges\", \"grapes\"]}', '$[fruits]') AS string_array;\n\nSELECT JSON_EXTRACT_ARRAY('{\"fruits\": [\"apples\", \"oranges\", \"grapes\"]}', '$.fruits') AS string_array;\n\n-- The queries above produce the following result:\n/*---------------------------------*\n| string_array                    |\n+---------------------------------+\n| [\"apples\", \"oranges\", \"grapes\"] |\n*---------------------------------*/\n\nIn cases where a JSON key uses invalid JSONPath characters, you can escape those characters using single quotes and brackets, ` [' '] ` . For example:\n\n\nSELECT JSON_EXTRACT_ARRAY('{\"a.b\": {\"c\": [\"world\"]}}', \"$['a.b'].c\") AS hello;\n\n/*-----------*\n| hello     |\n+-----------+\n| [\"world\"] |\n*-----------*/\n\nThe following examples explore how invalid requests and empty arrays are handled:\n\n* If a JSONPath is invalid, an error is thrown.\n* If a JSON-formatted string is invalid, the output is NULL.\n* It is okay to have empty arrays in the JSON-formatted string.\n\n\n-- An error is thrown if you provide an invalid JSONPath.\nSELECT JSON_EXTRACT_ARRAY('[\"foo\", \"bar\", \"baz\"]', 'INVALID_JSONPath') AS result;\n\n-- If the JSONPath does not refer to an array, then NULL is returned.\nSELECT JSON_EXTRACT_ARRAY('{\"a\": \"foo\"}', '$.a') AS result;\n\n/*--------*\n| result |\n+--------+\n| NULL   |\n*--------*/\n\n-- If a key that does not exist is specified, then the result is NULL.\nSELECT JSON_EXTRACT_ARRAY('{\"a\": \"foo\"}', '$.b') AS result;\n\n/*--------*\n| result |\n+--------+\n| NULL   |\n*--------*/\n\n-- Empty arrays in JSON-formatted strings are supported.\nSELECT JSON_EXTRACT_ARRAY('{\"a\": \"foo\", \"b\": []}', '$.b') AS result;\n\n/*--------*\n| result |\n+--------+\n| []     |\n*--------*/"
            },
            "JSON_EXTRACT_SCALAR": {
                "name": "JSON_EXTRACT_SCALAR",
                "summary": "(Deprecated) Extracts a JSON scalar value and converts it to a SQL ` STRING ` value.",
                "description": "JSON_EXTRACT_SCALAR(json_string_expr[, json_path])\n\n\nJSON_EXTRACT_SCALAR(json_expr[, json_path])\n\n**Description**\n\nExtracts a JSON scalar value and converts it to a SQL ` STRING ` value. In addition, this function:\n\n* Removes the outermost quotes and unescapes the return values.\n* Returns a SQL ` NULL ` if a non-scalar value is selected.\n* Uses single quotes and brackets to escape invalid  JSONPath  characters in JSON keys. For example: ` ['a.b'] ` .\n\nArguments:\n\n* ` json_string_expr ` : A JSON-formatted string. For example:\n\n'{\"name\": \"Jane\", \"age\": \"6\"}'\n\n* ` json_expr ` : JSON. For example:\n\nJSON '{\"name\": \"Jane\", \"age\": \"6\"}'\n\n* ` json_path ` : The  JSONPath  . This identifies the data that you want to obtain from the input. If this optional parameter is not provided, then the JSONPath ` $ ` symbol is applied, which means that all of the data is analyzed.\n\nIf ` json_path ` returns a JSON ` null ` or a non-scalar value (in other words, if ` json_path ` refers to an object or an array), then a SQL ` NULL `\nis returned.\n\nThere are differences between the JSON-formatted string and JSON input types.\nFor details, see  Differences between the JSON and JSON-formatted STRING types .\n\n**Return type**\n\n` STRING `\n\n**Examples**\n\nIn the following example, ` age ` is extracted.\n\n\nSELECT JSON_EXTRACT_SCALAR(JSON '{\"name\": \"Jakob\", \"age\": \"6\" }', '$.age') AS scalar_age;\n\n/*------------*\n| scalar_age |\n+------------+\n| 6          |\n*------------*/\n\nThe following example compares how results are returned for the ` JSON_EXTRACT\n` and ` JSON_EXTRACT_SCALAR ` functions.\n\n\nSELECT JSON_EXTRACT('{\"name\": \"Jakob\", \"age\": \"6\" }', '$.name') AS json_name,\nJSON_EXTRACT_SCALAR('{\"name\": \"Jakob\", \"age\": \"6\" }', '$.name') AS scalar_name,\nJSON_EXTRACT('{\"name\": \"Jakob\", \"age\": \"6\" }', '$.age') AS json_age,\nJSON_EXTRACT_SCALAR('{\"name\": \"Jakob\", \"age\": \"6\" }', '$.age') AS scalar_age;\n\n/*-----------+-------------+----------+------------*\n| json_name | scalar_name | json_age | scalar_age |\n+-----------+-------------+----------+------------+\n| \"Jakob\"   | Jakob       | \"6\"      | 6          |\n*-----------+-------------+----------+------------*/\n\n\nSELECT JSON_EXTRACT('{\"fruits\": [\"apple\", \"banana\"]}', '$.fruits') AS json_extract,\nJSON_EXTRACT_SCALAR('{\"fruits\": [\"apple\", \"banana\"]}', '$.fruits') AS json_extract_scalar;\n\n/*--------------------+---------------------*\n| json_extract       | json_extract_scalar |\n+--------------------+---------------------+\n| [\"apple\",\"banana\"] | NULL                |\n*--------------------+---------------------*/\n\nIn cases where a JSON key uses invalid JSONPath characters, you can escape those characters using single quotes and brackets, ` [' '] ` . For example:\n\n\nSELECT JSON_EXTRACT_SCALAR('{\"a.b\": {\"c\": \"world\"}}', \"$['a.b'].c\") AS hello;\n\n/*-------*\n| hello |\n+-------+\n| world |\n*-------*/"
            },
            "JSON_EXTRACT_STRING_ARRAY": {
                "name": "JSON_EXTRACT_STRING_ARRAY",
                "summary": "(Deprecated) Extracts a JSON array of scalar values and converts it to a SQL ` ARRAY<STRING> ` value.",
                "description": "JSON_EXTRACT_STRING_ARRAY(json_string_expr[, json_path])\n\n\nJSON_EXTRACT_STRING_ARRAY(json_expr[, json_path])\n\n**Description**\n\nExtracts a JSON array of scalar values and converts it to a SQL `\nARRAY<STRING> ` value. In addition, this function:\n\n* Removes the outermost quotes and unescapes the values.\n* Returns a SQL ` NULL ` if the selected value is not an array or not an array containing only scalar values.\n* Uses single quotes and brackets to escape invalid  JSONPath  characters in JSON keys. For example: ` ['a.b'] ` .\n\nArguments:\n\n* ` json_string_expr ` : A JSON-formatted string. For example:\n\n'[\"apples\", \"oranges\", \"grapes\"]'\n\n* ` json_expr ` : JSON. For example:\n\nJSON '[\"apples\", \"oranges\", \"grapes\"]'\n\n* ` json_path ` : The  JSONPath  . This identifies the data that you want to obtain from the input. If this optional parameter is not provided, then the JSONPath ` $ ` symbol is applied, which means that all of the data is analyzed.\n\nThere are differences between the JSON-formatted string and JSON input types.\nFor details, see  Differences between the JSON and JSON-formatted STRING types .\n\nCaveats:\n\n* A JSON ` null ` in the input array produces a SQL ` NULL ` as the output for that JSON ` null ` . If the output contains a ` NULL ` array element, an error is produced because the final output cannot be an array with ` NULL ` values.\n* If a JSONPath matches an array that contains scalar objects and a JSON ` null ` , then the output of the function must be transformed because the final output cannot be an array with ` NULL ` values.\n\n**Return type**\n\n` ARRAY<STRING> `\n\n**Examples**\n\nThis extracts items in JSON to a string array:\n\n\nSELECT JSON_EXTRACT_STRING_ARRAY( JSON '{\"fruits\": [\"apples\", \"oranges\", \"grapes\"]}', '$.fruits'\n) AS string_array;\n\n/*---------------------------*\n| string_array              |\n+---------------------------+\n| [apples, oranges, grapes] |\n*---------------------------*/\n\nThe following example compares how results are returned for the `\nJSON_EXTRACT_ARRAY ` and ` JSON_EXTRACT_STRING_ARRAY ` functions.\n\n\nSELECT JSON_EXTRACT_ARRAY('[\"apples\", \"oranges\"]') AS json_array,\nJSON_EXTRACT_STRING_ARRAY('[\"apples\", \"oranges\"]') AS string_array;\n\n/*-----------------------+-------------------*\n| json_array            | string_array      |\n+-----------------------+-------------------+\n| [\"apples\", \"oranges\"] | [apples, oranges] |\n*-----------------------+-------------------*/\n\nThis extracts the items in a JSON-formatted string to a string array:\n\n\n-- Strips the double quotes SELECT JSON_EXTRACT_STRING_ARRAY('[\"foo\", \"bar\", \"baz\"]', '$') AS string_array;\n\n/*-----------------*\n| string_array    |\n+-----------------+\n| [foo, bar, baz] |\n*-----------------*/\n\nThis extracts a string array and converts it to an integer array:\n\n\nSELECT ARRAY( SELECT CAST(integer_element AS INT64) FROM UNNEST( JSON_EXTRACT_STRING_ARRAY('[1, 2, 3]', '$') ) AS integer_element ) AS integer_array;\n\n/*---------------*\n| integer_array |\n+---------------+\n| [1, 2, 3]     |\n*---------------*/\n\nThese are equivalent:\n\n\nSELECT JSON_EXTRACT_STRING_ARRAY('{\"fruits\": [\"apples\", \"oranges\", \"grapes\"]}', '$[fruits]') AS string_array;\n\nSELECT JSON_EXTRACT_STRING_ARRAY('{\"fruits\": [\"apples\", \"oranges\", \"grapes\"]}', '$.fruits') AS string_array;\n\n-- The queries above produce the following result:\n/*---------------------------*\n| string_array              |\n+---------------------------+\n| [apples, oranges, grapes] |\n*---------------------------*/\n\nIn cases where a JSON key uses invalid JSONPath characters, you can escape those characters using single quotes and brackets: ` [' '] ` . For example:\n\n\nSELECT JSON_EXTRACT_STRING_ARRAY('{\"a.b\": {\"c\": [\"world\"]}}', \"$['a.b'].c\") AS hello;\n\n/*---------*\n| hello   |\n+---------+\n| [world] |\n*---------*/\n\nThe following examples explore how invalid requests and empty arrays are handled:\n\n\n-- An error is thrown if you provide an invalid JSONPath.\nSELECT JSON_EXTRACT_STRING_ARRAY('[\"foo\", \"bar\", \"baz\"]', 'INVALID_JSONPath') AS result;\n\n-- If the JSON formatted string is invalid, then NULL is returned.\nSELECT JSON_EXTRACT_STRING_ARRAY('}}', '$') AS result;\n\n/*--------*\n| result |\n+--------+\n| NULL   |\n*--------*/\n\n-- If the JSON document is NULL, then NULL is returned.\nSELECT JSON_EXTRACT_STRING_ARRAY(NULL, '$') AS result;\n\n/*--------*\n| result |\n+--------+\n| NULL   |\n*--------*/\n\n-- If a JSONPath does not match anything, then the output is NULL.\nSELECT JSON_EXTRACT_STRING_ARRAY('{\"a\": [\"foo\", \"bar\", \"baz\"]}', '$.b') AS result;\n\n/*--------*\n| result |\n+--------+\n| NULL   |\n*--------*/\n\n-- If a JSONPath matches an object that is not an array, then the output is NULL.\nSELECT JSON_EXTRACT_STRING_ARRAY('{\"a\": \"foo\"}', '$') AS result;\n\n/*--------*\n| result |\n+--------+\n| NULL   |\n*--------*/\n\n-- If a JSONPath matches an array of non-scalar objects, then the output is NULL.\nSELECT JSON_EXTRACT_STRING_ARRAY('{\"a\": [{\"b\": \"foo\", \"c\": 1}, {\"b\": \"bar\", \"c\":2}], \"d\": \"baz\"}', '$.a') AS result;\n\n/*--------*\n| result |\n+--------+\n| NULL   |\n*--------*/\n\n-- If a JSONPath matches an array of mixed scalar and non-scalar objects, then the output is NULL.\nSELECT JSON_EXTRACT_STRING_ARRAY('{\"a\": [10, {\"b\": 20}]', '$.a') AS result;\n\n/*--------*\n| result |\n+--------+\n| NULL   |\n*--------*/\n\n-- If a JSONPath matches an empty JSON array, then the output is an empty array instead of NULL.\nSELECT JSON_EXTRACT_STRING_ARRAY('{\"a\": \"foo\", \"b\": []}', '$.b') AS result;\n\n/*--------*\n| result |\n+--------+\n| []     |\n*--------*/\n\n-- The following query produces and error because the final output cannot be an\n-- array with NULLs.\nSELECT JSON_EXTRACT_STRING_ARRAY('[\"world\", 1, null]') AS result;"
            },
            "JSON_OBJECT": {
                "name": "JSON_OBJECT",
                "summary": "Creates a JSON object.",
                "description": "* Signature 1  : ` JSON_OBJECT([json_key, json_value][, ...]) `\n* Signature 2  : ` JSON_OBJECT(json_key_array, json_value_array) `\n\n####  Signature 1\n\n\nJSON_OBJECT([json_key, json_value][, ...])\n\n**Description**\n\nCreates a JSON object, using key-value pairs.\n\nArguments:\n\n* ` json_key ` : A ` STRING ` value that represents a key.\n* ` json_value ` : A  JSON encoding-supported  value.\n\nDetails:\n\n* If two keys are passed in with the same name, only the first key-value pair is preserved.\n* The order of key-value pairs is not preserved.\n* If ` json_key ` is ` NULL ` , an error is produced.\n\n**Return type**\n\n` JSON `\n\n**Examples**\n\nYou can create an empty JSON object by passing in no JSON keys and values. For example:\n\n\nSELECT JSON_OBJECT() AS json_data\n\n/*-----------*\n| json_data |\n+-----------+\n| {}        |\n*-----------*/\n\nYou can create a JSON object by passing in key-value pairs. For example:\n\n\nSELECT JSON_OBJECT('foo', 10, 'bar', TRUE) AS json_data\n\n/*-----------------------*\n| json_data             |\n+-----------------------+\n| {\"bar\":true,\"foo\":10} |\n*-----------------------*/\n\n\nSELECT JSON_OBJECT('foo', 10, 'bar', ['a', 'b']) AS json_data\n\n/*----------------------------*\n| json_data                  |\n+----------------------------+\n| {\"bar\":[\"a\",\"b\"],\"foo\":10} |\n*----------------------------*/\n\n\nSELECT JSON_OBJECT('a', NULL, 'b', JSON 'null') AS json_data\n\n/*---------------------*\n| json_data           |\n+---------------------+\n| {\"a\":null,\"b\":null} |\n*---------------------*/\n\n\nSELECT JSON_OBJECT('a', 10, 'a', 'foo') AS json_data\n\n/*-----------*\n| json_data |\n+-----------+\n| {\"a\":10}  |\n*-----------*/\n\n\nWITH Items AS (SELECT 'hello' AS key, 'world' AS value) SELECT JSON_OBJECT(key, value) AS json_data FROM Items\n\n/*-------------------*\n| json_data         |\n+-------------------+\n| {\"hello\":\"world\"} |\n*-------------------*/\n\nAn error is produced if a SQL ` NULL ` is passed in for a JSON key.\n\n\n-- Error: A key cannot be NULL.\nSELECT JSON_OBJECT(NULL, 1) AS json_data\n\nAn error is produced if the number of JSON keys and JSON values don't match:\n\n\n-- Error: No matching signature for function JSON_OBJECT for argument types:\n-- STRING, INT64, STRING SELECT JSON_OBJECT('a', 1, 'b') AS json_data\n\n####  Signature 2\n\n\nJSON_OBJECT(json_key_array, json_value_array)\n\nCreates a JSON object, using an array of keys and values.\n\nArguments:\n\n* ` json_key_array ` : An array of zero or more ` STRING ` keys.\n* ` json_value_array ` : An array of zero or more  JSON encoding-supported  values.\n\nDetails:\n\n* If two keys are passed in with the same name, only the first key-value pair is preserved.\n* The order of key-value pairs is not preserved.\n* The number of keys must match the number of values, otherwise an error is produced.\n* If any argument is ` NULL ` , an error is produced.\n* If a key in ` json_key_array ` is ` NULL ` , an error is produced.\n\n**Return type**\n\n` JSON `\n\n**Examples**\n\nYou can create an empty JSON object by passing in an empty array of keys and values. For example:\n\n\nSELECT JSON_OBJECT(CAST([] AS ARRAY<STRING>), []) AS json_data\n\n/*-----------*\n| json_data |\n+-----------+\n| {}        |\n*-----------*/\n\nYou can create a JSON object by passing in an array of keys and an array of values. For example:\n\n\nSELECT JSON_OBJECT(['a', 'b'], [10, NULL]) AS json_data\n\n/*-------------------*\n| json_data         |\n+-------------------+\n| {\"a\":10,\"b\":null} |\n*-------------------*/\n\n\nSELECT JSON_OBJECT(['a', 'b'], [JSON '10', JSON '\"foo\"']) AS json_data\n\n/*--------------------*\n| json_data          |\n+--------------------+\n| {\"a\":10,\"b\":\"foo\"} |\n*--------------------*/\n\n\nSELECT JSON_OBJECT(\n['a', 'b'],\n[STRUCT(10 AS id, 'Red' AS color), STRUCT(20 AS id, 'Blue' AS color)]) AS json_data\n\n/*------------------------------------------------------------*\n| json_data                                                  |\n+------------------------------------------------------------+\n| {\"a\":{\"color\":\"Red\",\"id\":10},\"b\":{\"color\":\"Blue\",\"id\":20}} |\n*------------------------------------------------------------*/\n\n\nSELECT JSON_OBJECT(\n['a', 'b'],\n[TO_JSON(10), TO_JSON(['foo', 'bar'])]) AS json_data\n\n/*----------------------------*\n| json_data                  |\n+----------------------------+\n| {\"a\":10,\"b\":[\"foo\",\"bar\"]} |\n*----------------------------*/\n\nThe following query groups by ` id ` and then creates an array of keys and values from the rows with the same ` id ` :\n\n\nWITH Fruits AS ( SELECT 0 AS id, 'color' AS json_key, 'red' AS json_value UNION ALL SELECT 0, 'fruit', 'apple' UNION ALL SELECT 1, 'fruit', 'banana' UNION ALL SELECT 1, 'ripe', 'true'\n) SELECT JSON_OBJECT(ARRAY_AGG(json_key), ARRAY_AGG(json_value)) AS json_data FROM Fruits GROUP BY id\n\n/*----------------------------------*\n| json_data                        |\n+----------------------------------+\n| {\"color\":\"red\",\"fruit\":\"apple\"}  |\n| {\"fruit\":\"banana\",\"ripe\":\"true\"} |\n*----------------------------------*/\n\nAn error is produced if the size of the JSON keys and values arrays don't match:\n\n\n-- Error: The number of keys and values must match.\nSELECT JSON_OBJECT(['a', 'b'], [10]) AS json_data\n\nAn error is produced if the array of JSON keys or JSON values is a SQL ` NULL\n` .\n\n\n-- Error: The keys array cannot be NULL.\nSELECT JSON_OBJECT(CAST(NULL AS ARRAY<STRING>), [10, 20]) AS json_data\n\n\n-- Error: The values array cannot be NULL.\nSELECT JSON_OBJECT(['a', 'b'], CAST(NULL AS ARRAY<INT64>)) AS json_data"
            },
            "JSON_QUERY": {
                "name": "JSON_QUERY",
                "summary": "Extracts a JSON value and converts it to a SQL JSON-\nformatted ` STRING ` or ` JSON ` value.",
                "description": "JSON_QUERY(json_string_expr, json_path)\n\n\nJSON_QUERY(json_expr, json_path)\n\n**Description**\n\nExtracts a JSON value and converts it to a SQL JSON-formatted ` STRING ` or `\nJSON ` value. This function uses double quotes to escape invalid  JSONPath characters in JSON keys. For example: ` \"a.b\" ` .\n\nArguments:\n\n* ` json_string_expr ` : A JSON-formatted string. For example:\n\n'{\"class\": {\"students\": [{\"name\": \"Jane\"}]}}'\n\nExtracts a SQL ` NULL ` when a JSON-formatted string ` null ` is encountered.\nFor example:\n\nSELECT JSON_QUERY(\"null\", \"$\") -- Returns a SQL NULL\n\n* ` json_expr ` : JSON. For example:\n\nJSON '{\"class\": {\"students\": [{\"name\": \"Jane\"}]}}'\n\nExtracts a JSON ` null ` when a JSON ` null ` is encountered.\n\nSELECT JSON_QUERY(JSON 'null', \"$\") -- Returns a JSON 'null'\n\n* ` json_path ` : The  JSONPath  . This identifies the data that you want to obtain from the input.\n\nThere are differences between the JSON-formatted string and JSON input types.\nFor details, see  Differences between the JSON and JSON-formatted STRING types .\n\n**Return type**\n\n* ` json_string_expr ` : A JSON-formatted ` STRING `\n* ` json_expr ` : ` JSON `\n\n**Examples**\n\nIn the following example, JSON data is extracted and returned as JSON.\n\n\nSELECT JSON_QUERY(JSON '{\"class\": {\"students\": [{\"id\": 5}, {\"id\": 12}]}}', '$.class') AS json_data;\n\n/*-----------------------------------*\n| json_data                         |\n+-----------------------------------+\n| {\"students\":[{\"id\":5},{\"id\":12}]} |\n*-----------------------------------*/\n\nIn the following examples, JSON data is extracted and returned as JSON-\nformatted strings.\n\n\nSELECT JSON_QUERY(json_text, '$') AS json_text_string FROM UNNEST([\n'{\"class\": {\"students\": [{\"name\": \"Jane\"}]}}',\n'{\"class\": {\"students\": []}}',\n'{\"class\": {\"students\": [{\"name\": \"John\"}, {\"name\": \"Jamie\"}]}}'\n]) AS json_text;\n\n/*-----------------------------------------------------------*\n| json_text_string                                          |\n+-----------------------------------------------------------+\n| {\"class\":{\"students\":[{\"name\":\"Jane\"}]}}                  |\n| {\"class\":{\"students\":[]}}                                 |\n| {\"class\":{\"students\":[{\"name\":\"John\"},{\"name\":\"Jamie\"}]}} |\n*-----------------------------------------------------------*/\n\n\nSELECT JSON_QUERY(json_text, '$.class.students[0]') AS first_student FROM UNNEST([\n'{\"class\": {\"students\": [{\"name\": \"Jane\"}]}}',\n'{\"class\": {\"students\": []}}',\n'{\"class\": {\"students\": [{\"name\": \"John\"}, {\"name\": \"Jamie\"}]}}'\n]) AS json_text;\n\n/*-----------------*\n| first_student   |\n+-----------------+\n| {\"name\":\"Jane\"} |\n| NULL            |\n| {\"name\":\"John\"} |\n*-----------------*/\n\n\nSELECT JSON_QUERY(json_text, '$.class.students[1].name') AS second_student_name FROM UNNEST([\n'{\"class\": {\"students\": [{\"name\": \"Jane\"}]}}',\n'{\"class\": {\"students\": []}}',\n'{\"class\": {\"students\": [{\"name\": \"John\"}, {\"name\": null}]}}',\n'{\"class\": {\"students\": [{\"name\": \"John\"}, {\"name\": \"Jamie\"}]}}'\n]) AS json_text;\n\n/*----------------*\n| second_student |\n+----------------+\n| NULL           |\n| NULL           |\n| NULL           |\n| \"Jamie\"        |\n*----------------*/\n\n\nSELECT JSON_QUERY(json_text, '$.class.\"students\"') AS student_names FROM UNNEST([\n'{\"class\": {\"students\": [{\"name\": \"Jane\"}]}}',\n'{\"class\": {\"students\": []}}',\n'{\"class\": {\"students\": [{\"name\": \"John\"}, {\"name\": \"Jamie\"}]}}'\n]) AS json_text;\n\n/*------------------------------------*\n| student_names                      |\n+------------------------------------+\n| [{\"name\":\"Jane\"}]                  |\n| []                                 |\n| [{\"name\":\"John\"},{\"name\":\"Jamie\"}] |\n*------------------------------------*/\n\n\nSELECT JSON_QUERY('{\"a\": null}', \"$.a\"); -- Returns a SQL NULL SELECT JSON_QUERY('{\"a\": null}', \"$.b\"); -- Returns a SQL NULL\n\n\nSELECT JSON_QUERY(JSON '{\"a\": null}', \"$.a\"); -- Returns a JSON 'null'\nSELECT JSON_QUERY(JSON '{\"a\": null}', \"$.b\"); -- Returns a SQL NULL"
            },
            "JSON_QUERY_ARRAY": {
                "name": "JSON_QUERY_ARRAY",
                "summary": "Extracts a JSON array and converts it to a SQL `\nARRAY<JSON-formatted STRING> ` or ` ARRAY<JSON> ` value.",
                "description": "JSON_QUERY_ARRAY(json_string_expr[, json_path])\n\n\nJSON_QUERY_ARRAY(json_expr[, json_path])\n\n**Description**\n\nExtracts a JSON array and converts it to a SQL ` ARRAY<JSON-formatted STRING>\n` or ` ARRAY<JSON> ` value. In addition, this function uses double quotes to escape invalid  JSONPath  characters in JSON keys. For example: ` \"a.b\" ` .\n\nArguments:\n\n* ` json_string_expr ` : A JSON-formatted string. For example:\n\n'[\"a\", \"b\", {\"key\": \"c\"}]'\n\n* ` json_expr ` : JSON. For example:\n\nJSON '[\"a\", \"b\", {\"key\": \"c\"}]'\n\n* ` json_path ` : The  JSONPath  . This identifies the data that you want to obtain from the input. If this optional parameter is not provided, then the JSONPath ` $ ` symbol is applied, which means that all of the data is analyzed.\n\nThere are differences between the JSON-formatted string and JSON input types.\nFor details, see  Differences between the JSON and JSON-formatted STRING types .\n\n**Return type**\n\n* ` json_string_expr ` : ` ARRAY<JSON-formatted STRING> `\n* ` json_expr ` : ` ARRAY<JSON> `\n\n**Examples**\n\nThis extracts items in JSON to an array of ` JSON ` values:\n\n\nSELECT JSON_QUERY_ARRAY( JSON '{\"fruits\": [\"apples\", \"oranges\", \"grapes\"]}', '$.fruits'\n) AS json_array;\n\n/*---------------------------------*\n| json_array                      |\n+---------------------------------+\n| [\"apples\", \"oranges\", \"grapes\"] |\n*---------------------------------*/\n\nThis extracts the items in a JSON-formatted string to a string array:\n\n\nSELECT JSON_QUERY_ARRAY('[1, 2, 3]') AS string_array;\n\n/*--------------*\n| string_array |\n+--------------+\n| [1, 2, 3]    |\n*--------------*/\n\nThis extracts a string array and converts it to an integer array:\n\n\nSELECT ARRAY( SELECT CAST(integer_element AS INT64) FROM UNNEST( JSON_QUERY_ARRAY('[1, 2, 3]','$') ) AS integer_element ) AS integer_array;\n\n/*---------------*\n| integer_array |\n+---------------+\n| [1, 2, 3]     |\n*---------------*/\n\nThis extracts string values in a JSON-formatted string to an array:\n\n\n-- Doesn't strip the double quotes SELECT JSON_QUERY_ARRAY('[\"apples\", \"oranges\", \"grapes\"]', '$') AS string_array;\n\n/*---------------------------------*\n| string_array                    |\n+---------------------------------+\n| [\"apples\", \"oranges\", \"grapes\"] |\n*---------------------------------*/\n\n-- Strips the double quotes SELECT ARRAY( SELECT JSON_VALUE(string_element, '$') FROM UNNEST(JSON_QUERY_ARRAY('[\"apples\", \"oranges\", \"grapes\"]', '$')) AS string_element ) AS string_array;\n\n/*---------------------------*\n| string_array              |\n+---------------------------+\n| [apples, oranges, grapes] |\n*---------------------------*/\n\nThis extracts only the items in the ` fruit ` property to an array:\n\n\nSELECT JSON_QUERY_ARRAY(\n'{\"fruit\": [{\"apples\": 5, \"oranges\": 10}, {\"apples\": 2, \"oranges\": 4}], \"vegetables\": [{\"lettuce\": 7, \"kale\": 8}]}',\n'$.fruit'\n) AS string_array;\n\n/*-------------------------------------------------------*\n| string_array                                          |\n+-------------------------------------------------------+\n| [{\"apples\":5,\"oranges\":10}, {\"apples\":2,\"oranges\":4}] |\n*-------------------------------------------------------*/\n\nThese are equivalent:\n\n\nSELECT JSON_QUERY_ARRAY('{\"fruits\": [\"apples\", \"oranges\", \"grapes\"]}', '$.fruits') AS string_array;\n\nSELECT JSON_QUERY_ARRAY('{\"fruits\": [\"apples\", \"oranges\", \"grapes\"]}', '$.\"fruits\"') AS string_array;\n\n-- The queries above produce the following result:\n/*---------------------------------*\n| string_array                    |\n+---------------------------------+\n| [\"apples\", \"oranges\", \"grapes\"] |\n*---------------------------------*/\n\nIn cases where a JSON key uses invalid JSONPath characters, you can escape those characters using double quotes: ` \" \" ` . For example:\n\n\nSELECT JSON_QUERY_ARRAY('{\"a.b\": {\"c\": [\"world\"]}}', '$.\"a.b\".c') AS hello;\n\n/*-----------*\n| hello     |\n+-----------+\n| [\"world\"] |\n*-----------*/\n\nThe following examples show how invalid requests and empty arrays are handled:\n\n\n-- An error is returned if you provide an invalid JSONPath.\nSELECT JSON_QUERY_ARRAY('[\"foo\", \"bar\", \"baz\"]', 'INVALID_JSONPath') AS result;\n\n-- If the JSONPath does not refer to an array, then NULL is returned.\nSELECT JSON_QUERY_ARRAY('{\"a\": \"foo\"}', '$.a') AS result;\n\n/*--------*\n| result |\n+--------+\n| NULL   |\n*--------*/\n\n-- If a key that does not exist is specified, then the result is NULL.\nSELECT JSON_QUERY_ARRAY('{\"a\": \"foo\"}', '$.b') AS result;\n\n/*--------*\n| result |\n+--------+\n| NULL   |\n*--------*/\n\n-- Empty arrays in JSON-formatted strings are supported.\nSELECT JSON_QUERY_ARRAY('{\"a\": \"foo\", \"b\": []}', '$.b') AS result;\n\n/*--------*\n| result |\n+--------+\n| []     |\n*--------*/"
            },
            "JSON_REMOVE": {
                "name": "JSON_REMOVE",
                "summary": "Produces JSON with the specified JSON data removed.",
                "description": "JSON_REMOVE(json_expr, json_path[, ...])\n\nProduces a new SQL ` JSON ` value with the specified JSON data removed.\n\nArguments:\n\n* ` json_expr ` : JSON. For example:\n\nJSON '{\"class\": {\"students\": [{\"name\": \"Jane\"}]}}'\n\n* ` json_path ` : Remove data at this  JSONPath  in ` json_expr ` .\n\nDetails:\n\n* Paths are evaluated left to right. The JSON produced by evaluating the first path is the JSON for the next path.\n* The operation ignores non-existent paths and continue processing the rest of the paths.\n* For each path, the entire matched JSON subtree is deleted.\n* If the path matches a JSON object key, this function deletes the key-value pair.\n* If the path matches an array element, this function deletes the specific element from the matched array.\n* If removing the path results in an empty JSON object or empty JSON array, the empty structure is preserved.\n* If ` json_path ` is ` $ ` or an invalid  JSONPath  , an error is produced.\n* If ` json_path ` is SQL ` NULL ` , the path operation is ignored.\n\n**Return type**\n\n` JSON `\n\n**Examples**\n\nIn the following example, the path ` $[1] ` is matched and removes ` [\"b\",\n\"c\"] ` .\n\n\nSELECT JSON_REMOVE(JSON '[\"a\", [\"b\", \"c\"], \"d\"]', '$[1]') AS json_data\n\n/*-----------*\n| json_data |\n+-----------+\n| [\"a\",\"d\"] |\n*-----------*/\n\nYou can use the field access operator to pass JSON data into this function.\nFor example:\n\n\nWITH T AS (SELECT JSON '{\"a\": {\"b\": 10, \"c\": 20}}' AS data) SELECT JSON_REMOVE(data.a, '$.b') AS json_data FROM T\n\n/*-----------*\n| json_data |\n+-----------+\n| {\"c\":20}  |\n*-----------*/\n\nIn the following example, the first path ` $[1] ` is matched and removes `\n[\"b\", \"c\"] ` . Then, the second path ` $[1] ` is matched and removes ` \"d\" ` .\n\n\nSELECT JSON_REMOVE(JSON '[\"a\", [\"b\", \"c\"], \"d\"]', '$[1]', '$[1]') AS json_data\n\n/*-----------*\n| json_data |\n+-----------+\n| [\"a\"]     |\n*-----------*/\n\nThe structure of an empty array is preserved when all elements are deleted from it. For example:\n\n\nSELECT JSON_REMOVE(JSON '[\"a\", [\"b\", \"c\"], \"d\"]', '$[1]', '$[1]', '$[0]') AS json_data\n\n/*-----------*\n| json_data |\n+-----------+\n| []        |\n*-----------*/\n\nIn the following example, the path ` $.a.b.c ` is matched and removes the `\n\"c\":\"d\" ` key-value pair from the JSON object.\n\n\nSELECT JSON_REMOVE(JSON '{\"a\": {\"b\": {\"c\": \"d\"}}}', '$.a.b.c') AS json_data\n\n/*----------------*\n| json_data      |\n+----------------+\n| {\"a\":{\"b\":{}}} |\n*----------------*/\n\nIn the following example, the path ` $.a.b ` is matched and removes the ` \"b\":\n{\"c\":\"d\"} ` key-value pair from the JSON object.\n\n\nSELECT JSON_REMOVE(JSON '{\"a\": {\"b\": {\"c\": \"d\"}}}', '$.a.b') AS json_data\n\n/*-----------*\n| json_data |\n+-----------+\n| {\"a\":{}}  |\n*-----------*/\n\nIn the following example, the path ` $.b ` is not valid, so the operation makes no changes.\n\n\nSELECT JSON_REMOVE(JSON '{\"a\": 1}', '$.b') AS json_data\n\n/*-----------*\n| json_data |\n+-----------+\n| {\"a\":1}   |\n*-----------*/\n\nIn the following example, path ` $.a.b ` and ` $.b ` don't exist, so those operations are ignored, but the others are processed.\n\n\nSELECT JSON_REMOVE(JSON '{\"a\": [1, 2, 3]}', '$.a[0]', '$.a.b', '$.b', '$.a[0]') AS json_data\n\n/*-----------*\n| json_data |\n+-----------+\n| {\"a\":[3]} |\n*-----------*/\n\nIf you pass in ` $ ` as the path, an error is produced. For example:\n\n\n-- Error: The JSONPath cannot be '$'\nSELECT JSON_REMOVE(JSON '{}', '$') AS json_data\n\nIn the following example, the operation is ignored because you can't remove data from a JSON null.\n\n\nSELECT JSON_REMOVE(JSON 'null', '$.a.b') AS json_data\n\n/*-----------*\n| json_data |\n+-----------+\n| null      |\n*-----------*/"
            },
            "JSON_SET": {
                "name": "JSON_SET",
                "summary": "Inserts or replaces JSON data.",
                "description": "JSON_SET( json_expr,\njson_path_value_pair[, ...]\n[, create_if_missing=> { TRUE | FALSE }]\n)\n\njson_path_value_pair:\njson_path, value\n\nProduces a new SQL ` JSON ` value with the specified JSON data inserted or replaced.\n\nArguments:\n\n* ` json_expr ` : JSON. For example:\n\nJSON '{\"class\": {\"students\": [{\"name\": \"Jane\"}]}}'\n\n* ` json_path_value_pair ` : A value and the  JSONPath  for that value. This includes:\n\n* ` json_path ` : Insert or replace ` value ` at this  JSONPath  in ` json_expr ` .\n\n* ` value ` : A  JSON encoding-supported  value to insert.\n\n* ` create_if_missing ` : An optional, mandatory named argument.\n\n* If TRUE (default), replaces or inserts data if the path does not exist.\n\n* If FALSE, only _existing_ JSONPath values are replaced. If the path doesn't exist, the set operation is ignored.\n\nDetails:\n\n* Path value pairs are evaluated left to right. The JSON produced by evaluating one pair becomes the JSON against which the next pair is evaluated.\n* If a matched path has an existing value, it overwrites the existing data with ` value ` .\n* If ` create_if_missing ` is ` TRUE ` :\n\n* If a path doesn't exist, the remainder of the path is recursively created.\n* If the matched path prefix points to a JSON null, the remainder of the path is recursively created, and ` value ` is inserted.\n* If a path token points to a JSON array and the specified index is _larger_ than the size of the array, pads the JSON array with JSON nulls, recursively creates the remainder of the path at the specified index, and inserts the path value pair.\n* This function applies all path value pair set operations even if an individual path value pair operation is invalid. For invalid operations, the operation is ignored and the function continues to process the rest of the path value pairs.\n\n* If the path exists but has an incompatible type at any given path token, no update happens for that specific path value pair.\n\n* If any ` json_path ` is an invalid  JSONPath  , an error is produced.\n\n* If ` json_expr ` is SQL ` NULL ` , the function returns SQL ` NULL ` .\n\n* If ` json_path ` is SQL ` NULL ` , the ` json_path_value_pair ` operation is ignored.\n\n* If ` create_if_missing ` is SQL ` NULL ` , the set operation is ignored.\n\n**Return type**\n\n` JSON `\n\n**Examples**\n\nIn the following example, the path ` $ ` matches the entire ` JSON ` value and replaces it with ` {\"b\": 2, \"c\": 3} ` .\n\n\nSELECT JSON_SET(JSON '{\"a\": 1}', '$', JSON '{\"b\": 2, \"c\": 3}') AS json_data\n\n/*---------------*\n| json_data     |\n+---------------+\n| {\"b\":2,\"c\":3} |\n*---------------*/\n\nIn the following example, ` create_if_missing ` is ` FALSE ` and the path `\n$.b ` doesn't exist, so the set operation is ignored.\n\n\nSELECT JSON_SET( JSON '{\"a\": 1}',\n\"$.b\", 999,\ncreate_if_missing => false) AS json_data\n\n/*------------*\n| json_data  |\n+------------+\n| '{\"a\": 1}' |\n*------------*/\n\nIn the following example, ` create_if_missing ` is ` TRUE ` and the path ` $.a\n` exists, so the value is replaced.\n\n\nSELECT JSON_SET( JSON '{\"a\": 1}',\n\"$.a\", 999,\ncreate_if_missing => false) AS json_data\n\n/*--------------*\n| json_data    |\n+--------------+\n| '{\"a\": 999}' |\n*--------------*/\n\nIn the following example, the path ` $.a ` is matched, but ` $.a.b ` does not exist, so the new path and the value are inserted.\n\n\nSELECT JSON_SET(JSON '{\"a\": {}}', '$.a.b', 100) AS json_data\n\n/*-----------------*\n| json_data       |\n+-----------------+\n| {\"a\":{\"b\":100}} |\n*-----------------*/\n\nIn the following example, the path prefix ` $ ` points to a JSON null, so the remainder of the path is created for the value ` 100 ` .\n\n\nSELECT JSON_SET(JSON 'null', '$.a.b', 100) AS json_data\n\n/*-----------------*\n| json_data       |\n+-----------------+\n| {\"a\":{\"b\":100}} |\n*-----------------*/\n\nIn the following example, the path ` $.a.c ` implies that the value at ` $.a `\nis a JSON object but it's not. This part of the operation is ignored, but the other parts of the operation are completed successfully.\n\n\nSELECT JSON_SET( JSON '{\"a\": 1}',\n'$.b', 2,\n'$.a.c', 100,\n'$.d', 3) AS json_data\n\n/*---------------------*\n| json_data           |\n+---------------------+\n| {\"a\":1,\"b\":2,\"d\":3} |\n*---------------------*/\n\nIn the following example, the path ` $.a[2] ` implies that the value for ` $.a\n` is an array, but it's not, so the operation is ignored for that value.\n\n\nSELECT JSON_SET( JSON '{\"a\": 1}',\n'$.a[2]', 100,\n'$.b', 2) AS json_data\n\n/*---------------*\n| json_data     |\n+---------------+\n| {\"a\":1,\"b\":2} |\n*---------------*/\n\nIn the following example, the path ` $[1] ` is matched and replaces the array element value with ` foo ` .\n\n\nSELECT JSON_SET(JSON '[\"a\", [\"b\", \"c\"], \"d\"]', '$[1]', \"foo\") AS json_data\n\n/*-----------------*\n| json_data       |\n+-----------------+\n| [\"a\",\"foo\",\"d\"] |\n*-----------------*/\n\nIn the following example, the path ` $[1][0] ` is matched and replaces the array element value with ` foo ` .\n\n\nSELECT JSON_SET(JSON '[\"a\", [\"b\", \"c\"], \"d\"]', '$[1][0]', \"foo\") AS json_data\n\n/*-----------------------*\n| json_data             |\n+-----------------------+\n| [\"a\",[\"foo\",\"c\"],\"d\"] |\n*-----------------------*/\n\nIn the following example, the path prefix ` $ ` points to a JSON null, so the remainder of the path is created. The resulting array is padded with JSON nulls and appended with ` foo ` .\n\n\nSELECT JSON_SET(JSON 'null', '$[0][3]', \"foo\")\n\n/*--------------------------*\n| json_data                |\n+--------------------------+\n| [[null,null,null,\"foo\"]] |\n*--------------------------*/\n\nIn the following example, the path ` $[1] ` is matched, the matched array is extended since ` $[1][4] ` is larger than the existing array, and then ` foo `\nis inserted in the array.\n\n\nSELECT JSON_SET(JSON '[\"a\", [\"b\", \"c\"], \"d\"]', '$[1][4]', \"foo\") AS json_data\n\n/*-------------------------------------*\n| json_data                           |\n+-------------------------------------+\n| [\"a\",[\"b\",\"c\",null,null,\"foo\"],\"d\"] |\n*-------------------------------------*/\n\nIn the following example, the path ` $[1][0][0] ` implies that the value of `\n$[1][0] ` is an array, but it is not, so the operation is ignored.\n\n\nSELECT JSON_SET(JSON '[\"a\", [\"b\", \"c\"], \"d\"]', '$[1][0][0]', \"foo\") AS json_data\n\n/*---------------------*\n| json_data           |\n+---------------------+\n| [\"a\",[\"b\",\"c\"],\"d\"] |\n*---------------------*/\n\nIn the following example, the path ` $[1][2] ` is larger than the length of the matched array. The array length is extended and the remainder of the path is recursively created. The operation continues to the path ` $[1][2][1] ` and inserts ` foo ` .\n\n\nSELECT JSON_SET(JSON '[\"a\", [\"b\", \"c\"], \"d\"]', '$[1][2][1]', \"foo\") AS json_data\n\n/*----------------------------------*\n| json_data                        |\n+----------------------------------+\n| [\"a\",[\"b\",\"c\",[null,\"foo\"]],\"d\"] |\n*----------------------------------*/\n\nIn the following example, because the ` JSON ` object is empty, key ` b ` is inserted, and the remainder of the path is recursively created.\n\n\nSELECT JSON_SET(JSON '{}', '$.b[2].d', 100) AS json_data\n\n/*-----------------------------*\n| json_data                   |\n+-----------------------------+\n| {\"b\":[null,null,{\"d\":100}]} |\n*-----------------------------*/\n\nIn the following example, multiple values are set.\n\n\nSELECT JSON_SET( JSON '{\"a\": 1, \"b\": {\"c\":3}, \"d\": [4]}',\n'$.a', 'v1',\n'$.b.e', 'v2',\n'$.d[2]', 'v3') AS json_data\n\n/*---------------------------------------------------*\n| json_data                                         |\n+---------------------------------------------------+\n| {\"a\":\"v1\",\"b\":{\"c\":3,\"e\":\"v2\"},\"d\":[4,null,\"v3\"]} |\n*---------------------------------------------------*/"
            },
            "JSON_STRIP_NULLS": {
                "name": "JSON_STRIP_NULLS",
                "summary": "Removes JSON nulls from JSON objects and JSON arrays.",
                "description": "JSON_STRIP_NULLS( json_expr\n[, json_path]\n[, include_arrays => { TRUE | FALSE }]\n[, remove_empty => { TRUE | FALSE }]\n)\n\nRecursively removes JSON nulls from JSON objects and JSON arrays.\n\nArguments:\n\n* ` json_expr ` : JSON. For example:\n\nJSON '{\"a\": null, \"b\": \"c\"}'\n\n* ` json_path ` : Remove JSON nulls at this  JSONPath  for ` json_expr ` .\n\n* ` include_arrays ` : An optional, mandatory named argument that is either ` TRUE ` (default) or ` FALSE ` . If ` TRUE ` or omitted, the function removes JSON nulls from JSON arrays. If ` FALSE ` , does not.\n\n* ` remove_empty ` : An optional, mandatory named argument that is either ` TRUE ` or ` FALSE ` (default). If ` TRUE ` , the function removes empty JSON objects after JSON nulls are removed. If ` FALSE ` or omitted, does not.\n\nIf ` remove_empty ` is ` TRUE ` and ` include_arrays ` is ` TRUE ` or omitted,\nthe function additionally removes empty JSON arrays.\n\nDetails:\n\n* If a value is a JSON null, the associated key-value pair is removed.\n* If ` remove_empty ` is set to ` TRUE ` , the function recursively removes empty containers after JSON nulls are removed.\n* If the function generates JSON with nothing in it, the function returns a JSON null.\n* If ` json_path ` is an invalid  JSONPath  , an error is produced.\n* If ` json_expr ` is SQL ` NULL ` , the function returns SQL ` NULL ` .\n* If ` json_path ` , ` include_arrays ` , or ` remove_empty ` is SQL ` NULL ` , the function returns ` json_expr ` .\n\n**Return type**\n\n` JSON `\n\n**Examples**\n\nIn the following example, all JSON nulls are removed.\n\n\nSELECT JSON_STRIP_NULLS(JSON '{\"a\": null, \"b\": \"c\"}') AS json_data\n\n/*-----------*\n| json_data |\n+-----------+\n| {\"b\":\"c\"} |\n*-----------*/\n\nIn the following example, all JSON nulls are removed from a JSON array.\n\n\nSELECT JSON_STRIP_NULLS(JSON '[1, null, 2, null]') AS json_data\n\n/*-----------*\n| json_data |\n+-----------+\n| [1,2]     |\n*-----------*/\n\nIn the following example, ` include_arrays ` is set as ` FALSE ` so that JSON nulls are not removed from JSON arrays.\n\n\nSELECT JSON_STRIP_NULLS(JSON '[1, null, 2, null]', include_arrays=>FALSE) AS json_data\n\n/*-----------------*\n| json_data       |\n+-----------------+\n| [1,null,2,null] |\n*-----------------*/\n\nIn the following example, ` remove_empty ` is omitted and defaults to ` FALSE\n` , and the empty structures are retained.\n\n\nSELECT JSON_STRIP_NULLS(JSON '[1, null, 2, null, [null]]') AS json_data\n\n/*-----------*\n| json_data |\n+-----------+\n| [1,2,[]]  |\n*-----------*/\n\nIn the following example, ` remove_empty ` is set as ` TRUE ` , and the empty structures are removed.\n\n\nSELECT JSON_STRIP_NULLS( JSON '[1, null, 2, null, [null]]',\nremove_empty=>TRUE) AS json_data\n\n/*-----------*\n| json_data |\n+-----------+\n| [1,2]     |\n*-----------*/\n\nIn the following examples, ` remove_empty ` is set as ` TRUE ` , and the empty structures are removed. Because no JSON data is left the function returns JSON null.\n\n\nSELECT JSON_STRIP_NULLS(JSON '{\"a\": null}', remove_empty=>TRUE) AS json_data\n\n/*-----------*\n| json_data |\n+-----------+\n| null      |\n*-----------*/\n\n\nSELECT JSON_STRIP_NULLS(JSON '{\"a\": [null]}', remove_empty=>TRUE) AS json_data\n\n/*-----------*\n| json_data |\n+-----------+\n| null      |\n*-----------*/\n\nIn the following example, empty structures are removed for JSON objects, but not JSON arrays.\n\n\nSELECT JSON_STRIP_NULLS( JSON '{\"a\": {\"b\": {\"c\": null}}, \"d\": [null], \"e\": [], \"f\": 1}',\ninclude_arrays=>FALSE,\nremove_empty=>TRUE) AS json_data\n\n/*---------------------------*\n| json_data                 |\n+---------------------------+\n| {\"d\":[null],\"e\":[],\"f\":1} |\n*---------------------------*/\n\nIn the following example, empty structures are removed for both JSON objects,\nand JSON arrays.\n\n\nSELECT JSON_STRIP_NULLS( JSON '{\"a\": {\"b\": {\"c\": null}}, \"d\": [null], \"e\": [], \"f\": 1}',\nremove_empty=>TRUE) AS json_data\n\n/*-----------*\n| json_data |\n+-----------+\n| {\"f\":1}   |\n*-----------*/\n\nIn the following example, because no JSON data is left, the function returns a JSON null.\n\n\nSELECT JSON_STRIP_NULLS(JSON 'null') AS json_data\n\n/*-----------*\n| json_data |\n+-----------+\n| null      |\n*-----------*/"
            },
            "JSON_TYPE": {
                "name": "JSON_TYPE",
                "summary": "Gets the JSON type of the outermost JSON value and converts the name of this type to a SQL ` STRING ` value.",
                "description": "JSON_TYPE(json_expr)\n\n**Description**\n\nGets the JSON type of the outermost JSON value and converts the name of this type to a SQL ` STRING ` value. The names of these JSON types can be returned:\n` object ` , ` array ` , ` string ` , ` number ` , ` boolean ` , ` null `\n\nArguments:\n\n* ` json_expr ` : JSON. For example:\n\nJSON '{\"name\": \"sky\", \"color\": \"blue\"}'\n\nIf this expression is SQL ` NULL ` , the function returns SQL ` NULL ` . If the extracted JSON value is not a valid JSON type, an error is produced.\n\n**Return type**\n\n` STRING `\n\n**Examples**\n\n\nSELECT json_val, JSON_TYPE(json_val) AS type FROM UNNEST(\n[\nJSON '\"apple\"',\nJSON '10',\nJSON '3.14',\nJSON 'null',\nJSON '{\"city\": \"New York\", \"State\": \"NY\"}',\nJSON '[\"apple\", \"banana\"]',\nJSON 'false'\n]\n) AS json_val;\n\n/*----------------------------------+---------*\n| json_val                         | type    |\n+----------------------------------+---------+\n| \"apple\"                          | string  |\n| 10                               | number  |\n| 3.14                             | number  |\n| null                             | null    |\n| {\"State\":\"NY\",\"city\":\"New York\"} | object  |\n| [\"apple\",\"banana\"]               | array   |\n| false                            | boolean |\n*----------------------------------+---------*/"
            },
            "JSON_VALUE": {
                "name": "JSON_VALUE",
                "summary": "Extracts a JSON scalar value and converts it to a SQL `\nSTRING ` value.",
                "description": "JSON_VALUE(json_string_expr[, json_path])\n\n\nJSON_VALUE(json_expr[, json_path])\n\n**Description**\n\nExtracts a JSON scalar value and converts it to a SQL ` STRING ` value. In addition, this function:\n\n* Removes the outermost quotes and unescapes the values.\n* Returns a SQL ` NULL ` if a non-scalar value is selected.\n* Uses double quotes to escape invalid  JSONPath  characters in JSON keys. For example: ` \"a.b\" ` .\n\nArguments:\n\n* ` json_string_expr ` : A JSON-formatted string. For example:\n\n'{\"name\": \"Jakob\", \"age\": \"6\"}'\n\n* ` json_expr ` : JSON. For example:\n\nJSON '{\"name\": \"Jane\", \"age\": \"6\"}'\n\n* ` json_path ` : The  JSONPath  . This identifies the data that you want to obtain from the input. If this optional parameter is not provided, then the JSONPath ` $ ` symbol is applied, which means that all of the data is analyzed.\n\nIf ` json_path ` returns a JSON ` null ` or a non-scalar value (in other words, if ` json_path ` refers to an object or an array), then a SQL ` NULL `\nis returned.\n\nThere are differences between the JSON-formatted string and JSON input types.\nFor details, see  Differences between the JSON and JSON-formatted STRING types .\n\n**Return type**\n\n` STRING `\n\n**Examples**\n\nIn the following example, JSON data is extracted and returned as a scalar value.\n\n\nSELECT JSON_VALUE(JSON '{\"name\": \"Jakob\", \"age\": \"6\" }', '$.age') AS scalar_age;\n\n/*------------*\n| scalar_age |\n+------------+\n| 6          |\n*------------*/\n\nThe following example compares how results are returned for the ` JSON_QUERY `\nand ` JSON_VALUE ` functions.\n\n\nSELECT JSON_QUERY('{\"name\": \"Jakob\", \"age\": \"6\"}', '$.name') AS json_name,\nJSON_VALUE('{\"name\": \"Jakob\", \"age\": \"6\"}', '$.name') AS scalar_name,\nJSON_QUERY('{\"name\": \"Jakob\", \"age\": \"6\"}', '$.age') AS json_age,\nJSON_VALUE('{\"name\": \"Jakob\", \"age\": \"6\"}', '$.age') AS scalar_age;\n\n/*-----------+-------------+----------+------------*\n| json_name | scalar_name | json_age | scalar_age |\n+-----------+-------------+----------+------------+\n| \"Jakob\"   | Jakob       | \"6\"      | 6          |\n*-----------+-------------+----------+------------*/\n\n\nSELECT JSON_QUERY('{\"fruits\": [\"apple\", \"banana\"]}', '$.fruits') AS json_query,\nJSON_VALUE('{\"fruits\": [\"apple\", \"banana\"]}', '$.fruits') AS json_value;\n\n/*--------------------+------------*\n| json_query         | json_value |\n+--------------------+------------+\n| [\"apple\",\"banana\"] | NULL       |\n*--------------------+------------*/\n\nIn cases where a JSON key uses invalid JSONPath characters, you can escape those characters using double quotes. For example:\n\n\nSELECT JSON_VALUE('{\"a.b\": {\"c\": \"world\"}}', '$.\"a.b\".c') AS hello;\n\n/*-------*\n| hello |\n+-------+\n| world |\n*-------*/"
            },
            "JSON_VALUE_ARRAY": {
                "name": "JSON_VALUE_ARRAY",
                "summary": "Extracts a JSON array of scalar values and converts it to a SQL ` ARRAY<STRING> ` value.",
                "description": "JSON_VALUE_ARRAY(json_string_expr[, json_path])\n\n\nJSON_VALUE_ARRAY(json_expr[, json_path])\n\n**Description**\n\nExtracts a JSON array of scalar values and converts it to a SQL `\nARRAY<STRING> ` value. In addition, this function:\n\n* Removes the outermost quotes and unescapes the values.\n* Returns a SQL ` NULL ` if the selected value is not an array or not an array containing only scalar values.\n* Uses double quotes to escape invalid  JSONPath  characters in JSON keys. For example: ` \"a.b\" ` .\n\nArguments:\n\n* ` json_string_expr ` : A JSON-formatted string. For example:\n\n'[\"apples\", \"oranges\", \"grapes\"]'\n\n* ` json_expr ` : JSON. For example:\n\nJSON '[\"apples\", \"oranges\", \"grapes\"]'\n\n* ` json_path ` : The  JSONPath  . This identifies the data that you want to obtain from the input. If this optional parameter is not provided, then the JSONPath ` $ ` symbol is applied, which means that all of the data is analyzed.\n\nThere are differences between the JSON-formatted string and JSON input types.\nFor details, see  Differences between the JSON and JSON-formatted STRING types .\n\nCaveats:\n\n* A JSON ` null ` in the input array produces a SQL ` NULL ` as the output for JSON ` null ` . If the output contains a ` NULL ` array element, an error is produced because the final output cannot be an array with ` NULL ` values.\n* If a JSONPath matches an array that contains scalar objects and a JSON ` null ` , then the output of the function must be transformed because the final output cannot be an array with ` NULL ` values.\n\n**Return type**\n\n` ARRAY<STRING> `\n\n**Examples**\n\nThis extracts items in JSON to a string array:\n\n\nSELECT JSON_VALUE_ARRAY( JSON '{\"fruits\": [\"apples\", \"oranges\", \"grapes\"]}', '$.fruits'\n) AS string_array;\n\n/*---------------------------*\n| string_array              |\n+---------------------------+\n| [apples, oranges, grapes] |\n*---------------------------*/\n\nThe following example compares how results are returned for the `\nJSON_QUERY_ARRAY ` and ` JSON_VALUE_ARRAY ` functions.\n\n\nSELECT JSON_QUERY_ARRAY('[\"apples\", \"oranges\"]') AS json_array,\nJSON_VALUE_ARRAY('[\"apples\", \"oranges\"]') AS string_array;\n\n/*-----------------------+-------------------*\n| json_array            | string_array      |\n+-----------------------+-------------------+\n| [\"apples\", \"oranges\"] | [apples, oranges] |\n*-----------------------+-------------------*/\n\nThis extracts the items in a JSON-formatted string to a string array:\n\n\n-- Strips the double quotes SELECT JSON_VALUE_ARRAY('[\"foo\", \"bar\", \"baz\"]', '$') AS string_array;\n\n/*-----------------*\n| string_array    |\n+-----------------+\n| [foo, bar, baz] |\n*-----------------*/\n\nThis extracts a string array and converts it to an integer array:\n\n\nSELECT ARRAY( SELECT CAST(integer_element AS INT64) FROM UNNEST( JSON_VALUE_ARRAY('[1, 2, 3]', '$') ) AS integer_element ) AS integer_array;\n\n/*---------------*\n| integer_array |\n+---------------+\n| [1, 2, 3]     |\n*---------------*/\n\nThese are equivalent:\n\n\nSELECT JSON_VALUE_ARRAY('{\"fruits\": [\"apples\", \"oranges\", \"grapes\"]}', '$.fruits') AS string_array;\nSELECT JSON_VALUE_ARRAY('{\"fruits\": [\"apples\", \"oranges\", \"grapes\"]}', '$.\"fruits\"') AS string_array;\n\n-- The queries above produce the following result:\n/*---------------------------*\n| string_array              |\n+---------------------------+\n| [apples, oranges, grapes] |\n*---------------------------*/\n\nIn cases where a JSON key uses invalid JSONPath characters, you can escape those characters using double quotes: ` \" \" ` . For example:\n\n\nSELECT JSON_VALUE_ARRAY('{\"a.b\": {\"c\": [\"world\"]}}', '$.\"a.b\".c') AS hello;\n\n/*---------*\n| hello   |\n+---------+\n| [world] |\n*---------*/\n\nThe following examples explore how invalid requests and empty arrays are handled:\n\n\n-- An error is thrown if you provide an invalid JSONPath.\nSELECT JSON_VALUE_ARRAY('[\"foo\", \"bar\", \"baz\"]', 'INVALID_JSONPath') AS result;\n\n-- If the JSON-formatted string is invalid, then NULL is returned.\nSELECT JSON_VALUE_ARRAY('}}', '$') AS result;\n\n/*--------*\n| result |\n+--------+\n| NULL   |\n*--------*/\n\n-- If the JSON document is NULL, then NULL is returned.\nSELECT JSON_VALUE_ARRAY(NULL, '$') AS result;\n\n/*--------*\n| result |\n+--------+\n| NULL   |\n*--------*/\n\n-- If a JSONPath does not match anything, then the output is NULL.\nSELECT JSON_VALUE_ARRAY('{\"a\": [\"foo\", \"bar\", \"baz\"]}', '$.b') AS result;\n\n/*--------*\n| result |\n+--------+\n| NULL   |\n*--------*/\n\n-- If a JSONPath matches an object that is not an array, then the output is NULL.\nSELECT JSON_VALUE_ARRAY('{\"a\": \"foo\"}', '$') AS result;\n\n/*--------*\n| result |\n+--------+\n| NULL   |\n*--------*/\n\n-- If a JSONPath matches an array of non-scalar objects, then the output is NULL.\nSELECT JSON_VALUE_ARRAY('{\"a\": [{\"b\": \"foo\", \"c\": 1}, {\"b\": \"bar\", \"c\": 2}], \"d\": \"baz\"}', '$.a') AS result;\n\n/*--------*\n| result |\n+--------+\n| NULL   |\n*--------*/\n\n-- If a JSONPath matches an array of mixed scalar and non-scalar objects,\n-- then the output is NULL.\nSELECT JSON_VALUE_ARRAY('{\"a\": [10, {\"b\": 20}]', '$.a') AS result;\n\n/*--------*\n| result |\n+--------+\n| NULL   |\n*--------*/\n\n-- If a JSONPath matches an empty JSON array, then the output is an empty array instead of NULL.\nSELECT JSON_VALUE_ARRAY('{\"a\": \"foo\", \"b\": []}', '$.b') AS result;\n\n/*--------*\n| result |\n+--------+\n| []     |\n*--------*/\n\n-- The following query produces and error because the final output cannot be an\n-- array with NULLs.\nSELECT JSON_VALUE_ARRAY('[\"world\", 1, null]') AS result;"
            },
            "LAX_BOOL": {
                "name": "LAX_BOOL",
                "summary": "Attempts to convert a JSON value to a SQL ` BOOL ` value.",
                "description": "LAX_BOOL(json_expr)\n\n**Description**\n\nAttempts to convert a JSON value to a SQL ` BOOL ` value.\n\nArguments:\n\n* ` json_expr ` : JSON. For example:\n\nJSON 'true'\n\nDetails:\n\n* If ` json_expr ` is SQL ` NULL ` , the function returns SQL ` NULL ` .\n* See the conversion rules in the next section for additional ` NULL ` handling.\n\n**Conversion rules**\n\nFrom JSON type  |  To SQL ` BOOL `\n---|---\nboolean  |  If the JSON boolean is ` true ` , returns ` TRUE ` . Otherwise,\nreturns ` FALSE ` .\nstring  |  If the JSON string is ` 'true' ` , returns ` TRUE ` . If the JSON string is ` 'false' ` , returns ` FALSE ` . If the JSON string is any other value or has whitespace in it, returns ` NULL ` . This conversion is case-\ninsensitive.\nnumber  |  If the JSON number is a representation of ` 0 ` , returns ` FALSE `\n. Otherwise, returns ` TRUE ` .\nother type or null  |  ` NULL `\n\n**Return type**\n\n` BOOL `\n\n**Examples**\n\nExample with input that is a JSON boolean:\n\n\nSELECT LAX_BOOL(JSON 'true') AS result;\n\n/*--------*\n| result |\n+--------+\n| true   |\n*--------*/\n\nExamples with inputs that are JSON strings:\n\n\nSELECT LAX_BOOL(JSON '\"true\"') AS result;\n\n/*--------*\n| result |\n+--------+\n| TRUE   |\n*--------*/\n\n\nSELECT LAX_BOOL(JSON '\"true \"') AS result;\n\n/*--------*\n| result |\n+--------+\n| NULL   |\n*--------*/\n\n\nSELECT LAX_BOOL(JSON '\"foo\"') AS result;\n\n/*--------*\n| result |\n+--------+\n| NULL   |\n*--------*/\n\nExamples with inputs that are JSON numbers:\n\n\nSELECT LAX_BOOL(JSON '10') AS result;\n\n/*--------*\n| result |\n+--------+\n| TRUE   |\n*--------*/\n\n\nSELECT LAX_BOOL(JSON '0') AS result;\n\n/*--------*\n| result |\n+--------+\n| FALSE  |\n*--------*/\n\n\nSELECT LAX_BOOL(JSON '0.0') AS result;\n\n/*--------*\n| result |\n+--------+\n| FALSE  |\n*--------*/\n\n\nSELECT LAX_BOOL(JSON '-1.1') AS result;\n\n/*--------*\n| result |\n+--------+\n| TRUE   |\n*--------*/"
            },
            "LAX_FLOAT64": {
                "name": "LAX_FLOAT64",
                "summary": "Attempts to convert a JSON value to a SQL ` FLOAT64 `\nvalue.",
                "description": "LAX_FLOAT64(json_expr)\n\n**Description**\n\nAttempts to convert a JSON value to a SQL ` FLOAT64 ` value.\n\nArguments:\n\n* ` json_expr ` : JSON. For example:\n\nJSON '9.8'\n\nDetails:\n\n* If ` json_expr ` is SQL ` NULL ` , the function returns SQL ` NULL ` .\n* See the conversion rules in the next section for additional ` NULL ` handling.\n\n**Conversion rules**\n\nFrom JSON type  |  To SQL ` FLOAT64 `\n---|---\nboolean  |  ` NULL `\nstring  |  If the JSON string represents a JSON number, parses it as a `\nBIGNUMERIC ` value, and then safe casts the result as a ` FLOAT64 ` value. If the JSON string can't be converted, returns ` NULL ` .\nnumber  |  Casts the JSON number as a ` FLOAT64 ` value. Large JSON numbers are rounded.\nother type or null  |  ` NULL `\n\n**Return type**\n\n` FLOAT64 `\n\n**Examples**\n\nExamples with inputs that are JSON numbers:\n\n\nSELECT LAX_FLOAT64(JSON '9.8') AS result;\n\n/*--------*\n| result |\n+--------+\n| 9.8    |\n*--------*/\n\n\nSELECT LAX_FLOAT64(JSON '9') AS result;\n\n/*--------*\n| result |\n+--------+\n| 9.0    |\n*--------*/\n\n\nSELECT LAX_FLOAT64(JSON '9007199254740993') AS result;\n\n/*--------------------*\n| result             |\n+--------------------+\n| 9007199254740992.0 |\n*--------------------*/\n\n\nSELECT LAX_FLOAT64(JSON '1e100') AS result;\n\n/*--------*\n| result |\n+--------+\n| 1e+100 |\n*--------*/\n\nExamples with inputs that are JSON booleans:\n\n\nSELECT LAX_FLOAT64(JSON 'true') AS result;\n\n/*--------*\n| result |\n+--------+\n| NULL   |\n*--------*/\n\n\nSELECT LAX_FLOAT64(JSON 'false') AS result;\n\n/*--------*\n| result |\n+--------+\n| NULL   |\n*--------*/\n\nExamples with inputs that are JSON strings:\n\n\nSELECT LAX_FLOAT64(JSON '\"10\"') AS result;\n\n/*--------*\n| result |\n+--------+\n| 10.0   |\n*--------*/\n\n\nSELECT LAX_FLOAT64(JSON '\"1.1\"') AS result;\n\n/*--------*\n| result |\n+--------+\n| 1.1    |\n*--------*/\n\n\nSELECT LAX_FLOAT64(JSON '\"1.1e2\"') AS result;\n\n/*--------*\n| result |\n+--------+\n| 110.0  |\n*--------*/\n\n\nSELECT LAX_FLOAT64(JSON '\"9007199254740993\"') AS result;\n\n/*--------------------*\n| result             |\n+--------------------+\n| 9007199254740992.0 |\n*--------------------*/\n\n\nSELECT LAX_FLOAT64(JSON '\"+1.5\"') AS result;\n\n/*--------*\n| result |\n+--------+\n| 1.5    |\n*--------*/\n\n\nSELECT LAX_FLOAT64(JSON '\"NaN\"') AS result;\n\n/*--------*\n| result |\n+--------+\n| NaN    |\n*--------*/\n\n\nSELECT LAX_FLOAT64(JSON '\"Inf\"') AS result;\n\n/*----------*\n| result   |\n+----------+\n| Infinity |\n*----------*/\n\n\nSELECT LAX_FLOAT64(JSON '\"-InfiNiTY\"') AS result;\n\n/*-----------*\n| result    |\n+-----------+\n| -Infinity |\n*-----------*/\n\n\nSELECT LAX_FLOAT64(JSON '\"foo\"') AS result;\n\n/*--------*\n| result |\n+--------+\n| NULL   |\n*--------*/"
            },
            "LAX_INT64": {
                "name": "LAX_INT64",
                "summary": "Attempts to convert a JSON value to a SQL ` INT64 ` value.",
                "description": "LAX_INT64(json_expr)\n\n**Description**\n\nAttempts to convert a JSON value to a SQL ` INT64 ` value.\n\nArguments:\n\n* ` json_expr ` : JSON. For example:\n\nJSON '999'\n\nDetails:\n\n* If ` json_expr ` is SQL ` NULL ` , the function returns SQL ` NULL ` .\n* See the conversion rules in the next section for additional ` NULL ` handling.\n\n**Conversion rules**\n\nFrom JSON type  |  To SQL ` INT64 `\n---|---\nboolean  |  If the JSON boolean is ` true ` , returns ` 1 ` . If ` false ` ,\nreturns ` 0 ` .\nstring  |  If the JSON string represents a JSON number, parses it as a `\nBIGNUMERIC ` value, and then safe casts the results as an ` INT64 ` value. If the JSON string can't be converted, returns ` NULL ` .\nnumber  |  Casts the JSON number as an ` INT64 ` value. If the JSON number can't be converted, returns ` NULL ` .\nother type or null  |  ` NULL `\n\n**Return type**\n\n` INT64 `\n\n**Examples**\n\nExamples with inputs that are JSON numbers:\n\n\nSELECT LAX_INT64(JSON '10') AS result;\n\n/*--------*\n| result |\n+--------+\n| 10     |\n*--------*/\n\n\nSELECT LAX_INT64(JSON '10.0') AS result;\n\n/*--------*\n| result |\n+--------+\n| 10     |\n*--------*/\n\n\nSELECT LAX_INT64(JSON '1.1') AS result;\n\n/*--------*\n| result |\n+--------+\n| 1      |\n*--------*/\n\n\nSELECT LAX_INT64(JSON '3.5') AS result;\n\n/*--------*\n| result |\n+--------+\n| 4      |\n*--------*/\n\n\nSELECT LAX_INT64(JSON '1.1e2') AS result;\n\n/*--------*\n| result |\n+--------+\n| 110    |\n*--------*/\n\n\nSELECT LAX_INT64(JSON '1e100') AS result;\n\n/*--------*\n| result |\n+--------+\n| NULL   |\n*--------*/\n\nExamples with inputs that are JSON booleans:\n\n\nSELECT LAX_INT64(JSON 'true') AS result;\n\n/*--------*\n| result |\n+--------+\n| 1      |\n*--------*/\n\n\nSELECT LAX_INT64(JSON 'false') AS result;\n\n/*--------*\n| result |\n+--------+\n| 0      |\n*--------*/\n\nExamples with inputs that are JSON strings:\n\n\nSELECT LAX_INT64(JSON '\"10\"') AS result;\n\n/*--------*\n| result |\n+--------+\n| 10     |\n*--------*/\n\n\nSELECT LAX_INT64(JSON '\"1.1\"') AS result;\n\n/*--------*\n| result |\n+--------+\n| 1      |\n*--------*/\n\n\nSELECT LAX_INT64(JSON '\"1.1e2\"') AS result;\n\n/*--------*\n| result |\n+--------+\n| 110    |\n*--------*/\n\n\nSELECT LAX_INT64(JSON '\"+1.5\"') AS result;\n\n/*--------*\n| result |\n+--------+\n| 2      |\n*--------*/\n\n\nSELECT LAX_INT64(JSON '\"1e100\"') AS result;\n\n/*--------*\n| result |\n+--------+\n| NULL   |\n*--------*/\n\n\nSELECT LAX_INT64(JSON '\"foo\"') AS result;\n\n/*--------*\n| result |\n+--------+\n| NULL   |\n*--------*/"
            },
            "LAX_STRING": {
                "name": "LAX_STRING",
                "summary": "Attempts to convert a JSON value to a SQL ` STRING ` value.",
                "description": "LAX_STRING(json_expr)\n\n**Description**\n\nAttempts to convert a JSON value to a SQL ` STRING ` value.\n\nArguments:\n\n* ` json_expr ` : JSON. For example:\n\nJSON '\"name\"'\n\nDetails:\n\n* If ` json_expr ` is SQL ` NULL ` , the function returns SQL ` NULL ` .\n* See the conversion rules in the next section for additional ` NULL ` handling.\n\n**Conversion rules**\n\nFrom JSON type  |  To SQL ` STRING `\n---|---\nboolean  |  If the JSON boolean is ` true ` , returns ` 'true' ` . If ` false\n` , returns ` 'false' ` .\nstring  |  Returns the JSON string as a ` STRING ` value.\nnumber  |  Returns the JSON number as a ` STRING ` value.\nother type or null  |  ` NULL `\n\n**Return type**\n\n` STRING `\n\n**Examples**\n\nExamples with inputs that are JSON strings:\n\n\nSELECT LAX_STRING(JSON '\"purple\"') AS result;\n\n/*--------*\n| result |\n+--------+\n| purple |\n*--------*/\n\n\nSELECT LAX_STRING(JSON '\"10\"') AS result;\n\n/*--------*\n| result |\n+--------+\n| 10     |\n*--------*/\n\nExamples with inputs that are JSON booleans:\n\n\nSELECT LAX_STRING(JSON 'true') AS result;\n\n/*--------*\n| result |\n+--------+\n| true   |\n*--------*/\n\n\nSELECT LAX_STRING(JSON 'false') AS result;\n\n/*--------*\n| result |\n+--------+\n| false  |\n*--------*/\n\nExamples with inputs that are JSON numbers:\n\n\nSELECT LAX_STRING(JSON '10.0') AS result;\n\n/*--------*\n| result |\n+--------+\n| 10     |\n*--------*/\n\n\nSELECT LAX_STRING(JSON '10') AS result;\n\n/*--------*\n| result |\n+--------+\n| 10     |\n*--------*/\n\n\nSELECT LAX_STRING(JSON '1e100') AS result;\n\n/*--------*\n| result |\n+--------+\n| 1e+100 |\n*--------*/"
            },
            "PARSE_JSON": {
                "name": "PARSE_JSON",
                "summary": "Converts a JSON-formatted ` STRING ` value to a ` JSON `\nvalue.",
                "description": "PARSE_JSON(json_string_expr[, wide_number_mode=>{ 'exact' | 'round' }])\n\n**Description**\n\nConverts a JSON-formatted ` STRING ` value to a ` JSON ` value.\n\nArguments:\n\n* ` json_string_expr ` : A JSON-formatted string. For example:\n\n'{\"class\": {\"students\": [{\"name\": \"Jane\"}]}}'\n\n* ` wide_number_mode ` : Optional mandatory-named argument that determines how to handle numbers that cannot be stored in a ` JSON ` value without the loss of precision. If used, ` wide_number_mode ` must include one of these values:\n\n* ` exact ` (default): Only accept numbers that can be stored without loss of precision. If a number that cannot be stored without loss of precision is encountered, the function throws an error.\n* ` round ` : If a number that cannot be stored without loss of precision is encountered, attempt to round it to a number that can be stored without loss of precision. If the number cannot be rounded, the function throws an error.\n\nIf a number appears in a JSON object or array, the ` wide_number_mode `\nargument is applied to the number in the object or array.\n\nNumbers from the following domains can be stored in JSON without loss of precision:\n\n* 64-bit signed/unsigned integers, such as ` INT64 `\n* ` FLOAT64 `\n\n**Return type**\n\n` JSON `\n\n**Examples**\n\nIn the following example, a JSON-formatted string is converted to ` JSON ` .\n\n\nSELECT PARSE_JSON('{\"coordinates\": [10, 20], \"id\": 1}') AS json_data;\n\n/*--------------------------------*\n| json_data                      |\n+--------------------------------+\n| {\"coordinates\":[10,20],\"id\":1} |\n*--------------------------------*/\n\nThe following queries fail because:\n\n* The number that was passed in cannot be stored without loss of precision.\n* ` wide_number_mode=>'exact' ` is used implicitly in the first query and explicitly in the second query.\n\n\nSELECT PARSE_JSON('{\"id\": 922337203685477580701}') AS json_data; -- fails SELECT PARSE_JSON('{\"id\": 922337203685477580701}', wide_number_mode=>'exact') AS json_data; -- fails\n\nThe following query rounds the number to a number that can be stored in JSON.\n\n\nSELECT PARSE_JSON('{\"id\": 922337203685477580701}', wide_number_mode=>'round') AS json_data;\n\n/*------------------------------*\n| json_data                    |\n+------------------------------+\n| {\"id\":9.223372036854776e+20} |\n*------------------------------*/"
            },
            "STRING": {
                "name": "STRING",
                "summary": "Converts a JSON string to a SQL ` STRING ` value.",
                "description": "STRING(json_expr)\n\n**Description**\n\nConverts a JSON string to a SQL ` STRING ` value.\n\nArguments:\n\n* ` json_expr ` : JSON. For example:\n\nJSON '\"purple\"'\n\nIf the JSON value is not a string, an error is produced. If the expression is SQL ` NULL ` , the function returns SQL ` NULL ` .\n\n**Return type**\n\n` STRING `\n\n**Examples**\n\n\nSELECT STRING(JSON '\"purple\"') AS color;\n\n/*--------*\n| color  |\n+--------+\n| purple |\n*--------*/\n\n\nSELECT STRING(JSON_QUERY(JSON '{\"name\": \"sky\", \"color\": \"blue\"}', \"$.color\")) AS color;\n\n/*-------*\n| color |\n+-------+\n| blue  |\n*-------*/\n\nThe following examples show how invalid requests are handled:\n\n\n-- An error is thrown if the JSON is not of type string.\nSELECT STRING(JSON '123') AS result; -- Throws an error SELECT STRING(JSON 'null') AS result; -- Throws an error SELECT SAFE.STRING(JSON '123') AS result; -- Returns a SQL NULL"
            },
            "TO_JSON": {
                "name": "TO_JSON",
                "summary": "Converts a SQL value to a JSON value.",
                "description": "TO_JSON(sql_value[, stringify_wide_numbers=>{ TRUE | FALSE }])\n\n**Description**\n\nConverts a SQL value to a JSON value.\n\nArguments:\n\n* ` sql_value ` : The SQL value to convert to a JSON value. You can review the GoogleSQL data types that this function supports and their JSON encodings  here  .\n* ` stringify_wide_numbers ` : Optional mandatory-named argument that is either ` TRUE ` or ` FALSE ` (default).\n\n* If ` TRUE ` , numeric values outside of the ` FLOAT64 ` type domain are encoded as strings.\n* If ` FALSE ` (default), numeric values outside of the ` FLOAT64 ` type domain are not encoded as strings, but are stored as JSON numbers. If a numerical value cannot be stored in JSON without loss of precision, an error is thrown.\n\nThe following numerical data types are affected by the `\nstringify_wide_numbers ` argument:\n\n* ` INT64 `\n\n* ` NUMERIC `\n\n* ` BIGNUMERIC `\n\nIf one of these numerical data types appears in a container data type such as an ` ARRAY ` or ` STRUCT ` , the ` stringify_wide_numbers ` argument is applied to the numerical data types in the container data type.\n\n**Return type**\n\n` JSON `\n\n**Examples**\n\nIn the following example, the query converts rows in a table to JSON values.\n\n\nWith CoordinatesTable AS ( (SELECT 1 AS id, [10, 20] AS coordinates) UNION ALL (SELECT 2 AS id, [30, 40] AS coordinates) UNION ALL (SELECT 3 AS id, [50, 60] AS coordinates)) SELECT TO_JSON(t) AS json_objects FROM CoordinatesTable AS t;\n\n/*--------------------------------*\n| json_objects                   |\n+--------------------------------+\n| {\"coordinates\":[10,20],\"id\":1} |\n| {\"coordinates\":[30,40],\"id\":2} |\n| {\"coordinates\":[50,60],\"id\":3} |\n*--------------------------------*/\n\nIn the following example, the query returns a large numerical value as a JSON string.\n\n\nSELECT TO_JSON(9007199254740993, stringify_wide_numbers=>TRUE) as stringify_on;\n\n/*--------------------*\n| stringify_on       |\n+--------------------+\n| \"9007199254740993\" |\n*--------------------*/\n\nIn the following example, both queries return a large numerical value as a JSON number.\n\n\nSELECT TO_JSON(9007199254740993, stringify_wide_numbers=>FALSE) as stringify_off;\nSELECT TO_JSON(9007199254740993) as stringify_off;\n\n/*------------------*\n| stringify_off    |\n+------------------+\n| 9007199254740993 |\n*------------------*/\n\nIn the following example, only large numeric values are converted to JSON strings.\n\n\nWith T1 AS ( (SELECT 9007199254740993 AS id) UNION ALL (SELECT 2 AS id)) SELECT TO_JSON(t, stringify_wide_numbers=>TRUE) AS json_objects FROM T1 AS t;\n\n/*---------------------------*\n| json_objects              |\n+---------------------------+\n| {\"id\":\"9007199254740993\"} |\n| {\"id\":2}                  |\n*---------------------------*/\n\nIn this example, the values ` 9007199254740993 ` ( ` INT64 ` ) and ` 2.1 ` ( `\nFLOAT64 ` ) are converted to the common supertype ` FLOAT64 ` , which is not affected by the ` stringify_wide_numbers ` argument.\n\n\nWith T1 AS ( (SELECT 9007199254740993 AS id) UNION ALL (SELECT 2.1 AS id)) SELECT TO_JSON(t, stringify_wide_numbers=>TRUE) AS json_objects FROM T1 AS t;\n\n/*------------------------------*\n| json_objects                 |\n+------------------------------+\n| {\"id\":9.007199254740992e+15} |\n| {\"id\":2.1}                   |\n*------------------------------*/"
            },
            "TO_JSON_STRING": {
                "name": "TO_JSON_STRING",
                "summary": "Converts a SQL value to a JSON-formatted ` STRING `\nvalue.",
                "description": "TO_JSON_STRING(value[, pretty_print])\n\n**Description**\n\nConverts a SQL value to a JSON-formatted ` STRING ` value.\n\nArguments:\n\n* ` value ` : A SQL value. You can review the GoogleSQL data types that this function supports and their JSON encodings  here  .\n* ` pretty_print ` : Optional boolean parameter. If ` pretty_print ` is ` true ` , the `returned value is formatted for easy readability.\n\n**Return type**\n\nA JSON-formatted ` STRING `\n\n**Examples**\n\nConvert rows in a table to JSON-formatted strings.\n\n\nWith CoordinatesTable AS ( (SELECT 1 AS id, [10, 20] AS coordinates) UNION ALL (SELECT 2 AS id, [30, 40] AS coordinates) UNION ALL (SELECT 3 AS id, [50, 60] AS coordinates)) SELECT id, coordinates, TO_JSON_STRING(t) AS json_data FROM CoordinatesTable AS t;\n\n/*----+-------------+--------------------------------*\n| id | coordinates | json_data                      |\n+----+-------------+--------------------------------+\n| 1  | [10, 20]    | {\"id\":1,\"coordinates\":[10,20]} |\n| 2  | [30, 40]    | {\"id\":2,\"coordinates\":[30,40]} |\n| 3  | [50, 60]    | {\"id\":3,\"coordinates\":[50,60]} |\n*----+-------------+--------------------------------*/\n\nConvert rows in a table to JSON-formatted strings that are easy to read.\n\n\nWith CoordinatesTable AS ( (SELECT 1 AS id, [10, 20] AS coordinates) UNION ALL (SELECT 2 AS id, [30, 40] AS coordinates)) SELECT id, coordinates, TO_JSON_STRING(t, true) AS json_data FROM CoordinatesTable AS t;\n\n/*----+-------------+--------------------*\n| id | coordinates | json_data          |\n+----+-------------+--------------------+\n| 1  | [10, 20]    | {                  |\n|    |             |   \"id\": 1,         |\n|    |             |   \"coordinates\": [ |\n|    |             |     10,            |\n|    |             |     20             |\n|    |             |   ]                |\n|    |             | }                  |\n+----+-------------+--------------------+\n| 2  | [30, 40]    | {                  |\n|    |             |   \"id\": 2,         |\n|    |             |   \"coordinates\": [ |\n|    |             |     30,            |\n|    |             |     40             |\n|    |             |   ]                |\n|    |             | }                  |\n*----+-------------+--------------------*/"
            }
        }
    },
    {
        "category": "mathematical-functions",
        "description": "GoogleSQL for BigQuery supports mathematical functions. All mathematical functions have the following behaviors:\n\n* They return ` NULL ` if any of the input parameters is ` NULL ` .\n* They return ` NaN ` if any of the arguments is ` NaN ` .\n\n###  Categories\n\nCategory  |  Functions\n---|---\nTrigonometric  |  ` ACOS ` ` ACOSH ` ` ASIN ` ` ASINH ` ` ATAN ` ` ATAN2 ` `\nATANH ` ` COS ` ` COSH ` ` COT ` ` COTH ` ` CSC ` ` CSCH ` ` SEC ` ` SECH ` `\nSIN ` ` SINH ` ` TAN ` ` TANH ` |  Exponential and logarithmic  |  ` EXP ` ` LN ` ` LOG ` ` LOG10 `\nRounding and truncation  |  ` CEIL ` ` CEILING ` ` FLOOR ` ` ROUND ` ` TRUNC `\nPower and root  |  ` CBRT ` ` POW ` ` POWER ` ` SQRT `\nSign  |  ` ABS ` ` SIGN `\nDistance  |  ` COSINE_DISTANCE ` ` EUCLIDEAN_DISTANCE `\nComparison  |  ` GREATEST ` ` LEAST `\nRandom number generator  |  ` RAND `\nArithmetic and error handling  |  ` DIV ` ` IEEE_DIVIDE ` ` IS_INF ` ` IS_NAN\n` ` MOD ` ` SAFE_ADD ` ` SAFE_DIVIDE ` ` SAFE_MULTIPLY ` ` SAFE_NEGATE ` `\nSAFE_SUBTRACT `\nBucket  |  ` RANGE_BUCKET `",
        "source": "mathematical_functions.txt",
        "functions": {
            "ABS": {
                "name": "ABS",
                "summary": "Computes the absolute value of ` X ` .",
                "description": "ABS(X)\n\n**Description**\n\nComputes absolute value. Returns an error if the argument is an integer and the output value cannot be represented as the same type; this happens only for the largest negative input value, which has no positive representation.\n\nX  |  ABS(X)\n---|---\n25  |  25\n-25  |  25\n` +inf ` |  ` +inf `\n` -inf ` |  ` +inf `\n\n**Return Data Type**\n\nINPUT  |  ` INT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `\n---|---|---|---|---\nOUTPUT  |  ` INT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `"
            },
            "ACOS": {
                "name": "ACOS",
                "summary": "Computes the inverse cosine of ` X ` .",
                "description": "ACOS(X)\n\n**Description**\n\nComputes the principal value of the inverse cosine of X. The return value is in the range [0,\u03c0]. Generates an error if X is a value outside of the range\n[-1, 1].\n\nX  |  ACOS(X)\n---|---\n` +inf ` |  ` NaN `\n` -inf ` |  ` NaN `\n` NaN ` |  ` NaN `\nX < -1  |  Error X > 1  |  Error"
            },
            "ACOSH": {
                "name": "ACOSH",
                "summary": "Computes the inverse hyperbolic cosine of ` X ` .",
                "description": "ACOSH(X)\n\n**Description**\n\nComputes the inverse hyperbolic cosine of X. Generates an error if X is a value less than 1.\n\nX  |  ACOSH(X)\n---|---\n` +inf ` |  ` +inf `\n` -inf ` |  ` NaN `\n` NaN ` |  ` NaN `\nX < 1  |  Error"
            },
            "ASIN": {
                "name": "ASIN",
                "summary": "Computes the inverse sine of ` X ` .",
                "description": "ASIN(X)\n\n**Description**\n\nComputes the principal value of the inverse sine of X. The return value is in the range [-\u03c0/2,\u03c0/2]. Generates an error if X is outside of the range [-1, 1].\n\nX  |  ASIN(X)\n---|---\n` +inf ` |  ` NaN `\n` -inf ` |  ` NaN `\n` NaN ` |  ` NaN `\nX < -1  |  Error X > 1  |  Error"
            },
            "ASINH": {
                "name": "ASINH",
                "summary": "Computes the inverse hyperbolic sine of ` X ` .",
                "description": "ASINH(X)\n\n**Description**\n\nComputes the inverse hyperbolic sine of X. Does not fail.\n\nX  |  ASINH(X)\n---|---\n` +inf ` |  ` +inf `\n` -inf ` |  ` -inf `\n` NaN ` |  ` NaN `"
            },
            "ATAN": {
                "name": "ATAN",
                "summary": "Computes the inverse tangent of ` X ` .",
                "description": "ATAN(X)\n\n**Description**\n\nComputes the principal value of the inverse tangent of X. The return value is in the range [-\u03c0/2,\u03c0/2]. Does not fail.\n\nX  |  ATAN(X)\n---|---\n` +inf ` |  \u03c0/2\n` -inf ` |  -\u03c0/2\n` NaN ` |  ` NaN `"
            },
            "ATAN2": {
                "name": "ATAN2",
                "summary": "Computes the inverse tangent of ` X/Y ` , using the signs of ` X\n` and ` Y ` to determine the quadrant.",
                "description": "ATAN2(X, Y)\n\n**Description**\n\nCalculates the principal value of the inverse tangent of X/Y using the signs of the two arguments to determine the quadrant. The return value is in the range [-\u03c0,\u03c0].\n\nX  |  Y  |  ATAN2(X, Y)\n---|---|---\n` NaN ` |  Any value  |  ` NaN `\nAny value  |  ` NaN ` |  ` NaN `\n0.0  |  0.0  |  0.0 Positive Finite value  |  ` -inf ` |  \u03c0\nNegative Finite value  |  ` -inf ` |  -\u03c0\nFinite value  |  ` +inf ` |  0.0\n` +inf ` |  Finite value  |  \u03c0/2\n` -inf ` |  Finite value  |  -\u03c0/2\n` +inf ` |  ` -inf ` |  \u00be\u03c0\n` -inf ` |  ` -inf ` |  -\u00be\u03c0\n` +inf ` |  ` +inf ` |  \u03c0/4\n` -inf ` |  ` +inf ` |  -\u03c0/4"
            },
            "ATANH": {
                "name": "ATANH",
                "summary": "Computes the inverse hyperbolic tangent of ` X ` .",
                "description": "ATANH(X)\n\n**Description**\n\nComputes the inverse hyperbolic tangent of X. Generates an error if X is outside of the range (-1, 1).\n\nX  |  ATANH(X)\n---|---\n` +inf ` |  ` NaN `\n` -inf ` |  ` NaN `\n` NaN ` |  ` NaN `\nX < -1  |  Error X > 1  |  Error"
            },
            "CBRT": {
                "name": "CBRT",
                "summary": "Computes the cube root of ` X ` .",
                "description": "CBRT(X)\n\n**Description**\n\nComputes the cube root of ` X ` . ` X ` can be any data type that [ coerces to\n` FLOAT64 ` ](/bigquery/docs/reference/standard-\nsql/conversion_rules#conversion_rules) . Supports the ` SAFE. ` prefix.\n\nX  |  CBRT(X)\n---|---\n` +inf ` |  ` inf `\n` -inf ` |  ` -inf `\n` NaN ` |  ` NaN `\n` 0 ` |  ` 0 `\n` NULL ` |  ` NULL `\n\n**Return Data Type**\n\n` FLOAT64 `\n\n**Example**\n\n\nSELECT CBRT(27) AS cube_root;\n\n/*--------------------*\n| cube_root          |\n+--------------------+\n| 3.0000000000000004 |\n*--------------------*/"
            },
            "CEIL": {
                "name": "CEIL",
                "summary": "Gets the smallest integral value that is not less than ` X ` .",
                "description": "CEIL(X)\n\n**Description**\n\nReturns the smallest integral value that is not less than X.\n\nX  |  CEIL(X)\n---|---\n2.0  |  2.0 2.3  |  3.0 2.8  |  3.0 2.5  |  3.0\n-2.3  |  -2.0\n-2.8  |  -2.0\n-2.5  |  -2.0 0  |  0\n` +inf ` |  ` +inf `\n` -inf ` |  ` -inf `\n` NaN ` |  ` NaN `\n\n**Return Data Type**\n\nINPUT  |  ` INT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `\n---|---|---|---|---\nOUTPUT  |  ` FLOAT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `"
            },
            "CEILING": {
                "name": "CEILING",
                "summary": "Synonym of ` CEIL ` .",
                "description": "CEILING(X)\n\n**Description**\n\nSynonym of CEIL(X)"
            },
            "COS": {
                "name": "COS",
                "summary": "Computes the cosine of ` X ` .",
                "description": "COS(X)\n\n**Description**\n\nComputes the cosine of X where X is specified in radians. Never fails.\n\nX  |  COS(X)\n---|---\n` +inf ` |  ` NaN `\n` -inf ` |  ` NaN `\n` NaN ` |  ` NaN `"
            },
            "COSH": {
                "name": "COSH",
                "summary": "Computes the hyperbolic cosine of ` X ` .",
                "description": "COSH(X)\n\n**Description**\n\nComputes the hyperbolic cosine of X where X is specified in radians. Generates an error if overflow occurs.\n\nX  |  COSH(X)\n---|---\n` +inf ` |  ` +inf `\n` -inf ` |  ` +inf `\n` NaN ` |  ` NaN `"
            },
            "COSINE_DISTANCE": {
                "name": "COSINE_DISTANCE",
                "summary": "Computes the cosine distance between two vectors.",
                "description": "COSINE_DISTANCE(vector1, vector2)\n\n**Description**\n\nComputes the [ cosine distance\n](https://en.wikipedia.org/wiki/Cosine_similarity#Cosine_distance) between two vectors.\n\n**Definitions**\n\n* ` vector1 ` : A vector that is represented by an ` ARRAY<T> ` value or a sparse vector that is represented by an ` ARRAY<STRUCT<dimension,magnitude>> ` value.\n* ` vector2 ` : A vector that is represented by an ` ARRAY<T> ` value or a sparse vector that is represented by an ` ARRAY<STRUCT<dimension,magnitude>> ` value.\n\n**Details**\n\n* ` ARRAY<T> ` can be used to represent a vector. Each zero-based index in this array represents a dimension. The value for each element in this array represents a magnitude.\n\n` T ` can represent the following and must be the same for both vectors:\n\n* ` FLOAT64 `\n\nIn the following example vector, there are four dimensions. The magnitude is `\n10.0 ` for dimension ` 0 ` , ` 55.0 ` for dimension ` 1 ` , ` 40.0 ` for dimension ` 2 ` , and ` 34.0 ` for dimension ` 3 ` :\n\n[10.0, 55.0, 40.0, 34.0]\n\n* ` ARRAY<STRUCT<dimension,magnitude>> ` can be used to represent a sparse vector. With a sparse vector, you only need to include dimension-magnitude pairs for non-zero magnitudes. If a magnitude isn't present in the sparse vector, the magnitude is implicitly understood to be zero.\n\nFor example, if you have a vector with 10,000 dimensions, but only 10 dimensions have non-zero magnitudes, then the vector is a sparse vector. As a result, it's more efficient to describe a sparse vector by only mentioning its non-zero magnitudes.\n\nIn ` ARRAY<STRUCT<dimension,magnitude>> ` , ` STRUCT<dimension,magnitude> `\nrepresents a dimension-magnitude pair for each non-zero magnitude in a sparse vector. These parts need to be included for each dimension-magnitude pair:\n\n* ` dimension ` : A ` STRING ` or ` INT64 ` value that represents a dimension in a vector.\n\n* ` magnitude ` : A ` FLOAT64 ` value that represents a non-zero magnitude for a specific dimension in a vector.\n\nYou don't need to include empty dimension-magnitude pairs in a sparse vector.\nFor example, the following sparse vector and non-sparse vector are equivalent:\n\n-- sparse vector ARRAY<STRUCT<INT64, FLOAT64>>\n[(1, 10.0), (2: 30.0), (5, 40.0)]\n\n-- vector ARRAY<FLOAT64>\n[0.0, 10.0, 30.0, 0.0, 0.0, 40.0]\n\nIn a sparse vector, dimension-magnitude pairs don't need to be in any particular order. The following sparse vectors are equivalent:\n\n[('a', 10.0), ('b': 30.0), ('d': 40.0)]\n\n[('d': 40.0), ('a', 10.0), ('b': 30.0)]\n\n* Both non-sparse vectors in this function must share the same dimensions, and if they don't, an error is produced.\n\n* A vector can't be a zero vector. A vector is a zero vector if it has no dimensions or all dimensions have a magnitude of ` 0 ` , such as ` [] ` or ` [0.0, 0.0] ` . If a zero vector is encountered, an error is produced.\n\n* An error is produced if a magnitude in a vector is ` NULL ` .\n\n* If a vector is ` NULL ` , ` NULL ` is returned.\n\n**Return type**\n\n` FLOAT64 `\n\n**Examples**\n\nIn the following example, non-sparsevectors are used to compute the cosine distance:\n\n\nSELECT COSINE_DISTANCE([1.0, 2.0], [3.0, 4.0]) AS results;\n\n/*----------*\n| results  |\n+----------+\n| 0.016130 |\n*----------*/\n\nIn the following example, sparse vectors are used to compute the cosine distance:\n\n\nSELECT COSINE_DISTANCE(\n[(1, 1.0), (2, 2.0)],\n[(2, 4.0), (1, 3.0)]) AS results;\n\n/*----------*\n| results  |\n+----------+\n| 0.016130 |\n*----------*/\n\nThe ordering of numeric values in a vector doesn't impact the results produced by this function. For example these queries produce the same results even though the numeric values in each vector is in a different order:\n\n\nSELECT COSINE_DISTANCE([1.0, 2.0], [3.0, 4.0]) AS results;\n\n\nSELECT COSINE_DISTANCE([2.0, 1.0], [4.0, 3.0]) AS results;\n\n\nSELECT COSINE_DISTANCE([(1, 1.0), (2, 2.0)], [(1, 3.0), (2, 4.0)]) AS results;\n\n\n/*----------*\n| results  |\n+----------+\n| 0.016130 |\n*----------*/\n\nIn the following example, the function can't compute cosine distance against the first vector, which is a zero vector:\n\n\n-- ERROR SELECT COSINE_DISTANCE([0.0, 0.0], [3.0, 4.0]) AS results;\n\n\n-- ERROR SELECT COSINE_DISTANCE([(1, 0.0), (2, 0.0)], [(1, 3.0), (2, 4.0)]) AS results;\n\nBoth non-sparse vectors must have the same dimensions. If not, an error is produced. In the following example, the first vector has two dimensions and the second vector has three:\n\n\n-- ERROR SELECT COSINE_DISTANCE([9.0, 7.0], [8.0, 4.0, 5.0]) AS results;\n\nIf you use sparse vectors and you repeat a dimension, an error is produced:\n\n\n-- ERROR SELECT COSINE_DISTANCE(\n[(1, 9.0), (2, 7.0), (2, 8.0)], [(1, 8.0), (2, 4.0), (3, 5.0)]) AS results;"
            },
            "COT": {
                "name": "COT",
                "summary": "Computes the cotangent of ` X ` .",
                "description": "COT(X)\n\n**Description**\n\nComputes the cotangent for the angle of ` X ` , where ` X ` is specified in radians. ` X ` can be any data type that [ coerces to ` FLOAT64 `\n](/bigquery/docs/reference/standard-sql/conversion_rules#conversion_rules) .\nSupports the ` SAFE. ` prefix.\n\nX  |  COT(X)\n---|---\n` +inf ` |  ` NaN `\n` -inf ` |  ` NaN `\n` NaN ` |  ` NaN `\n` 0 ` |  ` Error `\n` NULL ` |  ` NULL `\n\n**Return Data Type**\n\n` FLOAT64 `\n\n**Example**\n\n\nSELECT COT(1) AS a, SAFE.COT(0) AS b;\n\n/*---------------------+------*\n| a                   | b    |\n+---------------------+------+\n| 0.64209261593433065 | NULL |\n*---------------------+------*/"
            },
            "COTH": {
                "name": "COTH",
                "summary": "Computes the hyperbolic cotangent of ` X ` .",
                "description": "COTH(X)\n\n**Description**\n\nComputes the hyperbolic cotangent for the angle of ` X ` , where ` X ` is specified in radians. ` X ` can be any data type that [ coerces to ` FLOAT64 `\n](/bigquery/docs/reference/standard-sql/conversion_rules#conversion_rules) .\nSupports the ` SAFE. ` prefix.\n\nX  |  COTH(X)\n---|---\n` +inf ` |  ` 1 `\n` -inf ` |  ` -1 `\n` NaN ` |  ` NaN `\n` 0 ` |  ` Error `\n` NULL ` |  ` NULL `\n\n**Return Data Type**\n\n` FLOAT64 `\n\n**Example**\n\n\nSELECT COTH(1) AS a, SAFE.COTH(0) AS b;\n\n/*----------------+------*\n| a              | b    |\n+----------------+------+\n| 1.313035285499 | NULL |\n*----------------+------*/"
            },
            "CSC": {
                "name": "CSC",
                "summary": "Computes the cosecant of ` X ` .",
                "description": "CSC(X)\n\n**Description**\n\nComputes the cosecant of the input angle, which is in radians. ` X ` can be any data type that [ coerces to ` FLOAT64 `\n](/bigquery/docs/reference/standard-sql/conversion_rules#conversion_rules) .\nSupports the ` SAFE. ` prefix.\n\nX  |  CSC(X)\n---|---\n` +inf ` |  ` NaN `\n` -inf ` |  ` NaN `\n` NaN ` |  ` NaN `\n` 0 ` |  ` Error `\n` NULL ` |  ` NULL `\n\n**Return Data Type**\n\n` FLOAT64 `\n\n**Example**\n\n\nSELECT CSC(100) AS a, CSC(-1) AS b, SAFE.CSC(0) AS c;\n\n/*----------------+-----------------+------*\n| a              | b               | c    |\n+----------------+-----------------+------+\n| -1.97485753142 | -1.188395105778 | NULL |\n*----------------+-----------------+------*/"
            },
            "CSCH": {
                "name": "CSCH",
                "summary": "Computes the hyperbolic cosecant of ` X ` .",
                "description": "CSCH(X)\n\n**Description**\n\nComputes the hyperbolic cosecant of the input angle, which is in radians. ` X\n` can be any data type that [ coerces to ` FLOAT64 `\n](/bigquery/docs/reference/standard-sql/conversion_rules#conversion_rules) .\nSupports the ` SAFE. ` prefix.\n\nX  |  CSCH(X)\n---|---\n` +inf ` |  ` 0 `\n` -inf ` |  ` 0 `\n` NaN ` |  ` NaN `\n` 0 ` |  ` Error `\n` NULL ` |  ` NULL `\n\n**Return Data Type**\n\n` FLOAT64 `\n\n**Example**\n\n\nSELECT CSCH(0.5) AS a, CSCH(-2) AS b, SAFE.CSCH(0) AS c;\n\n/*----------------+----------------+------*\n| a              | b              | c    |\n+----------------+----------------+------+\n| 1.919034751334 | -0.27572056477 | NULL |\n*----------------+----------------+------*/"
            },
            "DIV": {
                "name": "DIV",
                "summary": "Divides integer ` X ` by integer ` Y ` .",
                "description": "DIV(X, Y)\n\n**Description**\n\nReturns the result of integer division of X by Y. Division by zero returns an error. Division by -1 may overflow.\n\nX  |  Y  |  DIV(X, Y)\n---|---|---\n20  |  4  |  5 12  |  -7  |  -1 20  |  3  |  6 0  |  20  |  0 20  |  0  |  Error\n\n**Return Data Type**\n\nThe return data type is determined by the argument types with the following table.\n\nINPUT  |  ` INT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC `\n---|---|---|---\n\n` INT64 ` |  ` INT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC `\n\n` NUMERIC ` |  ` NUMERIC ` |  ` NUMERIC ` |  ` BIGNUMERIC `\n` BIGNUMERIC ` |  ` BIGNUMERIC ` |  ` BIGNUMERIC ` |  ` BIGNUMERIC `"
            },
            "EXP": {
                "name": "EXP",
                "summary": "Computes ` e ` to the power of ` X ` .",
                "description": "EXP(X)\n\n**Description**\n\nComputes _e_ to the power of X, also called the natural exponential function.\nIf the result underflows, this function returns a zero. Generates an error if the result overflows.\n\nX  |  EXP(X)\n---|---\n0.0  |  1.0\n` +inf ` |  ` +inf `\n` -inf ` |  0.0\n\n**Return Data Type**\n\nINPUT  |  ` INT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `\n---|---|---|---|---\nOUTPUT  |  ` FLOAT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `"
            },
            "EUCLIDEAN_DISTANCE": {
                "name": "EUCLIDEAN_DISTANCE",
                "summary": "Computes the Euclidean distance between two vectors.",
                "description": "EUCLIDEAN_DISTANCE(vector1, vector2)\n\n**Description**\n\nComputes the [ Euclidean distance\n](https://en.wikipedia.org/wiki/Euclidean_distance) between two vectors.\n\n**Definitions**\n\n* ` vector1 ` : A vector that is represented by an ` ARRAY<T> ` value or a sparse vector that is represented by an ` ARRAY<STRUCT<dimension,magnitude>> ` value.\n* ` vector2 ` : A vector that is represented by an ` ARRAY<T> ` value or a sparse vector that is represented by an ` ARRAY<STRUCT<dimension,magnitude>> ` value.\n\n**Details**\n\n* ` ARRAY<T> ` can be used to represent a vector. Each zero-based index in this array represents a dimension. The value for each element in this array represents a magnitude.\n\n` T ` can represent the following and must be the same for both vectors:\n\n* ` FLOAT64 `\n\nIn the following example vector, there are four dimensions. The magnitude is `\n10.0 ` for dimension ` 0 ` , ` 55.0 ` for dimension ` 1 ` , ` 40.0 ` for dimension ` 2 ` , and ` 34.0 ` for dimension ` 3 ` :\n\n[10.0, 55.0, 40.0, 34.0]\n\n* ` ARRAY<STRUCT<dimension,magnitude>> ` can be used to represent a sparse vector. With a sparse vector, you only need to include dimension-magnitude pairs for non-zero magnitudes. If a magnitude isn't present in the sparse vector, the magnitude is implicitly understood to be zero.\n\nFor example, if you have a vector with 10,000 dimensions, but only 10 dimensions have non-zero magnitudes, then the vector is a sparse vector. As a result, it's more efficient to describe a sparse vector by only mentioning its non-zero magnitudes.\n\nIn ` ARRAY<STRUCT<dimension,magnitude>> ` , ` STRUCT<dimension,magnitude> `\nrepresents a dimension-magnitude pair for each non-zero magnitude in a sparse vector. These parts need to be included for each dimension-magnitude pair:\n\n* ` dimension ` : A ` STRING ` or ` INT64 ` value that represents a dimension in a vector.\n\n* ` magnitude ` : A ` FLOAT64 ` value that represents a non-zero magnitude for a specific dimension in a vector.\n\nYou don't need to include empty dimension-magnitude pairs in a sparse vector.\nFor example, the following sparse vector and non-sparse vector are equivalent:\n\n-- sparse vector ARRAY<STRUCT<INT64, FLOAT64>>\n[(1, 10.0), (2: 30.0), (5, 40.0)]\n\n-- vector ARRAY<FLOAT64>\n[0.0, 10.0, 30.0, 0.0, 0.0, 40.0]\n\nIn a sparse vector, dimension-magnitude pairs don't need to be in any particular order. The following sparse vectors are equivalent:\n\n[('a', 10.0), ('b': 30.0), ('d': 40.0)]\n\n[('d': 40.0), ('a', 10.0), ('b': 30.0)]\n\n* Both non-sparse vectors in this function must share the same dimensions, and if they don't, an error is produced.\n\n* A vector can be a zero vector. A vector is a zero vector if it has no dimensions or all dimensions have a magnitude of ` 0 ` , such as ` [] ` or ` [0.0, 0.0] ` .\n\n* An error is produced if a magnitude in a vector is ` NULL ` .\n\n* If a vector is ` NULL ` , ` NULL ` is returned.\n\n**Return type**\n\n` FLOAT64 `\n\n**Examples**\n\nIn the following example, non-sparse vectors are used to compute the Euclidean distance:\n\n\nSELECT EUCLIDEAN_DISTANCE([1.0, 2.0], [3.0, 4.0]) AS results;\n\n/*----------*\n| results  |\n+----------+\n| 2.828    |\n*----------*/\n\nIn the following example, sparse vectors are used to compute the Euclidean distance:\n\n\nSELECT EUCLIDEAN_DISTANCE(\n[(1, 1.0), (2, 2.0)],\n[(2, 4.0), (1, 3.0)]) AS results;\n\n/*----------*\n| results  |\n+----------+\n| 2.828    |\n*----------*/\n\nThe ordering of magnitudes in a vector doesn't impact the results produced by this function. For example these queries produce the same results even though the magnitudes in each vector is in a different order:\n\n\nSELECT EUCLIDEAN_DISTANCE([1.0, 2.0], [3.0, 4.0]);\n\n\nSELECT EUCLIDEAN_DISTANCE([2.0, 1.0], [4.0, 3.0]);\n\n\nSELECT EUCLIDEAN_DISTANCE([(1, 1.0), (2, 2.0)], [(1, 3.0), (2, 4.0)]) AS results;\n\n\n/*----------*\n| results  |\n+----------+\n| 2.828    |\n*----------*/\n\nBoth non-sparse vectors must have the same dimensions. If not, an error is produced. In the following example, the first vector has two dimensions and the second vector has three:\n\n\n-- ERROR SELECT EUCLIDEAN_DISTANCE([9.0, 7.0], [8.0, 4.0, 5.0]) AS results;\n\nIf you use sparse vectors and you repeat a dimension, an error is produced:\n\n\n-- ERROR SELECT EUCLIDEAN_DISTANCE(\n[(1, 9.0), (2, 7.0), (2, 8.0)], [(1, 8.0), (2, 4.0), (3, 5.0)]) AS results;"
            },
            "FLOOR": {
                "name": "FLOOR",
                "summary": "Gets the largest integral value that is not greater than ` X ` .",
                "description": "FLOOR(X)\n\n**Description**\n\nReturns the largest integral value that is not greater than X.\n\nX  |  FLOOR(X)\n---|---\n2.0  |  2.0 2.3  |  2.0 2.8  |  2.0 2.5  |  2.0\n-2.3  |  -3.0\n-2.8  |  -3.0\n-2.5  |  -3.0 0  |  0\n` +inf ` |  ` +inf `\n` -inf ` |  ` -inf `\n` NaN ` |  ` NaN `\n\n**Return Data Type**\n\nINPUT  |  ` INT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `\n---|---|---|---|---\nOUTPUT  |  ` FLOAT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `"
            },
            "GREATEST": {
                "name": "GREATEST",
                "summary": "Gets the greatest value among ` X1,...,XN ` .",
                "description": "GREATEST(X1,...,XN)\n\n**Description**\n\nReturns the greatest value among ` X1,...,XN ` . If any argument is ` NULL ` ,\nreturns ` NULL ` . Otherwise, in the case of floating-point arguments, if any argument is ` NaN ` , returns ` NaN ` . In all other cases, returns the value among ` X1,...,XN ` that has the greatest value according to the ordering used by the ` ORDER BY ` clause. The arguments ` X1, ..., XN ` must be coercible to a common supertype, and the supertype must support ordering.\n\nX1,...,XN  |  GREATEST(X1,...,XN)\n---|---\n3,5,1  |  5\n\nThis function supports specifying [ collation\n](/bigquery/docs/reference/standard-sql/collation-concepts#collate_about) .\n\n**Return Data Types**\n\nData type of the input values."
            },
            "IEEE_DIVIDE": {
                "name": "IEEE_DIVIDE",
                "summary": "Divides ` X ` by ` Y ` , but does not generate errors for division by zero or overflow.",
                "description": "IEEE_DIVIDE(X, Y)\n\n**Description**\n\nDivides X by Y; this function never fails. Returns ` FLOAT64 ` . Unlike the division operator (/), this function does not generate errors for division by zero or overflow.\n\nX  |  Y  |  IEEE_DIVIDE(X, Y)\n---|---|---\n20.0  |  4.0  |  5.0 0.0  |  25.0  |  0.0 25.0  |  0.0  |  ` +inf `\n-25.0  |  0.0  |  ` -inf `\n0.0  |  0.0  |  ` NaN `\n0.0  |  ` NaN ` |  ` NaN `\n` NaN ` |  0.0  |  ` NaN `\n` +inf ` |  ` +inf ` |  ` NaN `\n` -inf ` |  ` -inf ` |  ` NaN `"
            },
            "IS_INF": {
                "name": "IS_INF",
                "summary": "Checks if ` X ` is positive or negative infinity.",
                "description": "IS_INF(X)\n\n**Description**\n\nReturns ` TRUE ` if the value is positive or negative infinity.\n\nX  |  IS_INF(X)\n---|---\n` +inf ` |  ` TRUE `\n` -inf ` |  ` TRUE `\n25  |  ` FALSE `"
            },
            "IS_NAN": {
                "name": "IS_NAN",
                "summary": "Checks if ` X ` is a ` NaN ` value.",
                "description": "IS_NAN(X)\n\n**Description**\n\nReturns ` TRUE ` if the value is a ` NaN ` value.\n\nX  |  IS_NAN(X)\n---|---\n` NaN ` |  ` TRUE `\n25  |  ` FALSE `"
            },
            "LEAST": {
                "name": "LEAST",
                "summary": "Gets the least value among ` X1,...,XN ` .",
                "description": "LEAST(X1,...,XN)\n\n**Description**\n\nReturns the least value among ` X1,...,XN ` . If any argument is ` NULL ` ,\nreturns ` NULL ` . Otherwise, in the case of floating-point arguments, if any argument is ` NaN ` , returns ` NaN ` . In all other cases, returns the value among ` X1,...,XN ` that has the least value according to the ordering used by the ` ORDER BY ` clause. The arguments ` X1, ..., XN ` must be coercible to a common supertype, and the supertype must support ordering.\n\nX1,...,XN  |  LEAST(X1,...,XN)\n---|---\n3,5,1  |  1\n\nThis function supports specifying [ collation\n](/bigquery/docs/reference/standard-sql/collation-concepts#collate_about) .\n\n**Return Data Types**\n\nData type of the input values."
            },
            "LN": {
                "name": "LN",
                "summary": "Computes the natural logarithm of ` X ` .",
                "description": "LN(X)\n\n**Description**\n\nComputes the natural logarithm of X. Generates an error if X is less than or equal to zero.\n\nX  |  LN(X)\n---|---\n1.0  |  0.0\n` +inf ` |  ` +inf `\n` X < 0 ` |  Error\n\n**Return Data Type**\n\nINPUT  |  ` INT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `\n---|---|---|---|---\nOUTPUT  |  ` FLOAT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `"
            },
            "LOG": {
                "name": "LOG",
                "summary": "Computes the natural logarithm of ` X ` or the logarithm of ` X `\nto base ` Y ` .",
                "description": "LOG(X [, Y])\n\n**Description**\n\nIf only X is present, ` LOG ` is a synonym of ` LN ` . If Y is also present, `\nLOG ` computes the logarithm of X to base Y.\n\nX  |  Y  |  LOG(X, Y)\n---|---|---\n100.0  |  10.0  |  2.0\n` -inf ` |  Any value  |  ` NaN `\nAny value  |  ` +inf ` |  ` NaN `\n` +inf ` |  0.0 < Y < 1.0  |  ` -inf `\n` +inf ` |  Y > 1.0  |  ` +inf `\nX <= 0  |  Any value  |  Error Any value  |  Y <= 0  |  Error Any value  |  1.0  |  Error\n\n**Return Data Type**\n\nINPUT  |  ` INT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `\n---|---|---|---|---\n` INT64 ` |  ` FLOAT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `\n` NUMERIC ` |  ` NUMERIC ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `\n` BIGNUMERIC ` |  ` BIGNUMERIC ` |  ` BIGNUMERIC ` |  ` BIGNUMERIC ` |  `\nFLOAT64 `\n` FLOAT64 ` |  ` FLOAT64 ` |  ` FLOAT64 ` |  ` FLOAT64 ` |  ` FLOAT64 `"
            },
            "LOG10": {
                "name": "LOG10",
                "summary": "Computes the natural logarithm of ` X ` to base 10.",
                "description": "LOG10(X)\n\n**Description**\n\nSimilar to ` LOG ` , but computes logarithm to base 10.\n\nX  |  LOG10(X)\n---|---\n100.0  |  2.0\n` -inf ` |  ` NaN `\n` +inf ` |  ` +inf `\nX <= 0  |  Error\n\n**Return Data Type**\n\nINPUT  |  ` INT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `\n---|---|---|---|---\nOUTPUT  |  ` FLOAT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `"
            },
            "MOD": {
                "name": "MOD",
                "summary": "Gets the remainder of the division of ` X ` by ` Y ` .",
                "description": "MOD(X, Y)\n\n**Description**\n\nModulo function: returns the remainder of the division of X by Y. Returned value has the same sign as X. An error is generated if Y is 0.\n\nX  |  Y  |  MOD(X, Y)\n---|---|---\n25  |  12  |  1 25  |  0  |  Error\n\n**Return Data Type**\n\nThe return data type is determined by the argument types with the following table.\n\nINPUT  |  ` INT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC `\n---|---|---|---\n\n` INT64 ` |  ` INT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC `\n\n` NUMERIC ` |  ` NUMERIC ` |  ` NUMERIC ` |  ` BIGNUMERIC `\n` BIGNUMERIC ` |  ` BIGNUMERIC ` |  ` BIGNUMERIC ` |  ` BIGNUMERIC `"
            },
            "POW": {
                "name": "POW",
                "summary": "Produces the value of ` X ` raised to the power of ` Y ` .",
                "description": "POW(X, Y)\n\n**Description**\n\nReturns the value of X raised to the power of Y. If the result underflows and is not representable, then the function returns a value of zero.\n\nX  |  Y  |  POW(X, Y)\n---|---|---\n2.0  |  3.0  |  8.0 1.0  |  Any value including ` NaN ` |  1.0 Any value including ` NaN ` |  0  |  1.0\n-1.0  |  ` +inf ` |  1.0\n-1.0  |  ` -inf ` |  1.0 ABS(X) < 1  |  ` -inf ` |  ` +inf `\nABS(X) > 1  |  ` -inf ` |  0.0 ABS(X) < 1  |  ` +inf ` |  0.0 ABS(X) > 1  |  ` +inf ` |  ` +inf `\n` -inf ` |  Y < 0  |  0.0\n` -inf ` |  Y > 0  |  ` -inf ` if Y is an odd integer, ` +inf ` otherwise\n` +inf ` |  Y < 0  |  0\n` +inf ` |  Y > 0  |  ` +inf `\nFinite value < 0  |  Non-integer  |  Error 0  |  Finite value < 0  |  Error\n\n**Return Data Type**\n\nThe return data type is determined by the argument types with the following table.\n\nINPUT  |  ` INT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `\n---|---|---|---|---\n` INT64 ` |  ` FLOAT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `\n` NUMERIC ` |  ` NUMERIC ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `\n` BIGNUMERIC ` |  ` BIGNUMERIC ` |  ` BIGNUMERIC ` |  ` BIGNUMERIC ` |  `\nFLOAT64 `\n` FLOAT64 ` |  ` FLOAT64 ` |  ` FLOAT64 ` |  ` FLOAT64 ` |  ` FLOAT64 `"
            },
            "POWER": {
                "name": "POWER",
                "summary": "Synonym of ` POW ` .",
                "description": "POWER(X, Y)\n\n**Description**\n\nSynonym of  ` POW(X, Y) ` ."
            },
            "RAND": {
                "name": "RAND",
                "summary": "Generates a pseudo-random value of type ` FLOAT64 ` in the range of ` [0, 1) ` .",
                "description": "RAND()\n\n**Description**\n\nGenerates a pseudo-random value of type ` FLOAT64 ` in the range of [0, 1),\ninclusive of 0 and exclusive of 1."
            },
            "RANGE_BUCKET": {
                "name": "RANGE_BUCKET",
                "summary": "Scans through a sorted array and returns the 0-based position of a point's upper bound.",
                "description": "RANGE_BUCKET(point, boundaries_array)\n\n**Description**\n\n` RANGE_BUCKET ` scans through a sorted array and returns the 0-based position of the point's upper bound. This can be useful if you need to group your data to build partitions, histograms, business-defined rules, and more.\n\n` RANGE_BUCKET ` follows these rules:\n\n* If the point exists in the array, returns the index of the next larger value.\n\nRANGE_BUCKET(20, [0, 10, 20, 30, 40]) -- 3 is return value RANGE_BUCKET(20, [0, 10, 20, 20, 40, 40]) -- 4 is return value\n\n* If the point does not exist in the array, but it falls between two values, returns the index of the larger value.\n\nRANGE_BUCKET(25, [0, 10, 20, 30, 40]) -- 3 is return value\n\n* If the point is smaller than the first value in the array, returns 0.\n\nRANGE_BUCKET(-10, [5, 10, 20, 30, 40]) -- 0 is return value\n\n* If the point is greater than or equal to the last value in the array, returns the length of the array.\n\nRANGE_BUCKET(80, [0, 10, 20, 30, 40]) -- 5 is return value\n\n* If the array is empty, returns 0.\n\nRANGE_BUCKET(80, []) -- 0 is return value\n\n* If the point is ` NULL ` or ` NaN ` , returns ` NULL ` .\n\nRANGE_BUCKET(NULL, [0, 10, 20, 30, 40]) -- NULL is return value\n\n* The data type for the point and array must be compatible.\n\nRANGE_BUCKET('a', ['a', 'b', 'c', 'd']) -- 1 is return value RANGE_BUCKET(1.2, [1, 1.2, 1.4, 1.6]) -- 2 is return value RANGE_BUCKET(1.2, [1, 2, 4, 6]) -- execution failure\n\nExecution failure occurs when:\n\n* The array has a ` NaN ` or ` NULL ` value in it.\n\nRANGE_BUCKET(80, [NULL, 10, 20, 30, 40]) -- execution failure\n\n* The array is not sorted in ascending order.\n\nRANGE_BUCKET(30, [10, 30, 20, 40, 50]) -- execution failure\n\n**Parameters**\n\n* ` point ` : A generic value.\n* ` boundaries_array ` : A generic array of values.\n\n**Note:** The data type for ` point ` and the element type of `\nboundaries_array ` must be equivalent. The data type must be [ comparable\n](/bigquery/docs/reference/standard-sql/data-types#data_type_properties) .\n\n**Return Value**\n\n` INT64 `\n\n**Examples**\n\nIn a table called ` students ` , check to see how many records would exist in each ` age_group ` bucket, based on a student's age:\n\n* age_group 0 (age < 10)\n* age_group 1 (age >= 10, age < 20)\n* age_group 2 (age >= 20, age < 30)\n* age_group 3 (age >= 30)\n\n\nWITH students AS ( SELECT 9 AS age UNION ALL SELECT 20 AS age UNION ALL SELECT 25 AS age UNION ALL SELECT 31 AS age UNION ALL SELECT 32 AS age UNION ALL SELECT 33 AS age ) SELECT RANGE_BUCKET(age, [10, 20, 30]) AS age_group, COUNT(*) AS count FROM students GROUP BY 1\n\n/*--------------+-------*\n| age_group    | count |\n+--------------+-------+\n| 0            | 1     |\n| 2            | 2     |\n| 3            | 3     |\n*--------------+-------*/"
            },
            "ROUND": {
                "name": "ROUND",
                "summary": "Rounds ` X ` to the nearest integer or rounds ` X ` to ` N `\ndecimal places after the decimal point.",
                "description": "ROUND(X [, N [, rounding_mode]])\n\n**Description**\n\nIf only X is present, rounds X to the nearest integer. If N is present, rounds X to N decimal places after the decimal point. If N is negative, rounds off digits to the left of the decimal point. Rounds halfway cases away from zero.\nGenerates an error if overflow occurs.\n\nIf X is a ` NUMERIC ` or ` BIGNUMERIC ` type, then you can explicitly set `\nrounding_mode ` to one of the following:\n\n* [ ` \"ROUND_HALF_AWAY_FROM_ZERO\" ` ](https://en.wikipedia.org/wiki/Rounding#Rounding_half_away_from_zero) : (Default) Rounds halfway cases away from zero.\n* [ ` \"ROUND_HALF_EVEN\" ` ](https://en.wikipedia.org/wiki/Rounding#Rounding_half_to_even) : Rounds halfway cases towards the nearest even digit.\n\nIf you set the ` rounding_mode ` and X is not a ` NUMERIC ` or ` BIGNUMERIC `\ntype, then the function generates an error.\n\nExpression  |  Return Value\n---|---\n` ROUND(2.0) ` |  2.0\n` ROUND(2.3) ` |  2.0\n` ROUND(2.8) ` |  3.0\n` ROUND(2.5) ` |  3.0\n` ROUND(-2.3) ` |  -2.0\n` ROUND(-2.8) ` |  -3.0\n` ROUND(-2.5) ` |  -3.0\n` ROUND(0) ` |  0\n` ROUND(+inf) ` |  ` +inf `\n` ROUND(-inf) ` |  ` -inf `\n` ROUND(NaN) ` |  ` NaN `\n` ROUND(123.7, -1) ` |  120.0\n` ROUND(1.235, 2) ` |  1.24\n` ROUND(NUMERIC \"2.25\", 1, \"ROUND_HALF_EVEN\") ` |  2.2\n` ROUND(NUMERIC \"2.35\", 1, \"ROUND_HALF_EVEN\") ` |  2.4\n` ROUND(NUMERIC \"2.251\", 1, \"ROUND_HALF_EVEN\") ` |  2.3\n` ROUND(NUMERIC \"-2.5\", 0, \"ROUND_HALF_EVEN\") ` |  -2\n` ROUND(NUMERIC \"2.5\", 0, \"ROUND_HALF_AWAY_FROM_ZERO\") ` |  3\n` ROUND(NUMERIC \"-2.5\", 0, \"ROUND_HALF_AWAY_FROM_ZERO\") ` |  -3\n\n**Return Data Type**\n\nINPUT  |  ` INT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `\n---|---|---|---|---\nOUTPUT  |  ` FLOAT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `"
            },
            "SAFE_ADD": {
                "name": "SAFE_ADD",
                "summary": "Equivalent to the addition operator ( ` X + Y ` ), but returns\n` NULL ` if overflow occurs.",
                "description": "SAFE_ADD(X, Y)\n\n**Description**\n\nEquivalent to the addition operator ( ` + ` ), but returns ` NULL ` if overflow occurs.\n\nX  |  Y  |  SAFE_ADD(X, Y)\n---|---|---\n5  |  4  |  9\n\n**Return Data Type**\n\nINPUT  |  ` INT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `\n---|---|---|---|---\n` INT64 ` |  ` INT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `\n` NUMERIC ` |  ` NUMERIC ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `\n` BIGNUMERIC ` |  ` BIGNUMERIC ` |  ` BIGNUMERIC ` |  ` BIGNUMERIC ` |  `\nFLOAT64 `\n` FLOAT64 ` |  ` FLOAT64 ` |  ` FLOAT64 ` |  ` FLOAT64 ` |  ` FLOAT64 `"
            },
            "SAFE_DIVIDE": {
                "name": "SAFE_DIVIDE",
                "summary": "Equivalent to the division operator ( ` X / Y ` ), but returns ` NULL ` if an error occurs.",
                "description": "SAFE_DIVIDE(X, Y)\n\n**Description**\n\nEquivalent to the division operator ( ` X / Y ` ), but returns ` NULL ` if an error occurs, such as a division by zero error.\n\nX  |  Y  |  SAFE_DIVIDE(X, Y)\n---|---|---\n20  |  4  |  5 0  |  20  |  ` 0 `\n20  |  0  |  ` NULL `\n\n**Return Data Type**\n\nINPUT  |  ` INT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `\n---|---|---|---|---\n` INT64 ` |  ` FLOAT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `\n` NUMERIC ` |  ` NUMERIC ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `\n` BIGNUMERIC ` |  ` BIGNUMERIC ` |  ` BIGNUMERIC ` |  ` BIGNUMERIC ` |  `\nFLOAT64 `\n` FLOAT64 ` |  ` FLOAT64 ` |  ` FLOAT64 ` |  ` FLOAT64 ` |  ` FLOAT64 `"
            },
            "SAFE_MULTIPLY": {
                "name": "SAFE_MULTIPLY",
                "summary": "Equivalent to the multiplication operator ( ` X * Y ` ),\nbut returns ` NULL ` if overflow occurs.",
                "description": "SAFE_MULTIPLY(X, Y)\n\n**Description**\n\nEquivalent to the multiplication operator ( ` * ` ), but returns ` NULL ` if overflow occurs.\n\nX  |  Y  |  SAFE_MULTIPLY(X, Y)\n---|---|---\n20  |  4  |  80\n\n**Return Data Type**\n\nINPUT  |  ` INT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `\n---|---|---|---|---\n` INT64 ` |  ` INT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `\n` NUMERIC ` |  ` NUMERIC ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `\n` BIGNUMERIC ` |  ` BIGNUMERIC ` |  ` BIGNUMERIC ` |  ` BIGNUMERIC ` |  `\nFLOAT64 `\n` FLOAT64 ` |  ` FLOAT64 ` |  ` FLOAT64 ` |  ` FLOAT64 ` |  ` FLOAT64 `"
            },
            "SAFE_NEGATE": {
                "name": "SAFE_NEGATE",
                "summary": "Equivalent to the unary minus operator ( ` -X ` ), but returns ` NULL ` if overflow occurs.",
                "description": "SAFE_NEGATE(X)\n\n**Description**\n\nEquivalent to the unary minus operator ( ` - ` ), but returns ` NULL ` if overflow occurs.\n\nX  |  SAFE_NEGATE(X)\n---|---\n+1  |  -1\n-1  |  +1 0  |  0\n\n**Return Data Type**\n\nINPUT  |  ` INT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `\n---|---|---|---|---\nOUTPUT  |  ` INT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `"
            },
            "SAFE_SUBTRACT": {
                "name": "SAFE_SUBTRACT",
                "summary": "Equivalent to the subtraction operator ( ` X - Y ` ), but returns ` NULL ` if overflow occurs.",
                "description": "SAFE_SUBTRACT(X, Y)\n\n**Description**\n\nReturns the result of Y subtracted from X. Equivalent to the subtraction operator ( ` - ` ), but returns ` NULL ` if overflow occurs.\n\nX  |  Y  |  SAFE_SUBTRACT(X, Y)\n---|---|---\n5  |  4  |  1\n\n**Return Data Type**\n\nINPUT  |  ` INT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `\n---|---|---|---|---\n` INT64 ` |  ` INT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `\n` NUMERIC ` |  ` NUMERIC ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `\n` BIGNUMERIC ` |  ` BIGNUMERIC ` |  ` BIGNUMERIC ` |  ` BIGNUMERIC ` |  `\nFLOAT64 `\n` FLOAT64 ` |  ` FLOAT64 ` |  ` FLOAT64 ` |  ` FLOAT64 ` |  ` FLOAT64 `"
            },
            "SEC": {
                "name": "SEC",
                "summary": "Computes the secant of ` X ` .",
                "description": "SEC(X)\n\n**Description**\n\nComputes the secant for the angle of ` X ` , where ` X ` is specified in radians. ` X ` can be any data type that [ coerces to ` FLOAT64 `\n](/bigquery/docs/reference/standard-sql/conversion_rules#conversion_rules) .\n\nX  |  SEC(X)\n---|---\n` +inf ` |  ` NaN `\n` -inf ` |  ` NaN `\n` NaN ` |  ` NaN `\n` NULL ` |  ` NULL `\n\n**Return Data Type**\n\n` FLOAT64 `\n\n**Example**\n\n\nSELECT SEC(100) AS a, SEC(-1) AS b;\n\n/*----------------+---------------*\n| a              | b             |\n+----------------+---------------+\n| 1.159663822905 | 1.85081571768 |\n*----------------+---------------*/"
            },
            "SECH": {
                "name": "SECH",
                "summary": "Computes the hyperbolic secant of ` X ` .",
                "description": "SECH(X)\n\n**Description**\n\nComputes the hyperbolic secant for the angle of ` X ` , where ` X ` is specified in radians. ` X ` can be any data type that [ coerces to ` FLOAT64 `\n](/bigquery/docs/reference/standard-sql/conversion_rules#conversion_rules) .\nNever produces an error.\n\nX  |  SECH(X)\n---|---\n` +inf ` |  ` 0 `\n` -inf ` |  ` 0 `\n` NaN ` |  ` NaN `\n` NULL ` |  ` NULL `\n\n**Return Data Type**\n\n` FLOAT64 `\n\n**Example**\n\n\nSELECT SECH(0.5) AS a, SECH(-2) AS b, SECH(100) AS c;\n\n/*----------------+----------------+---------------------*\n| a              | b              | c                   |\n+----------------+----------------+---------------------+\n| 0.88681888397  | 0.265802228834 | 7.4401519520417E-44 |\n*----------------+----------------+---------------------*/"
            },
            "SIGN": {
                "name": "SIGN",
                "summary": "Produces -1 , 0, or +1 for negative, zero, and positive arguments respectively.",
                "description": "SIGN(X)\n\n**Description**\n\nReturns ` -1 ` , ` 0 ` , or ` +1 ` for negative, zero and positive arguments respectively. For floating point arguments, this function does not distinguish between positive and negative zero.\n\nX  |  SIGN(X)\n---|---\n25  |  +1 0  |  0\n-25  |  -1 NaN  |  NaN\n\n**Return Data Type**\n\nINPUT  |  ` INT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `\n---|---|---|---|---\nOUTPUT  |  ` INT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `"
            },
            "SIN": {
                "name": "SIN",
                "summary": "Computes the sine of ` X ` .",
                "description": "SIN(X)\n\n**Description**\n\nComputes the sine of X where X is specified in radians. Never fails.\n\nX  |  SIN(X)\n---|---\n` +inf ` |  ` NaN `\n` -inf ` |  ` NaN `\n` NaN ` |  ` NaN `"
            },
            "SINH": {
                "name": "SINH",
                "summary": "Computes the hyperbolic sine of ` X ` .",
                "description": "SINH(X)\n\n**Description**\n\nComputes the hyperbolic sine of X where X is specified in radians. Generates an error if overflow occurs.\n\nX  |  SINH(X)\n---|---\n` +inf ` |  ` +inf `\n` -inf ` |  ` -inf `\n` NaN ` |  ` NaN `"
            },
            "SQRT": {
                "name": "SQRT",
                "summary": "Computes the square root of ` X ` .",
                "description": "SQRT(X)\n\n**Description**\n\nComputes the square root of X. Generates an error if X is less than 0.\n\nX  |  SQRT(X)\n---|---\n` 25.0 ` |  ` 5.0 `\n` +inf ` |  ` +inf `\n` X < 0 ` |  Error\n\n**Return Data Type**\n\nINPUT  |  ` INT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `\n---|---|---|---|---\nOUTPUT  |  ` FLOAT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `"
            },
            "TAN": {
                "name": "TAN",
                "summary": "Computes the tangent of ` X ` .",
                "description": "TAN(X)\n\n**Description**\n\nComputes the tangent of X where X is specified in radians. Generates an error if overflow occurs.\n\nX  |  TAN(X)\n---|---\n` +inf ` |  ` NaN `\n` -inf ` |  ` NaN `\n` NaN ` |  ` NaN `"
            },
            "TANH": {
                "name": "TANH",
                "summary": "Computes the hyperbolic tangent of ` X ` .",
                "description": "TANH(X)\n\n**Description**\n\nComputes the hyperbolic tangent of X where X is specified in radians. Does not fail.\n\nX  |  TANH(X)\n---|---\n` +inf ` |  1.0\n` -inf ` |  -1.0\n` NaN ` |  ` NaN `"
            },
            "TRUNC": {
                "name": "TRUNC",
                "summary": "Rounds a number like ` ROUND(X) ` or ` ROUND(X, N) ` , but always rounds towards zero and never overflows.",
                "description": "TRUNC(X [, N])\n\n**Description**\n\nIf only X is present, ` TRUNC ` rounds X to the nearest integer whose absolute value is not greater than the absolute value of X. If N is also present, `\nTRUNC ` behaves like ` ROUND(X, N) ` , but always rounds towards zero and never overflows.\n\nX  |  TRUNC(X)\n---|---\n2.0  |  2.0 2.3  |  2.0 2.8  |  2.0 2.5  |  2.0\n-2.3  |  -2.0\n-2.8  |  -2.0\n-2.5  |  -2.0 0  |  0\n` +inf ` |  ` +inf `\n` -inf ` |  ` -inf `\n` NaN ` |  ` NaN `\n\n**Return Data Type**\n\nINPUT  |  ` INT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `\n---|---|---|---|---\nOUTPUT  |  ` FLOAT64 ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `"
            }
        }
    },
    {
        "category": "navigation-functions",
        "description": "GoogleSQL for BigQuery supports navigation functions. Navigation functions are a subset of window functions. To create a window function call and learn about the syntax for window functions, see [ Window function_calls\n](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\nNavigation functions generally compute some ` value_expression ` over a different row in the window frame from the current row. The ` OVER ` clause syntax varies across navigation functions.\n\nFor all navigation functions, the result data type is the same type as `\nvalue_expression ` .",
        "source": "navigation_functions.txt",
        "functions": {
            "FIRST_VALUE": {
                "name": "FIRST_VALUE",
                "summary": "Gets a value for the first row in the current window frame.",
                "description": "FIRST_VALUE (value_expression [{RESPECT | IGNORE} NULLS]) OVER over_clause\n\nover_clause:\n{ named_window | ( [ window_specification ] ) }\n\nwindow_specification:\n[ named_window ]\n[ PARTITION BY partition_expression [, ...] ]\nORDER BY expression [ { ASC | DESC }  ] [, ...]\n[ window_frame_clause ]\n\n\n**Description**\n\nReturns the value of the ` value_expression ` for the first row in the current window frame.\n\nThis function includes ` NULL ` values in the calculation unless ` IGNORE NULLS ` is present. If ` IGNORE NULLS ` is present, the function excludes `\nNULL ` values from the calculation.\n\nTo learn more about the ` OVER ` clause and how to use it, see [ Window function calls ](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n**Supported Argument Types**\n\n` value_expression ` can be any data type that an expression can return.\n\n**Return Data Type**\n\nSame type as ` value_expression ` .\n\n**Examples**\n\nThe following example computes the fastest time for each division.\n\n\nWITH finishers AS (SELECT 'Sophia Liu' as name,\nTIMESTAMP '2016-10-18 2:51:45' as finish_time,\n'F30-34' as division UNION ALL SELECT 'Lisa Stelzner', TIMESTAMP '2016-10-18 2:54:11', 'F35-39'\nUNION ALL SELECT 'Nikki Leith', TIMESTAMP '2016-10-18 2:59:01', 'F30-34'\nUNION ALL SELECT 'Lauren Matthews', TIMESTAMP '2016-10-18 3:01:17', 'F35-39'\nUNION ALL SELECT 'Desiree Berry', TIMESTAMP '2016-10-18 3:05:42', 'F35-39'\nUNION ALL SELECT 'Suzy Slane', TIMESTAMP '2016-10-18 3:06:24', 'F35-39'\nUNION ALL SELECT 'Jen Edwards', TIMESTAMP '2016-10-18 3:06:36', 'F30-34'\nUNION ALL SELECT 'Meghan Lederer', TIMESTAMP '2016-10-18 3:07:41', 'F30-34'\nUNION ALL SELECT 'Carly Forte', TIMESTAMP '2016-10-18 3:08:58', 'F25-29'\nUNION ALL SELECT 'Lauren Reasoner', TIMESTAMP '2016-10-18 3:10:14', 'F30-34') SELECT name,\nFORMAT_TIMESTAMP('%X', finish_time) AS finish_time,\ndivision,\nFORMAT_TIMESTAMP('%X', fastest_time) AS fastest_time,\nTIMESTAMP_DIFF(finish_time, fastest_time, SECOND) AS delta_in_seconds FROM ( SELECT name,\nfinish_time,\ndivision,\nFIRST_VALUE(finish_time) OVER (PARTITION BY division ORDER BY finish_time ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS fastest_time FROM finishers);\n\n/*-----------------+-------------+----------+--------------+------------------*\n| name            | finish_time | division | fastest_time | delta_in_seconds |\n+-----------------+-------------+----------+--------------+------------------+\n| Carly Forte     | 03:08:58    | F25-29   | 03:08:58     | 0                |\n| Sophia Liu      | 02:51:45    | F30-34   | 02:51:45     | 0                |\n| Nikki Leith     | 02:59:01    | F30-34   | 02:51:45     | 436              |\n| Jen Edwards     | 03:06:36    | F30-34   | 02:51:45     | 891              |\n| Meghan Lederer  | 03:07:41    | F30-34   | 02:51:45     | 956              |\n| Lauren Reasoner | 03:10:14    | F30-34   | 02:51:45     | 1109             |\n| Lisa Stelzner   | 02:54:11    | F35-39   | 02:54:11     | 0                |\n| Lauren Matthews | 03:01:17    | F35-39   | 02:54:11     | 426              |\n| Desiree Berry   | 03:05:42    | F35-39   | 02:54:11     | 691              |\n| Suzy Slane      | 03:06:24    | F35-39   | 02:54:11     | 733              |\n*-----------------+-------------+----------+--------------+------------------*/"
            },
            "LAG": {
                "name": "LAG",
                "summary": "Gets a value for a preceding row.",
                "description": "LAG (value_expression[, offset [, default_expression]]) OVER over_clause\n\nover_clause:\n{ named_window | ( [ window_specification ] ) }\n\nwindow_specification:\n[ named_window ]\n[ PARTITION BY partition_expression [, ...] ]\nORDER BY expression [ { ASC | DESC }  ] [, ...]\n\n\n**Description**\n\nReturns the value of the ` value_expression ` on a preceding row. Changing the\n` offset ` value changes which preceding row is returned; the default value is\n` 1 ` , indicating the previous row in the window frame. An error occurs if `\noffset ` is NULL or a negative value.\n\nThe optional ` default_expression ` is used if there isn't a row in the window frame at the specified offset. This expression must be a constant expression and its type must be implicitly coercible to the type of ` value_expression `\n. If left unspecified, ` default_expression ` defaults to NULL.\n\nTo learn more about the ` OVER ` clause and how to use it, see [ Window function calls ](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n**Supported Argument Types**\n\n* ` value_expression ` can be any data type that can be returned from an expression.\n* ` offset ` must be a non-negative integer literal or parameter.\n* ` default_expression ` must be compatible with the value expression type.\n\n**Return Data Type**\n\nSame type as ` value_expression ` .\n\n**Examples**\n\nThe following example illustrates a basic use of the ` LAG ` function.\n\n\nWITH finishers AS (SELECT 'Sophia Liu' as name,\nTIMESTAMP '2016-10-18 2:51:45' as finish_time,\n'F30-34' as division UNION ALL SELECT 'Lisa Stelzner', TIMESTAMP '2016-10-18 2:54:11', 'F35-39'\nUNION ALL SELECT 'Nikki Leith', TIMESTAMP '2016-10-18 2:59:01', 'F30-34'\nUNION ALL SELECT 'Lauren Matthews', TIMESTAMP '2016-10-18 3:01:17', 'F35-39'\nUNION ALL SELECT 'Desiree Berry', TIMESTAMP '2016-10-18 3:05:42', 'F35-39'\nUNION ALL SELECT 'Suzy Slane', TIMESTAMP '2016-10-18 3:06:24', 'F35-39'\nUNION ALL SELECT 'Jen Edwards', TIMESTAMP '2016-10-18 3:06:36', 'F30-34'\nUNION ALL SELECT 'Meghan Lederer', TIMESTAMP '2016-10-18 3:07:41', 'F30-34'\nUNION ALL SELECT 'Carly Forte', TIMESTAMP '2016-10-18 3:08:58', 'F25-29'\nUNION ALL SELECT 'Lauren Reasoner', TIMESTAMP '2016-10-18 3:10:14', 'F30-34') SELECT name,\nfinish_time,\ndivision,\nLAG(name) OVER (PARTITION BY division ORDER BY finish_time ASC) AS preceding_runner FROM finishers;\n\n/*-----------------+-------------+----------+------------------*\n| name            | finish_time | division | preceding_runner |\n+-----------------+-------------+----------+------------------+\n| Carly Forte     | 03:08:58    | F25-29   | NULL             |\n| Sophia Liu      | 02:51:45    | F30-34   | NULL             |\n| Nikki Leith     | 02:59:01    | F30-34   | Sophia Liu       |\n| Jen Edwards     | 03:06:36    | F30-34   | Nikki Leith      |\n| Meghan Lederer  | 03:07:41    | F30-34   | Jen Edwards      |\n| Lauren Reasoner | 03:10:14    | F30-34   | Meghan Lederer   |\n| Lisa Stelzner   | 02:54:11    | F35-39   | NULL             |\n| Lauren Matthews | 03:01:17    | F35-39   | Lisa Stelzner    |\n| Desiree Berry   | 03:05:42    | F35-39   | Lauren Matthews  |\n| Suzy Slane      | 03:06:24    | F35-39   | Desiree Berry    |\n*-----------------+-------------+----------+------------------*/\n\nThis next example uses the optional ` offset ` parameter.\n\n\nWITH finishers AS (SELECT 'Sophia Liu' as name,\nTIMESTAMP '2016-10-18 2:51:45' as finish_time,\n'F30-34' as division UNION ALL SELECT 'Lisa Stelzner', TIMESTAMP '2016-10-18 2:54:11', 'F35-39'\nUNION ALL SELECT 'Nikki Leith', TIMESTAMP '2016-10-18 2:59:01', 'F30-34'\nUNION ALL SELECT 'Lauren Matthews', TIMESTAMP '2016-10-18 3:01:17', 'F35-39'\nUNION ALL SELECT 'Desiree Berry', TIMESTAMP '2016-10-18 3:05:42', 'F35-39'\nUNION ALL SELECT 'Suzy Slane', TIMESTAMP '2016-10-18 3:06:24', 'F35-39'\nUNION ALL SELECT 'Jen Edwards', TIMESTAMP '2016-10-18 3:06:36', 'F30-34'\nUNION ALL SELECT 'Meghan Lederer', TIMESTAMP '2016-10-18 3:07:41', 'F30-34'\nUNION ALL SELECT 'Carly Forte', TIMESTAMP '2016-10-18 3:08:58', 'F25-29'\nUNION ALL SELECT 'Lauren Reasoner', TIMESTAMP '2016-10-18 3:10:14', 'F30-34') SELECT name,\nfinish_time,\ndivision,\nLAG(name, 2) OVER (PARTITION BY division ORDER BY finish_time ASC) AS two_runners_ahead FROM finishers;\n\n/*-----------------+-------------+----------+-------------------*\n| name            | finish_time | division | two_runners_ahead |\n+-----------------+-------------+----------+-------------------+\n| Carly Forte     | 03:08:58    | F25-29   | NULL              |\n| Sophia Liu      | 02:51:45    | F30-34   | NULL              |\n| Nikki Leith     | 02:59:01    | F30-34   | NULL              |\n| Jen Edwards     | 03:06:36    | F30-34   | Sophia Liu        |\n| Meghan Lederer  | 03:07:41    | F30-34   | Nikki Leith       |\n| Lauren Reasoner | 03:10:14    | F30-34   | Jen Edwards       |\n| Lisa Stelzner   | 02:54:11    | F35-39   | NULL              |\n| Lauren Matthews | 03:01:17    | F35-39   | NULL              |\n| Desiree Berry   | 03:05:42    | F35-39   | Lisa Stelzner     |\n| Suzy Slane      | 03:06:24    | F35-39   | Lauren Matthews   |\n*-----------------+-------------+----------+-------------------*/\n\nThe following example replaces NULL values with a default value.\n\n\nWITH finishers AS (SELECT 'Sophia Liu' as name,\nTIMESTAMP '2016-10-18 2:51:45' as finish_time,\n'F30-34' as division UNION ALL SELECT 'Lisa Stelzner', TIMESTAMP '2016-10-18 2:54:11', 'F35-39'\nUNION ALL SELECT 'Nikki Leith', TIMESTAMP '2016-10-18 2:59:01', 'F30-34'\nUNION ALL SELECT 'Lauren Matthews', TIMESTAMP '2016-10-18 3:01:17', 'F35-39'\nUNION ALL SELECT 'Desiree Berry', TIMESTAMP '2016-10-18 3:05:42', 'F35-39'\nUNION ALL SELECT 'Suzy Slane', TIMESTAMP '2016-10-18 3:06:24', 'F35-39'\nUNION ALL SELECT 'Jen Edwards', TIMESTAMP '2016-10-18 3:06:36', 'F30-34'\nUNION ALL SELECT 'Meghan Lederer', TIMESTAMP '2016-10-18 3:07:41', 'F30-34'\nUNION ALL SELECT 'Carly Forte', TIMESTAMP '2016-10-18 3:08:58', 'F25-29'\nUNION ALL SELECT 'Lauren Reasoner', TIMESTAMP '2016-10-18 3:10:14', 'F30-34') SELECT name,\nfinish_time,\ndivision,\nLAG(name, 2, 'Nobody') OVER (PARTITION BY division ORDER BY finish_time ASC) AS two_runners_ahead FROM finishers;\n\n/*-----------------+-------------+----------+-------------------*\n| name            | finish_time | division | two_runners_ahead |\n+-----------------+-------------+----------+-------------------+\n| Carly Forte     | 03:08:58    | F25-29   | Nobody            |\n| Sophia Liu      | 02:51:45    | F30-34   | Nobody            |\n| Nikki Leith     | 02:59:01    | F30-34   | Nobody            |\n| Jen Edwards     | 03:06:36    | F30-34   | Sophia Liu        |\n| Meghan Lederer  | 03:07:41    | F30-34   | Nikki Leith       |\n| Lauren Reasoner | 03:10:14    | F30-34   | Jen Edwards       |\n| Lisa Stelzner   | 02:54:11    | F35-39   | Nobody            |\n| Lauren Matthews | 03:01:17    | F35-39   | Nobody            |\n| Desiree Berry   | 03:05:42    | F35-39   | Lisa Stelzner     |\n| Suzy Slane      | 03:06:24    | F35-39   | Lauren Matthews   |\n*-----------------+-------------+----------+-------------------*/"
            },
            "LAST_VALUE": {
                "name": "LAST_VALUE",
                "summary": "Gets a value for the last row in the current window frame.",
                "description": "LAST_VALUE (value_expression [{RESPECT | IGNORE} NULLS]) OVER over_clause\n\nover_clause:\n{ named_window | ( [ window_specification ] ) }\n\nwindow_specification:\n[ named_window ]\n[ PARTITION BY partition_expression [, ...] ]\nORDER BY expression [ { ASC | DESC }  ] [, ...]\n[ window_frame_clause ]\n\n\n**Description**\n\nReturns the value of the ` value_expression ` for the last row in the current window frame.\n\nThis function includes ` NULL ` values in the calculation unless ` IGNORE NULLS ` is present. If ` IGNORE NULLS ` is present, the function excludes `\nNULL ` values from the calculation.\n\nTo learn more about the ` OVER ` clause and how to use it, see [ Window function calls ](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n**Supported Argument Types**\n\n` value_expression ` can be any data type that an expression can return.\n\n**Return Data Type**\n\nSame type as ` value_expression ` .\n\n**Examples**\n\nThe following example computes the slowest time for each division.\n\n\nWITH finishers AS (SELECT 'Sophia Liu' as name,\nTIMESTAMP '2016-10-18 2:51:45' as finish_time,\n'F30-34' as division UNION ALL SELECT 'Lisa Stelzner', TIMESTAMP '2016-10-18 2:54:11', 'F35-39'\nUNION ALL SELECT 'Nikki Leith', TIMESTAMP '2016-10-18 2:59:01', 'F30-34'\nUNION ALL SELECT 'Lauren Matthews', TIMESTAMP '2016-10-18 3:01:17', 'F35-39'\nUNION ALL SELECT 'Desiree Berry', TIMESTAMP '2016-10-18 3:05:42', 'F35-39'\nUNION ALL SELECT 'Suzy Slane', TIMESTAMP '2016-10-18 3:06:24', 'F35-39'\nUNION ALL SELECT 'Jen Edwards', TIMESTAMP '2016-10-18 3:06:36', 'F30-34'\nUNION ALL SELECT 'Meghan Lederer', TIMESTAMP '2016-10-18 3:07:41', 'F30-34'\nUNION ALL SELECT 'Carly Forte', TIMESTAMP '2016-10-18 3:08:58', 'F25-29'\nUNION ALL SELECT 'Lauren Reasoner', TIMESTAMP '2016-10-18 3:10:14', 'F30-34') SELECT name,\nFORMAT_TIMESTAMP('%X', finish_time) AS finish_time,\ndivision,\nFORMAT_TIMESTAMP('%X', slowest_time) AS slowest_time,\nTIMESTAMP_DIFF(slowest_time, finish_time, SECOND) AS delta_in_seconds FROM ( SELECT name,\nfinish_time,\ndivision,\nLAST_VALUE(finish_time) OVER (PARTITION BY division ORDER BY finish_time ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS slowest_time FROM finishers);\n\n/*-----------------+-------------+----------+--------------+------------------*\n| name            | finish_time | division | slowest_time | delta_in_seconds |\n+-----------------+-------------+----------+--------------+------------------+\n| Carly Forte     | 03:08:58    | F25-29   | 03:08:58     | 0                |\n| Sophia Liu      | 02:51:45    | F30-34   | 03:10:14     | 1109             |\n| Nikki Leith     | 02:59:01    | F30-34   | 03:10:14     | 673              |\n| Jen Edwards     | 03:06:36    | F30-34   | 03:10:14     | 218              |\n| Meghan Lederer  | 03:07:41    | F30-34   | 03:10:14     | 153              |\n| Lauren Reasoner | 03:10:14    | F30-34   | 03:10:14     | 0                |\n| Lisa Stelzner   | 02:54:11    | F35-39   | 03:06:24     | 733              |\n| Lauren Matthews | 03:01:17    | F35-39   | 03:06:24     | 307              |\n| Desiree Berry   | 03:05:42    | F35-39   | 03:06:24     | 42               |\n| Suzy Slane      | 03:06:24    | F35-39   | 03:06:24     | 0                |\n*-----------------+-------------+----------+--------------+------------------*/"
            },
            "LEAD": {
                "name": "LEAD",
                "summary": "Gets a value for a subsequent row.",
                "description": "LEAD (value_expression[, offset [, default_expression]]) OVER over_clause\n\nover_clause:\n{ named_window | ( [ window_specification ] ) }\n\nwindow_specification:\n[ named_window ]\n[ PARTITION BY partition_expression [, ...] ]\nORDER BY expression [ { ASC | DESC }  ] [, ...]\n\n\n**Description**\n\nReturns the value of the ` value_expression ` on a subsequent row. Changing the ` offset ` value changes which subsequent row is returned; the default value is ` 1 ` , indicating the next row in the window frame. An error occurs if ` offset ` is NULL or a negative value.\n\nThe optional ` default_expression ` is used if there isn't a row in the window frame at the specified offset. This expression must be a constant expression and its type must be implicitly coercible to the type of ` value_expression `\n. If left unspecified, ` default_expression ` defaults to NULL.\n\nTo learn more about the ` OVER ` clause and how to use it, see [ Window function calls ](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n**Supported Argument Types**\n\n* ` value_expression ` can be any data type that can be returned from an expression.\n* ` offset ` must be a non-negative integer literal or parameter.\n* ` default_expression ` must be compatible with the value expression type.\n\n**Return Data Type**\n\nSame type as ` value_expression ` .\n\n**Examples**\n\nThe following example illustrates a basic use of the ` LEAD ` function.\n\n\nWITH finishers AS (SELECT 'Sophia Liu' as name,\nTIMESTAMP '2016-10-18 2:51:45' as finish_time,\n'F30-34' as division UNION ALL SELECT 'Lisa Stelzner', TIMESTAMP '2016-10-18 2:54:11', 'F35-39'\nUNION ALL SELECT 'Nikki Leith', TIMESTAMP '2016-10-18 2:59:01', 'F30-34'\nUNION ALL SELECT 'Lauren Matthews', TIMESTAMP '2016-10-18 3:01:17', 'F35-39'\nUNION ALL SELECT 'Desiree Berry', TIMESTAMP '2016-10-18 3:05:42', 'F35-39'\nUNION ALL SELECT 'Suzy Slane', TIMESTAMP '2016-10-18 3:06:24', 'F35-39'\nUNION ALL SELECT 'Jen Edwards', TIMESTAMP '2016-10-18 3:06:36', 'F30-34'\nUNION ALL SELECT 'Meghan Lederer', TIMESTAMP '2016-10-18 3:07:41', 'F30-34'\nUNION ALL SELECT 'Carly Forte', TIMESTAMP '2016-10-18 3:08:58', 'F25-29'\nUNION ALL SELECT 'Lauren Reasoner', TIMESTAMP '2016-10-18 3:10:14', 'F30-34') SELECT name,\nfinish_time,\ndivision,\nLEAD(name) OVER (PARTITION BY division ORDER BY finish_time ASC) AS followed_by FROM finishers;\n\n/*-----------------+-------------+----------+-----------------*\n| name            | finish_time | division | followed_by     |\n+-----------------+-------------+----------+-----------------+\n| Carly Forte     | 03:08:58    | F25-29   | NULL            |\n| Sophia Liu      | 02:51:45    | F30-34   | Nikki Leith     |\n| Nikki Leith     | 02:59:01    | F30-34   | Jen Edwards     |\n| Jen Edwards     | 03:06:36    | F30-34   | Meghan Lederer  |\n| Meghan Lederer  | 03:07:41    | F30-34   | Lauren Reasoner |\n| Lauren Reasoner | 03:10:14    | F30-34   | NULL            |\n| Lisa Stelzner   | 02:54:11    | F35-39   | Lauren Matthews |\n| Lauren Matthews | 03:01:17    | F35-39   | Desiree Berry   |\n| Desiree Berry   | 03:05:42    | F35-39   | Suzy Slane      |\n| Suzy Slane      | 03:06:24    | F35-39   | NULL            |\n*-----------------+-------------+----------+-----------------*/\n\nThis next example uses the optional ` offset ` parameter.\n\n\nWITH finishers AS (SELECT 'Sophia Liu' as name,\nTIMESTAMP '2016-10-18 2:51:45' as finish_time,\n'F30-34' as division UNION ALL SELECT 'Lisa Stelzner', TIMESTAMP '2016-10-18 2:54:11', 'F35-39'\nUNION ALL SELECT 'Nikki Leith', TIMESTAMP '2016-10-18 2:59:01', 'F30-34'\nUNION ALL SELECT 'Lauren Matthews', TIMESTAMP '2016-10-18 3:01:17', 'F35-39'\nUNION ALL SELECT 'Desiree Berry', TIMESTAMP '2016-10-18 3:05:42', 'F35-39'\nUNION ALL SELECT 'Suzy Slane', TIMESTAMP '2016-10-18 3:06:24', 'F35-39'\nUNION ALL SELECT 'Jen Edwards', TIMESTAMP '2016-10-18 3:06:36', 'F30-34'\nUNION ALL SELECT 'Meghan Lederer', TIMESTAMP '2016-10-18 3:07:41', 'F30-34'\nUNION ALL SELECT 'Carly Forte', TIMESTAMP '2016-10-18 3:08:58', 'F25-29'\nUNION ALL SELECT 'Lauren Reasoner', TIMESTAMP '2016-10-18 3:10:14', 'F30-34') SELECT name,\nfinish_time,\ndivision,\nLEAD(name, 2) OVER (PARTITION BY division ORDER BY finish_time ASC) AS two_runners_back FROM finishers;\n\n/*-----------------+-------------+----------+------------------*\n| name            | finish_time | division | two_runners_back |\n+-----------------+-------------+----------+------------------+\n| Carly Forte     | 03:08:58    | F25-29   | NULL             |\n| Sophia Liu      | 02:51:45    | F30-34   | Jen Edwards      |\n| Nikki Leith     | 02:59:01    | F30-34   | Meghan Lederer   |\n| Jen Edwards     | 03:06:36    | F30-34   | Lauren Reasoner  |\n| Meghan Lederer  | 03:07:41    | F30-34   | NULL             |\n| Lauren Reasoner | 03:10:14    | F30-34   | NULL             |\n| Lisa Stelzner   | 02:54:11    | F35-39   | Desiree Berry    |\n| Lauren Matthews | 03:01:17    | F35-39   | Suzy Slane       |\n| Desiree Berry   | 03:05:42    | F35-39   | NULL             |\n| Suzy Slane      | 03:06:24    | F35-39   | NULL             |\n*-----------------+-------------+----------+------------------*/\n\nThe following example replaces NULL values with a default value.\n\n\nWITH finishers AS (SELECT 'Sophia Liu' as name,\nTIMESTAMP '2016-10-18 2:51:45' as finish_time,\n'F30-34' as division UNION ALL SELECT 'Lisa Stelzner', TIMESTAMP '2016-10-18 2:54:11', 'F35-39'\nUNION ALL SELECT 'Nikki Leith', TIMESTAMP '2016-10-18 2:59:01', 'F30-34'\nUNION ALL SELECT 'Lauren Matthews', TIMESTAMP '2016-10-18 3:01:17', 'F35-39'\nUNION ALL SELECT 'Desiree Berry', TIMESTAMP '2016-10-18 3:05:42', 'F35-39'\nUNION ALL SELECT 'Suzy Slane', TIMESTAMP '2016-10-18 3:06:24', 'F35-39'\nUNION ALL SELECT 'Jen Edwards', TIMESTAMP '2016-10-18 3:06:36', 'F30-34'\nUNION ALL SELECT 'Meghan Lederer', TIMESTAMP '2016-10-18 3:07:41', 'F30-34'\nUNION ALL SELECT 'Carly Forte', TIMESTAMP '2016-10-18 3:08:58', 'F25-29'\nUNION ALL SELECT 'Lauren Reasoner', TIMESTAMP '2016-10-18 3:10:14', 'F30-34') SELECT name,\nfinish_time,\ndivision,\nLEAD(name, 2, 'Nobody') OVER (PARTITION BY division ORDER BY finish_time ASC) AS two_runners_back FROM finishers;\n\n/*-----------------+-------------+----------+------------------*\n| name            | finish_time | division | two_runners_back |\n+-----------------+-------------+----------+------------------+\n| Carly Forte     | 03:08:58    | F25-29   | Nobody           |\n| Sophia Liu      | 02:51:45    | F30-34   | Jen Edwards      |\n| Nikki Leith     | 02:59:01    | F30-34   | Meghan Lederer   |\n| Jen Edwards     | 03:06:36    | F30-34   | Lauren Reasoner  |\n| Meghan Lederer  | 03:07:41    | F30-34   | Nobody           |\n| Lauren Reasoner | 03:10:14    | F30-34   | Nobody           |\n| Lisa Stelzner   | 02:54:11    | F35-39   | Desiree Berry    |\n| Lauren Matthews | 03:01:17    | F35-39   | Suzy Slane       |\n| Desiree Berry   | 03:05:42    | F35-39   | Nobody           |\n| Suzy Slane      | 03:06:24    | F35-39   | Nobody           |\n*-----------------+-------------+----------+------------------*/"
            },
            "NTH_VALUE": {
                "name": "NTH_VALUE",
                "summary": "Gets a value for the Nth row of the current window frame.",
                "description": "NTH_VALUE (value_expression, constant_integer_expression [{RESPECT | IGNORE} NULLS]) OVER over_clause\n\nover_clause:\n{ named_window | ( [ window_specification ] ) }\n\nwindow_specification:\n[ named_window ]\n[ PARTITION BY partition_expression [, ...] ]\nORDER BY expression [ { ASC | DESC }  ] [, ...]\n[ window_frame_clause ]\n\n\n**Description**\n\nReturns the value of ` value_expression ` at the Nth row of the current window frame, where Nth is defined by ` constant_integer_expression ` . Returns NULL if there is no such row.\n\nThis function includes ` NULL ` values in the calculation unless ` IGNORE NULLS ` is present. If ` IGNORE NULLS ` is present, the function excludes `\nNULL ` values from the calculation.\n\nTo learn more about the ` OVER ` clause and how to use it, see [ Window function calls ](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n**Supported Argument Types**\n\n* ` value_expression ` can be any data type that can be returned from an expression.\n* ` constant_integer_expression ` can be any constant expression that returns an integer.\n\n**Return Data Type**\n\nSame type as ` value_expression ` .\n\n**Examples**\n\n\nWITH finishers AS (SELECT 'Sophia Liu' as name,\nTIMESTAMP '2016-10-18 2:51:45' as finish_time,\n'F30-34' as division UNION ALL SELECT 'Lisa Stelzner', TIMESTAMP '2016-10-18 2:54:11', 'F35-39'\nUNION ALL SELECT 'Nikki Leith', TIMESTAMP '2016-10-18 2:59:01', 'F30-34'\nUNION ALL SELECT 'Lauren Matthews', TIMESTAMP '2016-10-18 3:01:17', 'F35-39'\nUNION ALL SELECT 'Desiree Berry', TIMESTAMP '2016-10-18 3:05:42', 'F35-39'\nUNION ALL SELECT 'Suzy Slane', TIMESTAMP '2016-10-18 3:06:24', 'F35-39'\nUNION ALL SELECT 'Jen Edwards', TIMESTAMP '2016-10-18 3:06:36', 'F30-34'\nUNION ALL SELECT 'Meghan Lederer', TIMESTAMP '2016-10-18 3:07:41', 'F30-34'\nUNION ALL SELECT 'Carly Forte', TIMESTAMP '2016-10-18 3:08:58', 'F25-29'\nUNION ALL SELECT 'Lauren Reasoner', TIMESTAMP '2016-10-18 3:10:14', 'F30-34') SELECT name,\nFORMAT_TIMESTAMP('%X', finish_time) AS finish_time,\ndivision,\nFORMAT_TIMESTAMP('%X', fastest_time) AS fastest_time,\nFORMAT_TIMESTAMP('%X', second_fastest) AS second_fastest FROM ( SELECT name,\nfinish_time,\ndivision,finishers,\nFIRST_VALUE(finish_time) OVER w1 AS fastest_time,\nNTH_VALUE(finish_time, 2) OVER w1 as second_fastest FROM finishers WINDOW w1 AS ( PARTITION BY division ORDER BY finish_time ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING));\n\n/*-----------------+-------------+----------+--------------+----------------*\n| name            | finish_time | division | fastest_time | second_fastest |\n+-----------------+-------------+----------+--------------+----------------+\n| Carly Forte     | 03:08:58    | F25-29   | 03:08:58     | NULL           |\n| Sophia Liu      | 02:51:45    | F30-34   | 02:51:45     | 02:59:01       |\n| Nikki Leith     | 02:59:01    | F30-34   | 02:51:45     | 02:59:01       |\n| Jen Edwards     | 03:06:36    | F30-34   | 02:51:45     | 02:59:01       |\n| Meghan Lederer  | 03:07:41    | F30-34   | 02:51:45     | 02:59:01       |\n| Lauren Reasoner | 03:10:14    | F30-34   | 02:51:45     | 02:59:01       |\n| Lisa Stelzner   | 02:54:11    | F35-39   | 02:54:11     | 03:01:17       |\n| Lauren Matthews | 03:01:17    | F35-39   | 02:54:11     | 03:01:17       |\n| Desiree Berry   | 03:05:42    | F35-39   | 02:54:11     | 03:01:17       |\n| Suzy Slane      | 03:06:24    | F35-39   | 02:54:11     | 03:01:17       |\n*-----------------+-------------+----------+--------------+----------------*/"
            },
            "PERCENTILE_CONT": {
                "name": "PERCENTILE_CONT",
                "summary": "Computes the specified percentile for a value, using linear interpolation.",
                "description": "PERCENTILE_CONT (value_expression, percentile [{RESPECT | IGNORE} NULLS]) OVER over_clause\n\nover_clause:\n{ named_window | ( [ window_specification ] ) }\n\nwindow_specification:\n[ named_window ]\n[ PARTITION BY partition_expression [, ...] ]\n\n\n**Description**\n\nComputes the specified percentile value for the value_expression, with linear interpolation.\n\nThis function ignores NULL values if ` RESPECT NULLS ` is absent. If ` RESPECT NULLS ` is present:\n\n* Interpolation between two ` NULL ` values returns ` NULL ` .\n* Interpolation between a ` NULL ` value and a non- ` NULL ` value returns the non- ` NULL ` value.\n\nTo learn more about the ` OVER ` clause and how to use it, see [ Window function calls ](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n` PERCENTILE_CONT ` can be used with differential privacy. To learn more, see\n[ Differentially private aggregate functions\n](/bigquery/docs/reference/standard-sql/aggregate-dp-functions) .\n\n**Supported Argument Types**\n\n* ` value_expression ` and ` percentile ` must have one of the following types:\n* ` NUMERIC `\n* ` BIGNUMERIC `\n* ` FLOAT64 `\n* ` percentile ` must be a literal in the range ` [0, 1] ` .\n\n**Return Data Type**\n\nThe return data type is determined by the argument types with the following table.\n\nINPUT  |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `\n---|---|---|---\n` NUMERIC ` |  ` NUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `\n` BIGNUMERIC ` |  ` BIGNUMERIC ` |  ` BIGNUMERIC ` |  ` FLOAT64 `\n` FLOAT64 ` |  ` FLOAT64 ` |  ` FLOAT64 ` |  ` FLOAT64 `\n\n**Examples**\n\nThe following example computes the value for some percentiles from a column of values while ignoring nulls.\n\n\nSELECT PERCENTILE_CONT(x, 0) OVER() AS min,\nPERCENTILE_CONT(x, 0.01) OVER() AS percentile1,\nPERCENTILE_CONT(x, 0.5) OVER() AS median,\nPERCENTILE_CONT(x, 0.9) OVER() AS percentile90,\nPERCENTILE_CONT(x, 1) OVER() AS max FROM UNNEST([0, 3, NULL, 1, 2]) AS x LIMIT 1;\n\n/*-----+-------------+--------+--------------+-----*\n| min | percentile1 | median | percentile90 | max |\n+-----+-------------+--------+--------------+-----+\n| 0   | 0.03        | 1.5    | 2.7          | 3   |\n*-----+-------------+--------+--------------+-----*/\n\nThe following example computes the value for some percentiles from a column of values while respecting nulls.\n\n\nSELECT PERCENTILE_CONT(x, 0 RESPECT NULLS) OVER() AS min,\nPERCENTILE_CONT(x, 0.01 RESPECT NULLS) OVER() AS percentile1,\nPERCENTILE_CONT(x, 0.5 RESPECT NULLS) OVER() AS median,\nPERCENTILE_CONT(x, 0.9 RESPECT NULLS) OVER() AS percentile90,\nPERCENTILE_CONT(x, 1 RESPECT NULLS) OVER() AS max FROM UNNEST([0, 3, NULL, 1, 2]) AS x LIMIT 1;\n\n/*------+-------------+--------+--------------+-----*\n| min  | percentile1 | median | percentile90 | max |\n+------+-------------+--------+--------------+-----+\n| NULL | 0           | 1      | 2.6          | 3   |\n*------+-------------+--------+--------------+-----*/"
            },
            "PERCENTILE_DISC": {
                "name": "PERCENTILE_DISC",
                "summary": "Computes the specified percentile for a discrete value.",
                "description": "PERCENTILE_DISC (value_expression, percentile [{RESPECT | IGNORE} NULLS]) OVER over_clause\n\nover_clause:\n{ named_window | ( [ window_specification ] ) }\n\nwindow_specification:\n[ named_window ]\n[ PARTITION BY partition_expression [, ...] ]\n\n\n**Description**\n\nComputes the specified percentile value for a discrete ` value_expression ` .\nThe returned value is the first sorted value of ` value_expression ` with cumulative distribution greater than or equal to the given ` percentile `\nvalue.\n\nThis function ignores ` NULL ` values unless ` RESPECT NULLS ` is present.\n\nTo learn more about the ` OVER ` clause and how to use it, see [ Window function calls ](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n**Supported Argument Types**\n\n* ` value_expression ` can be any orderable type.\n* ` percentile ` must be a literal in the range ` [0, 1] ` , with one of the following types:\n* ` NUMERIC `\n* ` BIGNUMERIC `\n* ` FLOAT64 `\n\n**Return Data Type**\n\nSame type as ` value_expression ` .\n\n**Examples**\n\nThe following example computes the value for some percentiles from a column of values while ignoring nulls.\n\n\nSELECT x,\nPERCENTILE_DISC(x, 0) OVER() AS min,\nPERCENTILE_DISC(x, 0.5) OVER() AS median,\nPERCENTILE_DISC(x, 1) OVER() AS max FROM UNNEST(['c', NULL, 'b', 'a']) AS x;\n\n/*------+-----+--------+-----*\n| x    | min | median | max |\n+------+-----+--------+-----+\n| c    | a   | b      | c   |\n| NULL | a   | b      | c   |\n| b    | a   | b      | c   |\n| a    | a   | b      | c   |\n*------+-----+--------+-----*/\n\nThe following example computes the value for some percentiles from a column of values while respecting nulls.\n\n\nSELECT x,\nPERCENTILE_DISC(x, 0 RESPECT NULLS) OVER() AS min,\nPERCENTILE_DISC(x, 0.5 RESPECT NULLS) OVER() AS median,\nPERCENTILE_DISC(x, 1 RESPECT NULLS) OVER() AS max FROM UNNEST(['c', NULL, 'b', 'a']) AS x;\n\n/*------+------+--------+-----*\n| x    | min  | median | max |\n+------+------+--------+-----+\n| c    | NULL | a      | c   |\n| NULL | NULL | a      | c   |\n| b    | NULL | a      | c   |\n| a    | NULL | a      | c   |\n*------+------+--------+-----*/"
            }
        }
    },
    {
        "category": "net-functions",
        "description": "GoogleSQL for BigQuery supports the following Net functions.",
        "source": "net_functions.txt",
        "functions": {
            "NET.HOST": {
                "name": "NET.HOST",
                "summary": "Gets the hostname from a URL.",
                "description": "NET.HOST(url)\n\n**Description**\n\nTakes a URL as a ` STRING ` value and returns the host. For best results, URL values should comply with the format as defined by [ RFC 3986\n](https://tools.ietf.org/html/rfc3986#appendix-A) . If the URL value does not comply with RFC 3986 formatting, this function makes a best effort to parse the input and return a relevant result. If the function cannot parse the input, it returns ` NULL ` .\n\n**Note:** The function does not perform any normalization.\n\n**Return Data Type**\n\n` STRING `\n\n**Example**\n\n\nSELECT FORMAT(\"%T\", input) AS input,\ndescription,\nFORMAT(\"%T\", NET.HOST(input)) AS host,\nFORMAT(\"%T\", NET.PUBLIC_SUFFIX(input)) AS suffix,\nFORMAT(\"%T\", NET.REG_DOMAIN(input)) AS domain FROM ( SELECT \"\" AS input, \"invalid input\" AS description UNION ALL SELECT \"http://abc.xyz\", \"standard URL\"\nUNION ALL SELECT \"//user:password@a.b:80/path?query\",\n\"standard URL with relative scheme, port, path and query, but no public suffix\"\nUNION ALL SELECT \"https://[::1]:80\", \"standard URL with IPv6 host\"\nUNION ALL SELECT \"http://\u4f8b\u5b50.\u5377\u7b52\u7eb8.\u4e2d\u56fd\", \"standard URL with internationalized domain name\"\nUNION ALL SELECT \"    www.Example.Co.UK    \",\n\"non-standard URL with spaces, upper case letters, and without scheme\"\nUNION ALL SELECT \"mailto:?to=&subject=&body=\", \"URI rather than URL--unsupported\"\n);\n\ninput  |  description  |  host  |  suffix  |  domain\n---|---|---|---|---\n\"\"  |  invalid input  |  NULL  |  NULL  |  NULL\n\"http://abc.xyz\"  |  standard URL  |  \"abc.xyz\"  |  \"xyz\"  |  \"abc.xyz\"\n\"//user:password@a.b:80/path?query\"  |  standard URL with relative scheme,\nport, path and query, but no public suffix  |  \"a.b\"  |  NULL  |  NULL\n\"https://[::1]:80\"  |  standard URL with IPv6 host  |  \"[::1]\"  |  NULL  |\nNULL\n\"http://\u4f8b\u5b50.\u5377\u7b52\u7eb8.\u4e2d\u56fd\"  |  standard URL with internationalized domain name  |\n\"\u4f8b\u5b50.\u5377\u7b52\u7eb8.\u4e2d\u56fd\"  |  \"\u4e2d\u56fd\"  |  \"\u5377\u7b52\u7eb8.\u4e2d\u56fd\"\n\" www.Example.Co.UK \"  |  non-standard URL with spaces, upper case letters,\nand without scheme  |  \"www.Example.Co.UK\"  |  \"Co.UK\"  |  \"Example.Co.UK\"\n\"mailto:?to=&subject=&body=\"  |  URI rather than URL--unsupported  |  \"mailto\"\n|  NULL  |  NULL"
            },
            "NET.IP_FROM_STRING": {
                "name": "NET.IP_FROM_STRING",
                "summary": "Converts an IPv4 or IPv6 address from a ` STRING `\nvalue to a ` BYTES ` value in network byte order.",
                "description": "NET.IP_FROM_STRING(addr_str)\n\n**Description**\n\nConverts an IPv4 or IPv6 address from text (STRING) format to binary (BYTES) format in network byte order.\n\nThis function supports the following formats for ` addr_str ` :\n\n* IPv4: Dotted-quad format. For example, ` 10.1.2.3 ` .\n* IPv6: Colon-separated format. For example, ` 1234:5678:90ab:cdef:1234:5678:90ab:cdef ` . For more examples, see the [ IP Version 6 Addressing Architecture ](http://www.ietf.org/rfc/rfc2373.txt) .\n\nThis function does not support [ CIDR notation\n](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing) , such as `\n10.1.2.3/32 ` .\n\nIf this function receives a ` NULL ` input, it returns ` NULL ` . If the input is considered invalid, an ` OUT_OF_RANGE ` error occurs.\n\n**Return Data Type**\n\nBYTES\n\n**Example**\n\n\nSELECT addr_str, FORMAT(\"%T\", NET.IP_FROM_STRING(addr_str)) AS ip_from_string FROM UNNEST([\n'48.49.50.51',\n'::1',\n'3031:3233:3435:3637:3839:4041:4243:4445',\n'::ffff:192.0.2.128'\n]) AS addr_str;\n\n/*---------------------------------------------------------------------------------------------------------------*\n| addr_str                                | ip_from_string                                                      |\n+---------------------------------------------------------------------------------------------------------------+\n| 48.49.50.51                             | b\"0123\"                                                             |\n| ::1                                     | b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\" |\n| 3031:3233:3435:3637:3839:4041:4243:4445 | b\"0123456789@ABCDE\"                                                 |\n| ::ffff:192.0.2.128                      | b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xff\\xc0\\x00\\x02\\x80\" |\n*---------------------------------------------------------------------------------------------------------------*/"
            },
            "NET.IP_NET_MASK": {
                "name": "NET.IP_NET_MASK",
                "summary": "Gets a network mask.",
                "description": "NET.IP_NET_MASK(num_output_bytes, prefix_length)\n\n**Description**\n\nReturns a network mask: a byte sequence with length equal to `\nnum_output_bytes ` , where the first ` prefix_length ` bits are set to 1 and the other bits are set to 0\\. ` num_output_bytes ` and ` prefix_length ` are INT64. This function throws an error if ` num_output_bytes ` is not 4 (for IPv4) or 16 (for IPv6). It also throws an error if ` prefix_length ` is negative or greater than ` 8 * num_output_bytes ` .\n\n**Return Data Type**\n\nBYTES\n\n**Example**\n\n\nSELECT x, y, FORMAT(\"%T\", NET.IP_NET_MASK(x, y)) AS ip_net_mask FROM UNNEST([\nSTRUCT(4 as x, 0 as y),\n(4, 20),\n(4, 32),\n(16, 0),\n(16, 1),\n(16, 128)\n]);\n\n/*--------------------------------------------------------------------------------*\n| x  | y   | ip_net_mask                                                         |\n+--------------------------------------------------------------------------------+\n| 4  | 0   | b\"\\x00\\x00\\x00\\x00\"                                                 |\n| 4  | 20  | b\"\\xff\\xff\\xf0\\x00\"                                                 |\n| 4  | 32  | b\"\\xff\\xff\\xff\\xff\"                                                 |\n| 16 | 0   | b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\" |\n| 16 | 1   | b\"\\x80\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\" |\n| 16 | 128 | b\"\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\" |\n*--------------------------------------------------------------------------------*/"
            },
            "NET.IP_TO_STRING": {
                "name": "NET.IP_TO_STRING",
                "summary": "Converts an IPv4 or IPv6 address from a ` BYTES `\nvalue in network byte order to a ` STRING ` value.",
                "description": "NET.IP_TO_STRING(addr_bin)\n\n**Description** Converts an IPv4 or IPv6 address from binary (BYTES) format in network byte order to text (STRING) format.\n\nIf the input is 4 bytes, this function returns an IPv4 address as a STRING. If the input is 16 bytes, it returns an IPv6 address as a STRING.\n\nIf this function receives a ` NULL ` input, it returns ` NULL ` . If the input has a length different from 4 or 16, an ` OUT_OF_RANGE ` error occurs.\n\n**Return Data Type**\n\nSTRING\n\n**Example**\n\n\nSELECT FORMAT(\"%T\", x) AS addr_bin, NET.IP_TO_STRING(x) AS ip_to_string FROM UNNEST([\nb\"0123\",\nb\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\",\nb\"0123456789@ABCDE\",\nb\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xff\\xc0\\x00\\x02\\x80\"\n]) AS x;\n\n/*---------------------------------------------------------------------------------------------------------------*\n| addr_bin                                                            | ip_to_string                            |\n+---------------------------------------------------------------------------------------------------------------+\n| b\"0123\"                                                             | 48.49.50.51                             |\n| b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\" | ::1                                     |\n| b\"0123456789@ABCDE\"                                                 | 3031:3233:3435:3637:3839:4041:4243:4445 |\n| b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xff\\xc0\\x00\\x02\\x80\" | ::ffff:192.0.2.128                      |\n*---------------------------------------------------------------------------------------------------------------*/"
            },
            "NET.IP_TRUNC": {
                "name": "NET.IP_TRUNC",
                "summary": "Converts a ` BYTES ` IPv4 or IPv6 address in network byte order to a ` BYTES ` subnet address.",
                "description": "NET.IP_TRUNC(addr_bin, prefix_length)\n\n**Description** Takes ` addr_bin ` , an IPv4 or IPv6 address in binary (BYTES) format in network byte order, and returns a subnet address in the same format.\nThe result has the same length as ` addr_bin ` , where the first `\nprefix_length ` bits are equal to those in ` addr_bin ` and the remaining bits are 0.\n\nThis function throws an error if ` LENGTH(addr_bin) ` is not 4 or 16, or if `\nprefix_len ` is negative or greater than ` LENGTH(addr_bin) * 8 ` .\n\n**Return Data Type**\n\nBYTES\n\n**Example**\n\n\nSELECT FORMAT(\"%T\", x) as addr_bin, prefix_length,\nFORMAT(\"%T\", NET.IP_TRUNC(x, prefix_length)) AS ip_trunc FROM UNNEST([\nSTRUCT(b\"\\xAA\\xBB\\xCC\\xDD\" as x, 0 as prefix_length),\n(b\"\\xAA\\xBB\\xCC\\xDD\", 11), (b\"\\xAA\\xBB\\xCC\\xDD\", 12),\n(b\"\\xAA\\xBB\\xCC\\xDD\", 24), (b\"\\xAA\\xBB\\xCC\\xDD\", 32),\n(b'0123456789@ABCDE', 80)\n]);\n\n/*-----------------------------------------------------------------------------*\n| addr_bin            | prefix_length | ip_trunc                              |\n+-----------------------------------------------------------------------------+\n| b\"\\xaa\\xbb\\xcc\\xdd\" | 0             | b\"\\x00\\x00\\x00\\x00\"                   |\n| b\"\\xaa\\xbb\\xcc\\xdd\" | 11            | b\"\\xaa\\xa0\\x00\\x00\"                   |\n| b\"\\xaa\\xbb\\xcc\\xdd\" | 12            | b\"\\xaa\\xb0\\x00\\x00\"                   |\n| b\"\\xaa\\xbb\\xcc\\xdd\" | 24            | b\"\\xaa\\xbb\\xcc\\x00\"                   |\n| b\"\\xaa\\xbb\\xcc\\xdd\" | 32            | b\"\\xaa\\xbb\\xcc\\xdd\"                   |\n| b\"0123456789@ABCDE\" | 80            | b\"0123456789\\x00\\x00\\x00\\x00\\x00\\x00\" |\n*-----------------------------------------------------------------------------*/"
            },
            "NET.IPV4_FROM_INT64": {
                "name": "NET.IPV4_FROM_INT64",
                "summary": "Converts an IPv4 address from an ` INT64 ` value to a ` BYTES ` value in network byte order.",
                "description": "NET.IPV4_FROM_INT64(integer_value)\n\n**Description**\n\nConverts an IPv4 address from integer format to binary (BYTES) format in network byte order. In the integer input, the least significant bit of the IP address is stored in the least significant bit of the integer, regardless of host or client architecture. For example, ` 1 ` means ` 0.0.0.1 ` , and `\n0x1FF ` means ` 0.0.1.255 ` .\n\nThis function checks that either all the most significant 32 bits are 0, or all the most significant 33 bits are 1 (sign-extended from a 32-bit integer).\nIn other words, the input should be in the range ` [-0x80000000, 0xFFFFFFFF] `\n; otherwise, this function throws an error.\n\nThis function does not support IPv6.\n\n**Return Data Type**\n\nBYTES\n\n**Example**\n\n\nSELECT x, x_hex, FORMAT(\"%T\", NET.IPV4_FROM_INT64(x)) AS ipv4_from_int64 FROM ( SELECT CAST(x_hex AS INT64) x, x_hex FROM UNNEST([\"0x0\", \"0xABCDEF\", \"0xFFFFFFFF\", \"-0x1\", \"-0x2\"]) AS x_hex );\n\n/*-----------------------------------------------*\n| x          | x_hex      | ipv4_from_int64     |\n+-----------------------------------------------+\n| 0          | 0x0        | b\"\\x00\\x00\\x00\\x00\" |\n| 11259375   | 0xABCDEF   | b\"\\x00\\xab\\xcd\\xef\" |\n| 4294967295 | 0xFFFFFFFF | b\"\\xff\\xff\\xff\\xff\" |\n| -1         | -0x1       | b\"\\xff\\xff\\xff\\xff\" |\n| -2         | -0x2       | b\"\\xff\\xff\\xff\\xfe\" |\n*-----------------------------------------------*/"
            },
            "NET.IPV4_TO_INT64": {
                "name": "NET.IPV4_TO_INT64",
                "summary": "Converts an IPv4 address from a ` BYTES ` value in network byte order to an ` INT64 ` value.",
                "description": "NET.IPV4_TO_INT64(addr_bin)\n\n**Description**\n\nConverts an IPv4 address from binary (BYTES) format in network byte order to integer format. In the integer output, the least significant bit of the IP address is stored in the least significant bit of the integer, regardless of host or client architecture. For example, ` 1 ` means ` 0.0.0.1 ` , and `\n0x1FF ` means ` 0.0.1.255 ` . The output is in the range ` [0, 0xFFFFFFFF] ` .\n\nIf the input length is not 4, this function throws an error.\n\nThis function does not support IPv6.\n\n**Return Data Type**\n\nINT64\n\n**Example**\n\n\nSELECT FORMAT(\"%T\", x) AS addr_bin,\nFORMAT(\"0x%X\", NET.IPV4_TO_INT64(x)) AS ipv4_to_int64 FROM UNNEST([b\"\\x00\\x00\\x00\\x00\", b\"\\x00\\xab\\xcd\\xef\", b\"\\xff\\xff\\xff\\xff\"]) AS x;\n\n/*-------------------------------------*\n| addr_bin            | ipv4_to_int64 |\n+-------------------------------------+\n| b\"\\x00\\x00\\x00\\x00\" | 0x0           |\n| b\"\\x00\\xab\\xcd\\xef\" | 0xABCDEF      |\n| b\"\\xff\\xff\\xff\\xff\" | 0xFFFFFFFF    |\n*-------------------------------------*/"
            },
            "NET.PUBLIC_SUFFIX": {
                "name": "NET.PUBLIC_SUFFIX",
                "summary": "Gets the public suffix from a URL.",
                "description": "NET.PUBLIC_SUFFIX(url)\n\n**Description**\n\nTakes a URL as a ` STRING ` value and returns the public suffix (such as ` com\n` , ` org ` , or ` net ` ). A public suffix is an ICANN domain registered at [\npublicsuffix.org ](https://publicsuffix.org/list/) . For best results, URL values should comply with the format as defined by [ RFC 3986\n](https://tools.ietf.org/html/rfc3986#appendix-A) . If the URL value does not comply with RFC 3986 formatting, this function makes a best effort to parse the input and return a relevant result.\n\nThis function returns ` NULL ` if any of the following is true:\n\n* It cannot parse the host from the input;\n* The parsed host contains adjacent dots in the middle (not leading or trailing);\n* The parsed host does not contain any public suffix.\n\nBefore looking up the public suffix, this function temporarily normalizes the host by converting uppercase English letters to lowercase and encoding all non-ASCII characters with [ Punycode ](https://en.wikipedia.org/wiki/Punycode) . The function then returns the public suffix as part of the original host instead of the normalized host.\n\n**Note:** The function does not perform [ Unicode normalization\n](https://en.wikipedia.org/wiki/Unicode_equivalence) .  **Note:** The public suffix data at [ publicsuffix.org ](https://publicsuffix.org/list/) also contains private domains. This function ignores the private domains.\n**Note:** The public suffix data may change over time. Consequently, input that produces a ` NULL ` result now may produce a non- ` NULL ` value in the future.\n\n**Return Data Type**\n\n` STRING `\n\n**Example**\n\n\nSELECT FORMAT(\"%T\", input) AS input,\ndescription,\nFORMAT(\"%T\", NET.HOST(input)) AS host,\nFORMAT(\"%T\", NET.PUBLIC_SUFFIX(input)) AS suffix,\nFORMAT(\"%T\", NET.REG_DOMAIN(input)) AS domain FROM ( SELECT \"\" AS input, \"invalid input\" AS description UNION ALL SELECT \"http://abc.xyz\", \"standard URL\"\nUNION ALL SELECT \"//user:password@a.b:80/path?query\",\n\"standard URL with relative scheme, port, path and query, but no public suffix\"\nUNION ALL SELECT \"https://[::1]:80\", \"standard URL with IPv6 host\"\nUNION ALL SELECT \"http://\u4f8b\u5b50.\u5377\u7b52\u7eb8.\u4e2d\u56fd\", \"standard URL with internationalized domain name\"\nUNION ALL SELECT \"    www.Example.Co.UK    \",\n\"non-standard URL with spaces, upper case letters, and without scheme\"\nUNION ALL SELECT \"mailto:?to=&subject=&body=\", \"URI rather than URL--unsupported\"\n);\n\ninput  |  description  |  host  |  suffix  |  domain\n---|---|---|---|---\n\"\"  |  invalid input  |  NULL  |  NULL  |  NULL\n\"http://abc.xyz\"  |  standard URL  |  \"abc.xyz\"  |  \"xyz\"  |  \"abc.xyz\"\n\"//user:password@a.b:80/path?query\"  |  standard URL with relative scheme,\nport, path and query, but no public suffix  |  \"a.b\"  |  NULL  |  NULL\n\"https://[::1]:80\"  |  standard URL with IPv6 host  |  \"[::1]\"  |  NULL  |\nNULL\n\"http://\u4f8b\u5b50.\u5377\u7b52\u7eb8.\u4e2d\u56fd\"  |  standard URL with internationalized domain name  |\n\"\u4f8b\u5b50.\u5377\u7b52\u7eb8.\u4e2d\u56fd\"  |  \"\u4e2d\u56fd\"  |  \"\u5377\u7b52\u7eb8.\u4e2d\u56fd\"\n\" www.Example.Co.UK \"  |  non-standard URL with spaces, upper case letters,\nand without scheme  |  \"www.Example.Co.UK\"  |  \"Co.UK\"  |  \"Example.Co.UK\n\"mailto:?to=&subject=&body=\"  |  URI rather than URL--unsupported  |  \"mailto\"\n|  NULL  |  NULL"
            },
            "NET.REG_DOMAIN": {
                "name": "NET.REG_DOMAIN",
                "summary": "Gets the registered or registrable domain from a URL.",
                "description": "NET.REG_DOMAIN(url)\n\n**Description**\n\nTakes a URL as a string and returns the registered or registrable domain (the public suffix  plus one preceding label), as a string. For best results, URL values should comply with the format as defined by [ RFC 3986\n](https://tools.ietf.org/html/rfc3986#appendix-A) . If the URL value does not comply with RFC 3986 formatting, this function makes a best effort to parse the input and return a relevant result.\n\nThis function returns ` NULL ` if any of the following is true:\n\n* It cannot parse the host from the input;\n* The parsed host contains adjacent dots in the middle (not leading or trailing);\n* The parsed host does not contain any public suffix;\n* The parsed host contains only a public suffix without any preceding label.\n\nBefore looking up the public suffix, this function temporarily normalizes the host by converting uppercase English letters to lowercase and encoding all non-ASCII characters with [ Punycode ](https://en.wikipedia.org/wiki/Punycode) . The function then returns the registered or registerable domain as part of the original host instead of the normalized host.\n\n**Note:** The function does not perform [ Unicode normalization\n](https://en.wikipedia.org/wiki/Unicode_equivalence) .  **Note:** The public suffix data at [ publicsuffix.org ](https://publicsuffix.org/list/) also contains private domains. This function does not treat a private domain as a public suffix. For example, if ` us.com ` is a private domain in the public suffix data, ` NET.REG_DOMAIN(\"foo.us.com\") ` returns ` us.com ` (the public suffix ` com ` plus the preceding label ` us ` ) rather than ` foo.us.com `\n(the private domain ` us.com ` plus the preceding label ` foo ` ).  **Note:**\nThe public suffix data may change over time. Consequently, input that produces a ` NULL ` result now may produce a non- ` NULL ` value in the future.\n\n**Return Data Type**\n\n` STRING `\n\n**Example**\n\n\nSELECT FORMAT(\"%T\", input) AS input,\ndescription,\nFORMAT(\"%T\", NET.HOST(input)) AS host,\nFORMAT(\"%T\", NET.PUBLIC_SUFFIX(input)) AS suffix,\nFORMAT(\"%T\", NET.REG_DOMAIN(input)) AS domain FROM ( SELECT \"\" AS input, \"invalid input\" AS description UNION ALL SELECT \"http://abc.xyz\", \"standard URL\"\nUNION ALL SELECT \"//user:password@a.b:80/path?query\",\n\"standard URL with relative scheme, port, path and query, but no public suffix\"\nUNION ALL SELECT \"https://[::1]:80\", \"standard URL with IPv6 host\"\nUNION ALL SELECT \"http://\u4f8b\u5b50.\u5377\u7b52\u7eb8.\u4e2d\u56fd\", \"standard URL with internationalized domain name\"\nUNION ALL SELECT \"    www.Example.Co.UK    \",\n\"non-standard URL with spaces, upper case letters, and without scheme\"\nUNION ALL SELECT \"mailto:?to=&subject=&body=\", \"URI rather than URL--unsupported\"\n);\n\ninput  |  description  |  host  |  suffix  |  domain\n---|---|---|---|---\n\"\"  |  invalid input  |  NULL  |  NULL  |  NULL\n\"http://abc.xyz\"  |  standard URL  |  \"abc.xyz\"  |  \"xyz\"  |  \"abc.xyz\"\n\"//user:password@a.b:80/path?query\"  |  standard URL with relative scheme,\nport, path and query, but no public suffix  |  \"a.b\"  |  NULL  |  NULL\n\"https://[::1]:80\"  |  standard URL with IPv6 host  |  \"[::1]\"  |  NULL  |\nNULL\n\"http://\u4f8b\u5b50.\u5377\u7b52\u7eb8.\u4e2d\u56fd\"  |  standard URL with internationalized domain name  |\n\"\u4f8b\u5b50.\u5377\u7b52\u7eb8.\u4e2d\u56fd\"  |  \"\u4e2d\u56fd\"  |  \"\u5377\u7b52\u7eb8.\u4e2d\u56fd\"\n\" www.Example.Co.UK \"  |  non-standard URL with spaces, upper case letters,\nand without scheme  |  \"www.Example.Co.UK\"  |  \"Co.UK\"  |  \"Example.Co.UK\"\n\"mailto:?to=&subject=&body=\"  |  URI rather than URL--unsupported  |  \"mailto\"\n|  NULL  |  NULL"
            },
            "NET.SAFE_IP_FROM_STRING": {
                "name": "NET.SAFE_IP_FROM_STRING",
                "summary": "Similar to the ` NET.IP_FROM_STRING ` , but returns ` NULL ` instead of producing an error if the input is invalid.",
                "description": "NET.SAFE_IP_FROM_STRING(addr_str)\n\n**Description**\n\nSimilar to  ` NET.IP_FROM_STRING ` , but returns ` NULL ` instead of throwing an error if the input is invalid.\n\n**Return Data Type**\n\nBYTES\n\n**Example**\n\n\nSELECT addr_str,\nFORMAT(\"%T\", NET.SAFE_IP_FROM_STRING(addr_str)) AS safe_ip_from_string FROM UNNEST([\n'48.49.50.51',\n'::1',\n'3031:3233:3435:3637:3839:4041:4243:4445',\n'::ffff:192.0.2.128',\n'48.49.50.51/32',\n'48.49.50',\n'::wxyz'\n]) AS addr_str;\n\n/*---------------------------------------------------------------------------------------------------------------*\n| addr_str                                | safe_ip_from_string                                                 |\n+---------------------------------------------------------------------------------------------------------------+\n| 48.49.50.51                             | b\"0123\"                                                             |\n| ::1                                     | b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\" |\n| 3031:3233:3435:3637:3839:4041:4243:4445 | b\"0123456789@ABCDE\"                                                 |\n| ::ffff:192.0.2.128                      | b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xff\\xc0\\x00\\x02\\x80\" |\n| 48.49.50.51/32                          | NULL                                                                |\n| 48.49.50                                | NULL                                                                |\n| ::wxyz                                  | NULL                                                                |\n*---------------------------------------------------------------------------------------------------------------*/"
            }
        }
    },
    {
        "category": "numbering-functions",
        "description": "GoogleSQL for BigQuery supports numbering functions. Numbering functions are a subset of window functions. To create a window function call and learn about the syntax for window functions, see [ Window function calls\n](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\nNumbering functions assign integer values to each row based on their position within the specified window. The ` OVER ` clause syntax varies across numbering functions.",
        "source": "numbering_functions.txt",
        "functions": {
            "CUME_DIST": {
                "name": "CUME_DIST",
                "summary": "Gets the cumulative distribution (relative position (0,1]) of each row within a window.",
                "description": "CUME_DIST() OVER over_clause\n\nover_clause:\n{ named_window | ( [ window_specification ] ) }\n\nwindow_specification:\n[ named_window ]\n[ PARTITION BY partition_expression [, ...] ]\nORDER BY expression [ { ASC | DESC }  ] [, ...]\n\n\n**Description**\n\nReturn the relative rank of a row defined as NP/NR. NP is defined to be the number of rows that either precede or are peers with the current row. NR is the number of rows in the partition.\n\nTo learn more about the ` OVER ` clause and how to use it, see [ Window function calls ](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n**Return Type**\n\n` FLOAT64 `\n\n**Example**\n\n\nWITH finishers AS (SELECT 'Sophia Liu' as name,\nTIMESTAMP '2016-10-18 2:51:45' as finish_time,\n'F30-34' as division UNION ALL SELECT 'Lisa Stelzner', TIMESTAMP '2016-10-18 2:54:11', 'F35-39'\nUNION ALL SELECT 'Nikki Leith', TIMESTAMP '2016-10-18 2:59:01', 'F30-34'\nUNION ALL SELECT 'Lauren Matthews', TIMESTAMP '2016-10-18 3:01:17', 'F35-39'\nUNION ALL SELECT 'Desiree Berry', TIMESTAMP '2016-10-18 3:05:42', 'F35-39'\nUNION ALL SELECT 'Suzy Slane', TIMESTAMP '2016-10-18 3:06:24', 'F35-39'\nUNION ALL SELECT 'Jen Edwards', TIMESTAMP '2016-10-18 3:06:36', 'F30-34'\nUNION ALL SELECT 'Meghan Lederer', TIMESTAMP '2016-10-18 2:59:01', 'F30-34') SELECT name,\nfinish_time,\ndivision,\nCUME_DIST() OVER (PARTITION BY division ORDER BY finish_time ASC) AS finish_rank FROM finishers;\n\n/*-----------------+------------------------+----------+-------------*\n| name            | finish_time            | division | finish_rank |\n+-----------------+------------------------+----------+-------------+\n| Sophia Liu      | 2016-10-18 09:51:45+00 | F30-34   | 0.25        |\n| Meghan Lederer  | 2016-10-18 09:59:01+00 | F30-34   | 0.75        |\n| Nikki Leith     | 2016-10-18 09:59:01+00 | F30-34   | 0.75        |\n| Jen Edwards     | 2016-10-18 10:06:36+00 | F30-34   | 1           |\n| Lisa Stelzner   | 2016-10-18 09:54:11+00 | F35-39   | 0.25        |\n| Lauren Matthews | 2016-10-18 10:01:17+00 | F35-39   | 0.5         |\n| Desiree Berry   | 2016-10-18 10:05:42+00 | F35-39   | 0.75        |\n| Suzy Slane      | 2016-10-18 10:06:24+00 | F35-39   | 1           |\n*-----------------+------------------------+----------+-------------*/"
            },
            "DENSE_RANK": {
                "name": "DENSE_RANK",
                "summary": "Gets the dense rank (1-based, no gaps) of each row within a window.",
                "description": "DENSE_RANK() OVER over_clause\n\nover_clause:\n{ named_window | ( [ window_specification ] ) }\n\nwindow_specification:\n[ named_window ]\n[ PARTITION BY partition_expression [, ...] ]\nORDER BY expression [ { ASC | DESC }  ] [, ...]\n\n\n**Description**\n\nReturns the ordinal (1-based) rank of each row within the window partition.\nAll peer rows receive the same rank value, and the subsequent rank value is incremented by one.\n\nTo learn more about the ` OVER ` clause and how to use it, see [ Window function calls ](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n**Return Type**\n\n` INT64 `\n\n**Examples**\n\n\nWITH Numbers AS (SELECT 1 as x UNION ALL SELECT 2 UNION ALL SELECT 2 UNION ALL SELECT 5 UNION ALL SELECT 8 UNION ALL SELECT 10 UNION ALL SELECT 10 ) SELECT x,\nDENSE_RANK() OVER (ORDER BY x ASC) AS dense_rank FROM Numbers\n\n/*-------------------------*\n| x          | dense_rank |\n+-------------------------+\n| 1          | 1          |\n| 2          | 2          |\n| 2          | 2          |\n| 5          | 3          |\n| 8          | 4          |\n| 10         | 5          |\n| 10         | 5          |\n*-------------------------*/\n\n\nWITH finishers AS (SELECT 'Sophia Liu' as name,\nTIMESTAMP '2016-10-18 2:51:45' as finish_time,\n'F30-34' as division UNION ALL SELECT 'Lisa Stelzner', TIMESTAMP '2016-10-18 2:54:11', 'F35-39'\nUNION ALL SELECT 'Nikki Leith', TIMESTAMP '2016-10-18 2:59:01', 'F30-34'\nUNION ALL SELECT 'Lauren Matthews', TIMESTAMP '2016-10-18 3:01:17', 'F35-39'\nUNION ALL SELECT 'Desiree Berry', TIMESTAMP '2016-10-18 3:05:42', 'F35-39'\nUNION ALL SELECT 'Suzy Slane', TIMESTAMP '2016-10-18 3:06:24', 'F35-39'\nUNION ALL SELECT 'Jen Edwards', TIMESTAMP '2016-10-18 3:06:36', 'F30-34'\nUNION ALL SELECT 'Meghan Lederer', TIMESTAMP '2016-10-18 2:59:01', 'F30-34') SELECT name,\nfinish_time,\ndivision,\nDENSE_RANK() OVER (PARTITION BY division ORDER BY finish_time ASC) AS finish_rank FROM finishers;\n\n/*-----------------+------------------------+----------+-------------*\n| name            | finish_time            | division | finish_rank |\n+-----------------+------------------------+----------+-------------+\n| Sophia Liu      | 2016-10-18 09:51:45+00 | F30-34   | 1           |\n| Meghan Lederer  | 2016-10-18 09:59:01+00 | F30-34   | 2           |\n| Nikki Leith     | 2016-10-18 09:59:01+00 | F30-34   | 2           |\n| Jen Edwards     | 2016-10-18 10:06:36+00 | F30-34   | 3           |\n| Lisa Stelzner   | 2016-10-18 09:54:11+00 | F35-39   | 1           |\n| Lauren Matthews | 2016-10-18 10:01:17+00 | F35-39   | 2           |\n| Desiree Berry   | 2016-10-18 10:05:42+00 | F35-39   | 3           |\n| Suzy Slane      | 2016-10-18 10:06:24+00 | F35-39   | 4           |\n*-----------------+------------------------+----------+-------------*/"
            },
            "NTILE": {
                "name": "NTILE",
                "summary": "Gets the quantile bucket number (1-based) of each row within a window.",
                "description": "NTILE(constant_integer_expression) OVER over_clause\n\nover_clause:\n{ named_window | ( [ window_specification ] ) }\n\nwindow_specification:\n[ named_window ]\n[ PARTITION BY partition_expression [, ...] ]\nORDER BY expression [ { ASC | DESC }  ] [, ...]\n\n\n**Description**\n\nThis function divides the rows into ` constant_integer_expression ` buckets based on row ordering and returns the 1-based bucket number that is assigned to each row. The number of rows in the buckets can differ by at most 1. The remainder values (the remainder of number of rows divided by buckets) are distributed one for each bucket, starting with bucket 1. If `\nconstant_integer_expression ` evaluates to NULL, 0 or negative, an error is provided.\n\nTo learn more about the ` OVER ` clause and how to use it, see [ Window function calls ](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n**Return Type**\n\n` INT64 `\n\n**Example**\n\n\nWITH finishers AS (SELECT 'Sophia Liu' as name,\nTIMESTAMP '2016-10-18 2:51:45' as finish_time,\n'F30-34' as division UNION ALL SELECT 'Lisa Stelzner', TIMESTAMP '2016-10-18 2:54:11', 'F35-39'\nUNION ALL SELECT 'Nikki Leith', TIMESTAMP '2016-10-18 2:59:01', 'F30-34'\nUNION ALL SELECT 'Lauren Matthews', TIMESTAMP '2016-10-18 3:01:17', 'F35-39'\nUNION ALL SELECT 'Desiree Berry', TIMESTAMP '2016-10-18 3:05:42', 'F35-39'\nUNION ALL SELECT 'Suzy Slane', TIMESTAMP '2016-10-18 3:06:24', 'F35-39'\nUNION ALL SELECT 'Jen Edwards', TIMESTAMP '2016-10-18 3:06:36', 'F30-34'\nUNION ALL SELECT 'Meghan Lederer', TIMESTAMP '2016-10-18 2:59:01', 'F30-34') SELECT name,\nfinish_time,\ndivision,\nNTILE(3) OVER (PARTITION BY division ORDER BY finish_time ASC) AS finish_rank FROM finishers;\n\n/*-----------------+------------------------+----------+-------------*\n| name            | finish_time            | division | finish_rank |\n+-----------------+------------------------+----------+-------------+\n| Sophia Liu      | 2016-10-18 09:51:45+00 | F30-34   | 1           |\n| Meghan Lederer  | 2016-10-18 09:59:01+00 | F30-34   | 1           |\n| Nikki Leith     | 2016-10-18 09:59:01+00 | F30-34   | 2           |\n| Jen Edwards     | 2016-10-18 10:06:36+00 | F30-34   | 3           |\n| Lisa Stelzner   | 2016-10-18 09:54:11+00 | F35-39   | 1           |\n| Lauren Matthews | 2016-10-18 10:01:17+00 | F35-39   | 1           |\n| Desiree Berry   | 2016-10-18 10:05:42+00 | F35-39   | 2           |\n| Suzy Slane      | 2016-10-18 10:06:24+00 | F35-39   | 3           |\n*-----------------+------------------------+----------+-------------*/"
            },
            "PERCENT_RANK": {
                "name": "PERCENT_RANK",
                "summary": "Gets the percentile rank (from 0 to 1) of each row within a window.",
                "description": "PERCENT_RANK() OVER over_clause\n\nover_clause:\n{ named_window | ( [ window_specification ] ) }\n\nwindow_specification:\n[ named_window ]\n[ PARTITION BY partition_expression [, ...] ]\nORDER BY expression [ { ASC | DESC }  ] [, ...]\n\n\n**Description**\n\nReturn the percentile rank of a row defined as (RK-1)/(NR-1), where RK is the\n` RANK ` of the row and NR is the number of rows in the partition. Returns 0 if NR=1.\n\nTo learn more about the ` OVER ` clause and how to use it, see [ Window function calls ](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n**Return Type**\n\n` FLOAT64 `\n\n**Example**\n\n\nWITH finishers AS (SELECT 'Sophia Liu' as name,\nTIMESTAMP '2016-10-18 2:51:45' as finish_time,\n'F30-34' as division UNION ALL SELECT 'Lisa Stelzner', TIMESTAMP '2016-10-18 2:54:11', 'F35-39'\nUNION ALL SELECT 'Nikki Leith', TIMESTAMP '2016-10-18 2:59:01', 'F30-34'\nUNION ALL SELECT 'Lauren Matthews', TIMESTAMP '2016-10-18 3:01:17', 'F35-39'\nUNION ALL SELECT 'Desiree Berry', TIMESTAMP '2016-10-18 3:05:42', 'F35-39'\nUNION ALL SELECT 'Suzy Slane', TIMESTAMP '2016-10-18 3:06:24', 'F35-39'\nUNION ALL SELECT 'Jen Edwards', TIMESTAMP '2016-10-18 3:06:36', 'F30-34'\nUNION ALL SELECT 'Meghan Lederer', TIMESTAMP '2016-10-18 2:59:01', 'F30-34') SELECT name,\nfinish_time,\ndivision,\nPERCENT_RANK() OVER (PARTITION BY division ORDER BY finish_time ASC) AS finish_rank FROM finishers;\n\n/*-----------------+------------------------+----------+---------------------*\n| name            | finish_time            | division | finish_rank         |\n+-----------------+------------------------+----------+---------------------+\n| Sophia Liu      | 2016-10-18 09:51:45+00 | F30-34   | 0                   |\n| Meghan Lederer  | 2016-10-18 09:59:01+00 | F30-34   | 0.33333333333333331 |\n| Nikki Leith     | 2016-10-18 09:59:01+00 | F30-34   | 0.33333333333333331 |\n| Jen Edwards     | 2016-10-18 10:06:36+00 | F30-34   | 1                   |\n| Lisa Stelzner   | 2016-10-18 09:54:11+00 | F35-39   | 0                   |\n| Lauren Matthews | 2016-10-18 10:01:17+00 | F35-39   | 0.33333333333333331 |\n| Desiree Berry   | 2016-10-18 10:05:42+00 | F35-39   | 0.66666666666666663 |\n| Suzy Slane      | 2016-10-18 10:06:24+00 | F35-39   | 1                   |\n*-----------------+------------------------+----------+---------------------*/"
            },
            "RANK": {
                "name": "RANK",
                "summary": "Gets the rank (1-based) of each row within a window.",
                "description": "RANK() OVER over_clause\n\nover_clause:\n{ named_window | ( [ window_specification ] ) }\n\nwindow_specification:\n[ named_window ]\n[ PARTITION BY partition_expression [, ...] ]\nORDER BY expression [ { ASC | DESC }  ] [, ...]\n\n\n**Description**\n\nReturns the ordinal (1-based) rank of each row within the ordered partition.\nAll peer rows receive the same rank value. The next row or set of peer rows receives a rank value which increments by the number of peers with the previous rank value, instead of ` DENSE_RANK ` , which always increments by 1.\n\nTo learn more about the ` OVER ` clause and how to use it, see [ Window function calls ](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n**Return Type**\n\n` INT64 `\n\n**Examples**\n\n\nWITH Numbers AS (SELECT 1 as x UNION ALL SELECT 2 UNION ALL SELECT 2 UNION ALL SELECT 5 UNION ALL SELECT 8 UNION ALL SELECT 10 UNION ALL SELECT 10 ) SELECT x,\nRANK() OVER (ORDER BY x ASC) AS rank FROM Numbers\n\n/*-------------------------*\n| x          | rank       |\n+-------------------------+\n| 1          | 1          |\n| 2          | 2          |\n| 2          | 2          |\n| 5          | 4          |\n| 8          | 5          |\n| 10         | 6          |\n| 10         | 6          |\n*-------------------------*/\n\n\nWITH finishers AS (SELECT 'Sophia Liu' as name,\nTIMESTAMP '2016-10-18 2:51:45' as finish_time,\n'F30-34' as division UNION ALL SELECT 'Lisa Stelzner', TIMESTAMP '2016-10-18 2:54:11', 'F35-39'\nUNION ALL SELECT 'Nikki Leith', TIMESTAMP '2016-10-18 2:59:01', 'F30-34'\nUNION ALL SELECT 'Lauren Matthews', TIMESTAMP '2016-10-18 3:01:17', 'F35-39'\nUNION ALL SELECT 'Desiree Berry', TIMESTAMP '2016-10-18 3:05:42', 'F35-39'\nUNION ALL SELECT 'Suzy Slane', TIMESTAMP '2016-10-18 3:06:24', 'F35-39'\nUNION ALL SELECT 'Jen Edwards', TIMESTAMP '2016-10-18 3:06:36', 'F30-34'\nUNION ALL SELECT 'Meghan Lederer', TIMESTAMP '2016-10-18 2:59:01', 'F30-34') SELECT name,\nfinish_time,\ndivision,\nRANK() OVER (PARTITION BY division ORDER BY finish_time ASC) AS finish_rank FROM finishers;\n\n/*-----------------+------------------------+----------+-------------*\n| name            | finish_time            | division | finish_rank |\n+-----------------+------------------------+----------+-------------+\n| Sophia Liu      | 2016-10-18 09:51:45+00 | F30-34   | 1           |\n| Meghan Lederer  | 2016-10-18 09:59:01+00 | F30-34   | 2           |\n| Nikki Leith     | 2016-10-18 09:59:01+00 | F30-34   | 2           |\n| Jen Edwards     | 2016-10-18 10:06:36+00 | F30-34   | 4           |\n| Lisa Stelzner   | 2016-10-18 09:54:11+00 | F35-39   | 1           |\n| Lauren Matthews | 2016-10-18 10:01:17+00 | F35-39   | 2           |\n| Desiree Berry   | 2016-10-18 10:05:42+00 | F35-39   | 3           |\n| Suzy Slane      | 2016-10-18 10:06:24+00 | F35-39   | 4           |\n*-----------------+------------------------+----------+-------------*/"
            },
            "ROW_NUMBER": {
                "name": "ROW_NUMBER",
                "summary": "Gets the sequential row number (1-based) of each row within a window.",
                "description": "ROW_NUMBER() OVER over_clause\n\nover_clause:\n{ named_window | ( [ window_specification ] ) }\n\nwindow_specification:\n[ named_window ]\n[ PARTITION BY partition_expression [, ...] ]\n[ ORDER BY expression [ { ASC | DESC }  ] [, ...] ]\n\n\n**Description**\n\nDoes not require the ` ORDER BY ` clause. Returns the sequential row ordinal (1-based) of each row for each ordered partition. If the ` ORDER BY ` clause is unspecified then the result is non-deterministic.\n\nTo learn more about the ` OVER ` clause and how to use it, see [ Window function calls ](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n**Return Type**\n\n` INT64 `\n\n**Examples**\n\n\nWITH Numbers AS (SELECT 1 as x UNION ALL SELECT 2 UNION ALL SELECT 2 UNION ALL SELECT 5 UNION ALL SELECT 8 UNION ALL SELECT 10 UNION ALL SELECT 10 ) SELECT x,\nROW_NUMBER() OVER (ORDER BY x) AS row_num FROM Numbers\n\n/*-------------------------*\n| x          | row_num    |\n+-------------------------+\n| 1          | 1          |\n| 2          | 2          |\n| 2          | 3          |\n| 5          | 4          |\n| 8          | 5          |\n| 10         | 6          |\n| 10         | 7          |\n*-------------------------*/\n\n\nWITH finishers AS (SELECT 'Sophia Liu' as name,\nTIMESTAMP '2016-10-18 2:51:45' as finish_time,\n'F30-34' as division UNION ALL SELECT 'Lisa Stelzner', TIMESTAMP '2016-10-18 2:54:11', 'F35-39'\nUNION ALL SELECT 'Nikki Leith', TIMESTAMP '2016-10-18 2:59:01', 'F30-34'\nUNION ALL SELECT 'Lauren Matthews', TIMESTAMP '2016-10-18 3:01:17', 'F35-39'\nUNION ALL SELECT 'Desiree Berry', TIMESTAMP '2016-10-18 3:05:42', 'F35-39'\nUNION ALL SELECT 'Suzy Slane', TIMESTAMP '2016-10-18 3:06:24', 'F35-39'\nUNION ALL SELECT 'Jen Edwards', TIMESTAMP '2016-10-18 3:06:36', 'F30-34'\nUNION ALL SELECT 'Meghan Lederer', TIMESTAMP '2016-10-18 2:59:01', 'F30-34') SELECT name,\nfinish_time,\ndivision,\nROW_NUMBER() OVER (PARTITION BY division ORDER BY finish_time ASC) AS finish_rank FROM finishers;\n\n/*-----------------+------------------------+----------+-------------*\n| name            | finish_time            | division | finish_rank |\n+-----------------+------------------------+----------+-------------+\n| Sophia Liu      | 2016-10-18 09:51:45+00 | F30-34   | 1           |\n| Meghan Lederer  | 2016-10-18 09:59:01+00 | F30-34   | 2           |\n| Nikki Leith     | 2016-10-18 09:59:01+00 | F30-34   | 3           |\n| Jen Edwards     | 2016-10-18 10:06:36+00 | F30-34   | 4           |\n| Lisa Stelzner   | 2016-10-18 09:54:11+00 | F35-39   | 1           |\n| Lauren Matthews | 2016-10-18 10:01:17+00 | F35-39   | 2           |\n| Desiree Berry   | 2016-10-18 10:05:42+00 | F35-39   | 3           |\n| Suzy Slane      | 2016-10-18 10:06:24+00 | F35-39   | 4           |\n*-----------------+------------------------+----------+-------------*/"
            }
        }
    },
    {
        "category": "range-functions",
        "description": "GoogleSQL for BigQuery supports the following range functions.",
        "source": "range-functions.txt",
        "functions": {
            "GENERATE_RANGE_ARRAY": {
                "name": "GENERATE_RANGE_ARRAY",
                "summary": "Splits a range into an array of subranges.",
                "description": "**Preview**\n\nThis product or feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of the [ Service Specific Terms\n](/terms/service-terms) . Pre-GA products and features are available \"as is\"\nand might have limited support. For more information, see the [ launch stage descriptions ](/products#product-launch-stages) .\n\n**Note:** To provide feedback or request support for this feature, send an email to [ bigquery-time-series-preview-support@google.com ](mailto:bigquery-\ntime-series-preview-support@google.com) .\n\n\nGENERATE_RANGE_ARRAY(range_to_split, step_interval)\n\n\nGENERATE_RANGE_ARRAY(range_to_split, step_interval, include_last_partial_range)\n\n**Description**\n\nSplits a range into an array of subranges.\n\n**Definitions**\n\n* ` range_to_split ` : The ` RANGE<T> ` value to split.\n* ` step_interval ` : The ` INTERVAL ` value, which determines the maximum size of each subrange in the resulting array. An [ interval single date and time part ](/bigquery/docs/reference/standard-sql/data-types#single_datetime_part_interval) is supported, but an interval range of date and time parts is not.\n\n* If ` range_to_split ` is ` RANGE<DATE> ` , these interval date parts are supported: ` YEAR ` to ` DAY ` .\n\n* If ` range_to_split ` is ` RANGE<DATETIME> ` , these interval date and time parts are supported: ` YEAR ` to ` SECOND ` .\n\n* If ` range_to_split ` is ` RANGE<TIMESTAMP> ` , these interval date and time parts are supported: ` DAY ` to ` SECOND ` .\n\n* ` include_last_partial_range ` : A ` BOOL ` value, which determines whether or not to include the last subrange if it's a partial subrange. If this argument is not specified, the default value is ` TRUE ` .\n\n* ` TRUE ` (default): The last subrange is included, even if it's smaller than ` step_interval ` .\n\n* ` FALSE ` : Exclude the last subrange if it's smaller than ` step_interval ` .\n\n**Details**\n\nReturns ` NULL ` if any input is ` NULL ` .\n\n**Return type**\n\n` ARRAY<RANGE<T>> `\n\n**Examples**\n\nIn the following example, a date range between ` 2020-01-01 ` and ` 2020-01-06\n` is split into an array of subranges that are one day long. There are no partial ranges.\n\n\nSELECT GENERATE_RANGE_ARRAY( RANGE(DATE '2020-01-01', DATE '2020-01-06'),\nINTERVAL 1 DAY) AS results;\n\n/*----------------------------+\n| results                    |\n+----------------------------+\n| [                          |\n|  [2020-01-01, 2020-01-02), |\n|  [2020-01-02, 2020-01-03), |\n|  [2020-01-03, 2020-01-04), |\n|  [2020-01-04, 2020-01-05), |\n|  [2020-01-05, 2020-01-06), |\n| ]                          |\n+----------------------------*/\n\nIn the following examples, a date range between ` 2020-01-01 ` and `\n2020-01-06 ` is split into an array of subranges that are two days long. The final subrange is smaller than two days:\n\n\nSELECT GENERATE_RANGE_ARRAY( RANGE(DATE '2020-01-01', DATE '2020-01-06'),\nINTERVAL 2 DAY) AS results;\n\n/*----------------------------+\n| results                    |\n+----------------------------+\n| [                          |\n|  [2020-01-01, 2020-01-03), |\n|  [2020-01-03, 2020-01-05), |\n|  [2020-01-05, 2020-01-06)  |\n| ]                          |\n+----------------------------*/\n\n\nSELECT GENERATE_RANGE_ARRAY( RANGE(DATE '2020-01-01', DATE '2020-01-06'),\nINTERVAL 2 DAY,\nTRUE) AS results;\n\n/*----------------------------+\n| results                    |\n+----------------------------+\n| [                          |\n|  [2020-01-01, 2020-01-03), |\n|  [2020-01-03, 2020-01-05), |\n|  [2020-01-05, 2020-01-06)  |\n| ]                          |\n+----------------------------*/\n\nIn the following example, a date range between ` 2020-01-01 ` and ` 2020-01-06\n` is split into an array of subranges that are two days long, but the final subrange is excluded because it's smaller than two days:\n\n\nSELECT GENERATE_RANGE_ARRAY( RANGE(DATE '2020-01-01', DATE '2020-01-06'),\nINTERVAL 2 DAY,\nFALSE) AS results;\n\n/*----------------------------+\n| results                    |\n+----------------------------+\n| [                          |\n|  [2020-01-01, 2020-01-03), |\n|  [2020-01-03, 2020-01-05)  |\n| ]                          |\n+----------------------------*/"
            },
            "RANGE": {
                "name": "RANGE",
                "summary": "Constructs a range of ` DATE ` , ` DATETIME ` , or ` TIMESTAMP `\nvalues.",
                "description": "**Preview**\n\nThis product or feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of the [ Service Specific Terms\n](/terms/service-terms) . Pre-GA products and features are available \"as is\"\nand might have limited support. For more information, see the [ launch stage descriptions ](/products#product-launch-stages) .\n\n**Note:** To provide feedback or request support for this feature, send an email to [ bigquery-time-series-preview-support@google.com ](mailto:bigquery-\ntime-series-preview-support@google.com) .\n\n\nRANGE(lower_bound, upper_bound)\n\n**Description**\n\nConstructs a range of [ ` DATE ` ](/bigquery/docs/reference/standard-sql/data-\ntypes#date_type) , [ ` DATETIME ` ](/bigquery/docs/reference/standard-\nsql/data-types#datetime_type) , or [ ` TIMESTAMP `\n](/bigquery/docs/reference/standard-sql/data-types#timestamp_type) values.\n\n**Definitions**\n\n* ` lower_bound ` : The range starts from this value. This can be a ` DATE ` , ` DATETIME ` , or ` TIMESTAMP ` value. If this value is ` NULL ` , the range doesn't include a lower bound.\n* ` upper_bound ` : The range ends before this value. This can be a ` DATE ` , ` DATETIME ` , or ` TIMESTAMP ` value. If this value is ` NULL ` , the range doesn't include an upper bound.\n\n**Details**\n\n` lower_bound ` and ` upper_bound ` must be of the same data type.\n\nProduces an error if ` lower_bound ` is greater than or equal to ` upper_bound\n` . To return ` NULL ` instead, add the ` SAFE. ` prefix to the function name.\n\n**Return type**\n\n` RANGE<T> ` , where ` T ` is the same data type as the input.\n\n**Examples**\n\nThe following query constructs a date range:\n\n\nSELECT RANGE(DATE '2022-12-01', DATE '2022-12-31') AS results;\n\n/*--------------------------+\n| results                  |\n+--------------------------+\n| [2022-12-01, 2022-12-31) |\n+--------------------------*/\n\nThe following query constructs a datetime range:\n\n\nSELECT RANGE(DATETIME '2022-10-01 14:53:27',\nDATETIME '2022-10-01 16:00:00') AS results;\n\n/*---------------------------------------------+\n| results                                     |\n+---------------------------------------------+\n| [2022-10-01T14:53:27, 2022-10-01T16:00:00)  |\n+---------------------------------------------*/\n\nThe following query constructs a timestamp range:\n\n\nSELECT RANGE(TIMESTAMP '2022-10-01 14:53:27 America/Los_Angeles',\nTIMESTAMP '2022-10-01 16:00:00 America/Los_Angeles') AS results;\n\n-- Results depend upon where this query was executed.\n/*------------------------------------------------------------------+\n| results                                                          |\n+------------------------------------------------------------------+\n| [2022-10-01 21:53:27.000000 UTC, 2022-10-01 23:00:00.000000 UTC) |\n+------------------------------------------------------------------*/\n\nThe following query constructs a date range with no lower bound:\n\n\nSELECT RANGE(NULL, DATE '2022-12-31') AS results;\n\n/*-------------------------+\n| results                 |\n+-------------------------+\n| [UNBOUNDED, 2022-12-31) |\n+-------------------------*/\n\nThe following query constructs a date range with no upper bound:\n\n\nSELECT RANGE(DATE '2022-10-01', NULL) AS results;\n\n/*--------------------------+\n| results                  |\n+--------------------------+\n| [2022-10-01, UNBOUNDED)  |\n+--------------------------*/"
            },
            "RANGE_CONTAINS": {
                "name": "RANGE_CONTAINS",
                "summary": "Signature 1: Checks if one range is in another range.\n\nSignature 2: Checks if a value is in a range.",
                "description": "**Preview**\n\nThis product or feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of the [ Service Specific Terms\n](/terms/service-terms) . Pre-GA products and features are available \"as is\"\nand might have limited support. For more information, see the [ launch stage descriptions ](/products#product-launch-stages) .\n\n**Note:** To provide feedback or request support for this feature, send an email to [ bigquery-time-series-preview-support@google.com ](mailto:bigquery-\ntime-series-preview-support@google.com) .\n\n* Signature 1  : Checks if every value in one range is in another range.\n* Signature 2  : Checks if a value is in a range.\n\n####  Signature 1\n\n\nRANGE_CONTAINS(outer_range, inner_range)\n\n**Description**\n\nChecks if the inner range is in the outer range.\n\n**Definitions**\n\n* ` outer_range ` : The ` RANGE<T> ` value to search within.\n* ` inner_range ` : The ` RANGE<T> ` value to search for in ` outer_range ` .\n\n**Details**\n\nReturns ` TRUE ` if ` inner_range ` exists in ` outer_range ` . Otherwise,\nreturns ` FALSE ` .\n\n` T ` must be of the same type for all inputs.\n\n**Return type**\n\n` BOOL `\n\n**Examples**\n\nIn the following query, the inner range is in the outer range:\n\n\nSELECT RANGE_CONTAINS( RANGE<DATE> '[2022-01-01, 2023-01-01)',\nRANGE<DATE> '[2022-04-01, 2022-07-01)') AS results;\n\n/*---------+\n| results |\n+---------+\n| TRUE    |\n+---------*/\n\nIn the following query, the inner range is not in the outer range:\n\n\nSELECT RANGE_CONTAINS( RANGE<DATE> '[2022-01-01, 2023-01-01)',\nRANGE<DATE> '[2023-01-01, 2023-04-01)') AS results;\n\n/*---------+\n| results |\n+---------+\n| FALSE   |\n+---------*/\n\n####  Signature 2\n\n\nRANGE_CONTAINS(range_to_search, value_to_find)\n\n**Description**\n\nChecks if a value is in a range.\n\n**Definitions**\n\n* ` range_to_search ` : The ` RANGE<T> ` value to search within.\n* ` value_to_find ` : The value to search for in ` range_to_search ` .\n\n**Details**\n\nReturns ` TRUE ` if ` value_to_find ` exists in ` range_to_search ` .\nOtherwise, returns ` FALSE ` .\n\nThe data type for ` value_to_find ` must be the same data type as ` T ` in `\nrange_to_search ` .\n\n**Return type**\n\n` BOOL `\n\n**Examples**\n\nIn the following query, the value ` 2022-04-01 ` is found in the range `\n[2022-01-01, 2023-01-01) ` :\n\n\nSELECT RANGE_CONTAINS( RANGE<DATE> '[2022-01-01, 2023-01-01)',\nDATE '2022-04-01') AS results;\n\n/*---------+\n| results |\n+---------+\n| TRUE    |\n+---------*/\n\nIn the following query, the value ` 2023-04-01 ` is not found in the range `\n[2022-01-01, 2023-01-01) ` :\n\n\nSELECT RANGE_CONTAINS( RANGE<DATE> '[2022-01-01, 2023-01-01)',\nDATE '2023-04-01') AS results;\n\n/*---------+\n| results |\n+---------+\n| FALSE   |\n+---------*/"
            },
            "RANGE_END": {
                "name": "RANGE_END",
                "summary": "Gets the upper bound of a range.",
                "description": "**Preview**\n\nThis product or feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of the [ Service Specific Terms\n](/terms/service-terms) . Pre-GA products and features are available \"as is\"\nand might have limited support. For more information, see the [ launch stage descriptions ](/products#product-launch-stages) .\n\n**Note:** To provide feedback or request support for this feature, send an email to [ bigquery-time-series-preview-support@google.com ](mailto:bigquery-\ntime-series-preview-support@google.com) .\n\n\nRANGE_END(range_to_check)\n\n**Description**\n\nGets the upper bound of a range.\n\n**Definitions**\n\n* ` range_to_check ` : The ` RANGE<T> ` value.\n\n**Details**\n\nReturns ` NULL ` if the upper bound in ` range_value ` is ` UNBOUNDED ` .\n\nReturns ` NULL ` if ` range_to_check ` is ` NULL ` .\n\n**Return type**\n\n` T ` in ` range_value `\n\n**Examples**\n\nIn the following query, the upper bound of the range is retrieved:\n\n\nSELECT RANGE_END(RANGE<DATE> '[2022-12-01, 2022-12-31)') AS results;\n\n/*------------+\n| results    |\n+------------+\n| 2022-12-31 |\n+------------*/\n\nIn the following query, the upper bound of the range is unbounded, so ` NULL `\nis returned:\n\n\nSELECT RANGE_END(RANGE<DATE> '[2022-12-01, UNBOUNDED)') AS results;\n\n/*------------+\n| results    |\n+------------+\n| NULL       |\n+------------*/"
            },
            "RANGE_INTERSECT": {
                "name": "RANGE_INTERSECT",
                "summary": "Gets a segment of two ranges that intersect.",
                "description": "**Preview**\n\nThis product or feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of the [ Service Specific Terms\n](/terms/service-terms) . Pre-GA products and features are available \"as is\"\nand might have limited support. For more information, see the [ launch stage descriptions ](/products#product-launch-stages) .\n\n**Note:** To provide feedback or request support for this feature, send an email to [ bigquery-time-series-preview-support@google.com ](mailto:bigquery-\ntime-series-preview-support@google.com) .\n\n\nRANGE_INTERSECT(range_a, range_b)\n\n**Description**\n\nGets a segment of two ranges that intersect.\n\n**Definitions**\n\n* ` range_a ` : The first ` RANGE<T> ` value.\n* ` range_b ` : The second ` RANGE<T> ` value.\n\n**Details**\n\nReturns ` NULL ` if any input is ` NULL ` .\n\nProduces an error if ` range_a ` and ` range_b ` don't overlap. To return `\nNULL ` instead, add the ` SAFE. ` prefix to the function name.\n\n` T ` must be of the same type for all inputs.\n\n**Return type**\n\n` RANGE<T> `\n\n**Examples**\n\n\nSELECT RANGE_INTERSECT( RANGE<DATE> '[2022-02-01, 2022-09-01)',\nRANGE<DATE> '[2021-06-15, 2022-04-15)') AS results;\n\n/*--------------------------+\n| results                  |\n+--------------------------+\n| [2022-02-01, 2022-04-15) |\n+--------------------------*/\n\n\nSELECT RANGE_INTERSECT( RANGE<DATE> '[2022-02-01, UNBOUNDED)',\nRANGE<DATE> '[2021-06-15, 2022-04-15)') AS results;\n\n/*--------------------------+\n| results                  |\n+--------------------------+\n| [2022-02-01, 2022-04-15) |\n+--------------------------*/\n\n\nSELECT RANGE_INTERSECT( RANGE<DATE> '[2022-02-01, UNBOUNDED)',\nRANGE<DATE> '[2021-06-15, UNBOUNDED)') AS results;\n\n/*-------------------------+\n| results                 |\n+-------------------------+\n| [2022-02-01, UNBOUNDED) |\n+-------------------------*/"
            },
            "RANGE_OVERLAPS": {
                "name": "RANGE_OVERLAPS",
                "summary": "Checks if two ranges overlap.",
                "description": "**Preview**\n\nThis product or feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of the [ Service Specific Terms\n](/terms/service-terms) . Pre-GA products and features are available \"as is\"\nand might have limited support. For more information, see the [ launch stage descriptions ](/products#product-launch-stages) .\n\n**Note:** To provide feedback or request support for this feature, send an email to [ bigquery-time-series-preview-support@google.com ](mailto:bigquery-\ntime-series-preview-support@google.com) .\n\n\nRANGE_OVERLAPS(range_a, range_b)\n\n**Description**\n\nChecks if two ranges overlap.\n\n**Definitions**\n\n* ` range_a ` : The first ` RANGE<T> ` value.\n* ` range_b ` : The second ` RANGE<T> ` value.\n\n**Details**\n\nReturns ` TRUE ` if a part of ` range_a ` intersects with ` range_b ` ,\notherwise returns ` FALSE ` .\n\n` T ` must be of the same type for all inputs.\n\nTo get the part of the range that overlaps, use the  ` RANGE_INTERSECT `\nfunction.\n\n**Return type**\n\n` BOOL `\n\n**Examples**\n\nIn the following query, the first and second ranges overlap between `\n2022-02-01 ` and ` 2022-04-15 ` :\n\n\nSELECT RANGE_OVERLAPS( RANGE<DATE> '[2022-02-01, 2022-09-01)',\nRANGE<DATE> '[2021-06-15, 2022-04-15)') AS results;\n\n/*---------+\n| results |\n+---------+\n| TRUE    |\n+---------*/\n\nIn the following query, the first and second ranges don't overlap:\n\n\nSELECT RANGE_OVERLAPS( RANGE<DATE> '[2020-02-01, 2020-09-01)',\nRANGE<DATE> '[2021-06-15, 2022-04-15)') AS results;\n\n/*---------+\n| results |\n+---------+\n| FALSE   |\n+---------*/\n\nIn the following query, the first and second ranges overlap between `\n2022-02-01 ` and ` UNBOUNDED ` :\n\n\nSELECT RANGE_OVERLAPS( RANGE<DATE> '[2022-02-01, UNBOUNDED)',\nRANGE<DATE> '[2021-06-15, UNBOUNDED)') AS results;\n\n/*---------+\n| results |\n+---------+\n| TRUE    |\n+---------*/"
            },
            "RANGE_SESSIONIZE": {
                "name": "RANGE_SESSIONIZE",
                "summary": "Produces a table of sessionized ranges.",
                "description": "**Preview**\n\nThis product or feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of the [ Service Specific Terms\n](/terms/service-terms) . Pre-GA products and features are available \"as is\"\nand might have limited support. For more information, see the [ launch stage descriptions ](/products#product-launch-stages) .\n\n**Note:** To provide feedback or request support for this feature, send an email to [ bigquery-time-series-preview-support@google.com ](mailto:bigquery-\ntime-series-preview-support@google.com) .\n\n\nRANGE_SESSIONIZE( TABLE table_name,\nrange_column,\npartitioning_columns )\n\n\nRANGE_SESSIONIZE( TABLE table_name,\nrange_column,\npartitioning_columns,\nsessionize_option )\n\n**Description**\n\nProduces a table of sessionized ranges.\n\n**Definitions**\n\n* ` table_name ` : A table expression that represents the name of the table to construct. This can represent any relation with ` range_column ` .\n* ` range_column ` : A ` STRING ` literal that indicates which ` RANGE ` column in a table contains the data to sessionize.\n* ` partitioning_columns ` : An ` ARRAY<STRING> ` literal that indicates which columns should partition the data before the data is sessionized.\n* ` sessionize_option ` : A ` STRING ` value that describes how order-adjacent ranges are sessionized. Your choices are as follows:\n\n* ` MEETS ` (default): Ranges that meet or overlap are sessionized.\n\n* ` OVERLAPS ` : Only a range that is overlapped by another range is sessionized.\n\nIf this argument is not provided, ` MEETS ` is used by default.\n\n**Details**\n\nThis function produces a table that includes all columns in the input table and an additional ` RANGE ` column called ` session_range ` , which indicates the start and end of a session. The start and end of each session is determined by the ` sessionize_option ` argument.\n\n**Return type**\n\n` TABLE `\n\n**Examples**\n\nThe examples in this section reference the following table called `\nmy_sessionized_range_table ` in a dataset called ` mydataset ` :\n\n\nINSERT mydataset.my_sessionized_range_table (emp_id, dept_id, duration) VALUES(10, 1000, RANGE<DATE> '[2010-01-10, 2010-03-10)'),\n(10, 2000, RANGE<DATE> '[2010-03-10, 2010-07-15)'),\n(10, 2000, RANGE<DATE> '[2010-06-15, 2010-08-18)'),\n(20, 2000, RANGE<DATE> '[2010-03-10, 2010-07-20)'),\n(20, 1000, RANGE<DATE> '[2020-05-10, 2020-09-20)');\n\nSELECT * FROM mydataset.my_sessionized_range_table ORDER BY emp_id;\n\n/*--------+---------+--------------------------+\n| emp_id | dept_id | duration                 |\n+--------+---------+--------------------------+\n| 10     | 1000    | [2010-01-10, 2010-03-10) |\n| 10     | 2000    | [2010-03-10, 2010-07-15) |\n| 10     | 2000    | [2010-06-15, 2010-08-18) |\n| 20     | 2000    | [2010-03-10, 2010-07-20) |\n| 20     | 1000    | [2020-05-10, 2020-09-20) |\n+--------+---------+--------------------------*/\n\nIn the following query, a table of sessionized data is produced for `\nmy_sessionized_range_table ` , and only ranges that meet or overlap are sessionized:\n\n\nSELECT emp_id, duration, session_range FROM RANGE_SESSIONIZE( TABLE mydataset.my_sessionized_range_table,\n'duration',\n['emp_id']) ORDER BY emp_id;\n\n/*--------+--------------------------+--------------------------+\n| emp_id | duration                 | session_range            |\n+--------+--------------------------+--------------------------+\n| 10     | [2010-01-10, 2010-03-10) | [2010-01-10, 2010-08-18) |\n| 10     | [2010-03-10, 2010-07-15) | [2010-01-10, 2010-08-18) |\n| 10     | [2010-06-15, 2010-08-18) | [2010-01-10, 2010-08-18) |\n| 20     | [2010-03-10, 2010-07-20) | [2010-03-10, 2010-07-20) |\n| 20     | [2020-05-10, 2020-09-20) | [2020-05-10, 2020-09-20) |\n+--------+-----------------------------------------------------*/\n\nIn the following query, a table of sessionized data is produced for `\nmy_sessionized_range_table ` , and only a range that is overlapped by another range is sessionized:\n\n\nSELECT emp_id, duration, session_range FROM RANGE_SESSIONIZE( TABLE mydataset.my_sessionized_range_table,\n'duration',\n['emp_id'],\n'OVERLAPS') ORDER BY emp_id;\n\n/*--------+--------------------------+--------------------------+\n| emp_id | duration                 | session_range            |\n+--------+--------------------------+--------------------------+\n| 10     | [2010-03-10, 2010-07-15) | [2010-03-10, 2010-08-18) |\n| 10     | [2010-06-15, 2010-08-18) | [2010-03-10, 2010-08-18) |\n| 10     | [2010-01-10, 2010-03-10) | [2010-01-10, 2010-03-10) |\n| 20     | [2020-05-10, 2020-09-20) | [2020-05-10, 2020-09-20) |\n| 20     | [2010-03-10, 2010-07-20) | [2010-03-10, 2010-07-20) |\n+--------+-----------------------------------------------------*/\n\nIf you need to normalize sessionized data, you can use a query similar to the following:\n\n\nSELECT emp_id, session_range AS normalized FROM ( SELECT emp_id, session_range FROM RANGE_SESSIONIZE( TABLE mydataset.my_sessionized_range_table,\n'duration',\n['emp_id'],\n'MEETS') ) GROUP BY emp_id, normalized;\n\n/*--------+--------------------------+\n| emp_id | normalized               |\n+--------+--------------------------+\n| 20     | [2010-03-10, 2010-07-20) |\n| 10     | [2010-01-10, 2010-08-18) |\n| 20     | [2020-05-10, 2020-09-20) |\n+--------+--------------------------*/"
            },
            "RANGE_START": {
                "name": "RANGE_START",
                "summary": "Gets the lower bound of a range.",
                "description": "**Preview**\n\nThis product or feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of the [ Service Specific Terms\n](/terms/service-terms) . Pre-GA products and features are available \"as is\"\nand might have limited support. For more information, see the [ launch stage descriptions ](/products#product-launch-stages) .\n\n**Note:** To provide feedback or request support for this feature, send an email to [ bigquery-time-series-preview-support@google.com ](mailto:bigquery-\ntime-series-preview-support@google.com) .\n\n\nRANGE_START(range_to_check)\n\n**Description**\n\nGets the lower bound of a range.\n\n**Definitions**\n\n* ` range_to_check ` : The ` RANGE<T> ` value.\n\n**Details**\n\nReturns ` NULL ` if the lower bound of ` range_value ` is ` UNBOUNDED ` .\n\nReturns ` NULL ` if ` range_to_check ` is ` NULL ` .\n\n**Return type**\n\n` T ` in ` range_value `\n\n**Examples**\n\nIn the following query, the lower bound of the range is retrieved:\n\n\nSELECT RANGE_START(RANGE<DATE> '[2022-12-01, 2022-12-31)') AS results;\n\n/*------------+\n| results    |\n+------------+\n| 2022-12-01 |\n+------------*/\n\nIn the following query, the lower bound of the range is unbounded, so ` NULL `\nis returned:\n\n\nSELECT RANGE_START(RANGE<DATE> '[UNBOUNDED, 2022-12-31)') AS results;\n\n/*------------+\n| results    |\n+------------+\n| NULL       |\n+------------*/"
            }
        }
    },
    {
        "category": "search-functions",
        "description": "GoogleSQL for BigQuery supports the following search functions.",
        "source": "search_functions.txt",
        "functions": {
            "SEARCH": {
                "name": "SEARCH",
                "summary": "Checks to see whether a table or other search data contains a set of search terms.",
                "description": "SEARCH( data_to_search, search_query\n[, json_scope=>{ 'JSON_VALUES' | 'JSON_KEYS' | 'JSON_KEYS_AND_VALUES' }]\n[, analyzer=>{ 'LOG_ANALYZER' | 'NO_OP_ANALYZER' | 'PATTERN_ANALYZER'}]\n[, analyzer_options=>analyzer_options_values]\n)\n\n**Description**\n\nThe ` SEARCH ` function checks to see whether a BigQuery table or other search data contains a set of search terms (tokens). It returns ` TRUE ` if all search terms appear in the data, based on the text analysis described in the [\ntext analyzer ](/bigquery/docs/reference/standard-sql/text-analysis) , and `\nFALSE ` otherwise.\n\n**Definitions**\n\n* ` data_to_search ` : The data to search over. The value can be:\n\n* Any GoogleSQL data type literal\n* A list of columns\n* A table reference\n* A column of any type\n\nA table reference is evaluated as a ` STRUCT ` whose fields are the columns of the table. ` data_to_search ` can be any type, but ` SEARCH ` will return `\nFALSE ` for all types except those listed here:\n\n* ` ARRAY<STRING> `\n* ` ARRAY<STRUCT> `\n* ` JSON `\n* ` STRING `\n* ` STRUCT `\n\nYou can search for string literals in columns of the preceding types. For additional rules, see  Search data rules  .\n\n* ` search_query ` : A ` STRING ` literal, or a ` STRING ` constant expression that represents the terms of the search query. If ` search_query ` is ` NULL ` , an error is returned. If ` search_query ` contains no tokens and the text analyzer is ` LOG_ANALYZER ` , an error is returned.\n* ` json_scope ` : Optional mandatory-named argument that takes one of the following values to indicate the scope of JSON data to be searched. It has no effect if ` data_to_search ` isn't a JSON value or doesn't contain a JSON field.\n\n* ` 'JSON_VALUES' ` (default): Only the JSON values are searched. If ` json_scope ` isn't provided, this is used by default.\n\n* ` 'JSON_KEYS' ` : Only the JSON keys are searched.\n\n* ` 'JSON_KEYS_AND_VALUES' ` : The JSON keys and values are searched.\n\n* ` analyzer ` : Optional mandatory-named argument that takes one of the following values to indicate the text analyzer to use:\n\n* ` 'LOG_ANALYZER' ` (default): Breaks the input into terms when delimiters are encountered and then normalizes the terms. For more information, see [ ` LOG_ANALYZER ` ](/bigquery/docs/reference/standard-sql/text-analysis#log_analyzer) .\n\n* ` 'NO_OP_ANALYZER' ` : Extracts the text as a single term (token), but doesn't apply normalization. For more information about this analyzer, see [ ` NO_OP_ANALYZER ` ](/bigquery/docs/reference/standard-sql/text-analysis#no_op_analyzer) .\n\n* ` 'PATTERN_ANALYZER' ` : Breaks the input into terms that match a regular expression. For more information, see [ ` PATTERN_ANALYZER ` text analyzer ](/bigquery/docs/reference/standard-sql/text-analysis#pattern_analyzer) .\n\n* ` analyzer_options ` : Optional mandatory-named argument that takes a list of text analysis rules as a JSON-formatted ` STRING ` . For more information, see [ Text analyzer options ](/bigquery/docs/reference/standard-sql/text-analysis#text_analyzer_options) .\n\n**Details**\n\nThe ` SEARCH ` function is designed to work with [ search indexes\n](/bigquery/docs/search-index) to optimize point lookups. Although the `\nSEARCH ` function works for tables that aren't indexed, its performance will be greatly improved with a search index. If both the analyzer and analyzer options match the one used to create the index, the search index will be used.\n\n**Rules for` search_query ` **\n\nBacktick rules for  ` search_query ` :\n\n* If the ` LOG_ANALYZER ` text analyzer is used, text enclosed in backticks forces an exact match.\n\nFor example, ` `Hello World` happy days ` becomes ` Hello World ` , ` happy `\n, and ` days ` .\n\n* Search terms enclosed in backticks must match exactly in ` data_to_search ` , subject to the following conditions:\n\n* It appears at the start of ` data_to_search ` or is immediately preceded by a delimiter.\n\n* It appears at the end of ` data_to_search ` or is immediately followed by a delimiter.\n\nFor example, ` SEARCH('foo.bar', '`foo.`') ` returns ` FALSE ` because the text enclosed in the backticks ` foo. ` is immediately followed by the character ` b ` in the search data ` foo.bar ` , rather than by a delimiter or the end of the string. However, ` SEARCH('foo..bar', '`foo.`') ` returns `\nTRUE ` because ` foo. ` is immediately followed by the delimiter ` . ` in the search data.\n\n* The backtick itself can be escaped using a backslash, as in ` \\`foobar\\` ` .\n\n* The following are reserved words and must be enclosed in backticks:\n\n` AND ` , ` NOT ` , ` OR ` , ` IN ` , and ` NEAR `\n\nReserved character rules for  ` search_query ` :\n\n* Text not enclosed in backticks requires the following reserved characters to be escaped by a double backslash ` \\\\ ` :\n\n* ` [ ] < > ( ) { } | ! ' \" * & ? + / : = - \\ ~ ^ `\n\n* If the quoted string is preceded by the character ` r ` or ` R ` , such as ` r\"my\\+string\" ` , then it is treated as a raw string and only a single backslash is required to escape the reserved characters. For more information about raw strings and escape sequences, see [ String and byte literals ](/bigquery/docs/reference/standard-sql/lexical#literals) .\n\n**How` search_query ` is broken into searchable terms **\n\nThe following table shows how  ` search_query ` is broken into searchable terms by the ` LOG_ANALYZER ` text analyzer. All entries are strings.\n\nsearch_query  |  searchable terms\n---|---\n127.0.0.1  |  127 0 1 127.0.0.1 . 127.0.0 127.0 0.0 0.0.1 0.1 foobar@example.com  |  foobar example com foobar@example example.com foobar@example.com The fox.  |  the fox The The fox The fox.\nfox fox.\n\nThe following table shows how ` search_query ` is broken into query terms by the ` LOG_ANALYZER ` text analyzer. All entries are strings.\n\nsearch_query  |  query terms\n---|---\n127.0.0.1  |  127 0 1\n\n`127.0.0.1`  |  127.0.0.1 foobar@example.com  |  foobar example com\n`foobar@example.com`  |  foobar@example.com\n\n**Rules for` data_to_search ` **\n\nGeneral rules for  ` data_to_search ` :\n\n* ` data_to_search ` must contain all terms, in any order, from the ` search_query ` for the function to return ` TRUE ` .\n* To perform a cross-field search, ` data_to_search ` must be a ` STRUCT ` , ` ARRAY ` , or ` JSON ` data type.\n* Each ` STRING ` field in a compound data type is individually searched for terms.\n* If at least one field in ` data_to_search ` includes all search terms in any order, ` SEARCH ` returns ` TRUE ` . Otherwise it has the following behavior:\n\n* If at least one ` STRING ` field is ` NULL ` , ` SEARCH ` returns ` NULL ` .\n\n* Otherwise, ` SEARCH ` returns ` FALSE ` .\n\n**Return type**\n\n` BOOL `\n\n**Examples**\n\nThe following queries show how tokens in ` search_query ` are analyzed by a `\nSEARCH ` function call using the default analyzer, ` LOG_ANALYZER ` :\n\n\nSELECT\n-- ERROR: `search_query` is NULL.\nSEARCH('foobarexample', NULL) AS a,\n\n-- ERROR: `search_query` contains no tokens.\nSEARCH('foobarexample', '') AS b,\n\n\nSELECT\n-- TRUE: '-' and ' ' are delimiters.\nSEARCH('foobar-example', 'foobar example') AS a,\n\n-- TRUE: The search query is a constant expression evaluated to 'foobar'.\nSEARCH('foobar-example', CONCAT('foo', 'bar')) AS b,\n\n-- FALSE: The search_query is not split.\nSEARCH('foobar-example', 'foobarexample') AS c,\n\n-- TRUE: The double backslash escapes the ampersand which is a delimiter.\nSEARCH('foobar-example', 'foobar\\\\&example') AS d,\n\n-- TRUE: The single backslash escapes the ampersand in a raw string.\nSEARCH('foobar-example', R'foobar\\&example')AS e,\n\n-- FALSE: The backticks indicate that there must be an exact match for\n-- foobar&example.\nSEARCH('foobar-example', '`foobar&example`') AS f,\n\n-- TRUE: An exact match is found.\nSEARCH('foobar&example', '`foobar&example`') AS g\n\n/*-------+-------+-------+-------+-------+-------+-------*\n| a     | b     | c     | d     | e     | f     | g     |\n+-------+-------+-------+-------+-------+-------+-------+\n| true  | true  | false | true  | true  | false | true  |\n*-------+-------+-------+-------+-------+-------+-------*/\n\n\nSELECT\n-- TRUE: The order of terms doesn't matter.\nSEARCH('foobar-example', 'example foobar') AS a,\n\n-- TRUE: Tokens are made lower-case.\nSEARCH('foobar-example', 'Foobar Example') AS b,\n\n-- TRUE: An exact match is found.\nSEARCH('foobar-example', '`foobar-example`') AS c,\n\n-- FALSE: Backticks preserve capitalization.\nSEARCH('foobar-example', '`Foobar`') AS d,\n\n-- FALSE: Backticks don't have special meaning for search_data and are\n-- not delimiters in the default LOG_ANALYZER.\nSEARCH('`foobar-example`', '`foobar-example`') AS e,\n\n-- TRUE: An exact match is found after the delimiter in search_data.\nSEARCH('foobar@example.com', '`example.com`') AS f,\n\n-- TRUE: An exact match is found between the space delimiters.\nSEARCH('a foobar-example b', '`foobar-example`') AS g;\n\n/*-------+-------+-------+-------+-------+-------+-------*\n| a     | b     | c     | d     | e     | f     | g     |\n+-------+-------+-------+-------+-------+-------+-------+\n| true  | true  | true  | false | false | true  | true  |\n*-------+-------+-------+-------+-------+-------+-------*/\n\n\nSELECT\n-- FALSE: No single array entry matches all search terms.\nSEARCH(['foobar', 'example'], 'foobar example') AS a,\n\n-- FALSE: The search_query is equivalent to foobar\\\\=.\nSEARCH('foobar=', '`foobar\\\\=`') AS b,\n\n-- FALSE: This is equivalent to the previous example.\nSEARCH('foobar=', R'`\\foobar=`') AS c,\n\n-- TRUE: The equals sign is a delimiter in the data and query.\nSEARCH('foobar=', 'foobar\\\\=') AS d,\n\n-- TRUE: This is equivalent to the previous example.\nSEARCH('foobar=', R'foobar\\=') AS e,\n\n-- TRUE: An exact match is found.\nSEARCH('foobar.example', '`foobar`') AS f,\n\n-- FALSE: `foobar.\\` is not analyzed because of backticks; it is not\n-- followed by a delimiter in search_data 'foobar.example'.\nSEARCH('foobar.example', '`foobar.\\`') AS g,\n\n-- TRUE: `foobar.` is not analyzed because of backticks; it is\n-- followed by the delimiter '.' in search_data 'foobar..example'.\nSEARCH('foobar..example', '`foobar.`') AS h;\n\n/*-------+-------+-------+-------+-------+-------+-------+-------*\n| a     | b     | c     | d     | e     | f     | g     | h     |\n+-------+-------+-------+-------+-------+-------+-------+-------+\n| false | false | false | true  | true  | true  | false | true  |\n*-------+-------+-------+-------+-------+-------+-------+-------*/\n\nThe following query shows examples of calls to the ` SEARCH ` function using the ` NO_OP_ANALYZER ` text analyzer and reasons for various return values:\n\n\nSELECT\n-- TRUE: exact match SEARCH('foobar', 'foobar', analyzer=>'NO_OP_ANALYZER') AS a,\n\n-- FALSE: Backticks are not special characters for `NO_OP_ANALYZER`.\nSEARCH('foobar', '\\`foobar\\`', analyzer=>'NO_OP_ANALYZER') AS b,\n\n-- FALSE: The capitalization does not match.\nSEARCH('foobar', 'Foobar', analyzer=>'NO_OP_ANALYZER') AS c,\n\n-- FALSE: There are no delimiters for `NO_OP_ANALYZER`.\nSEARCH('foobar example', 'foobar', analyzer=>'NO_OP_ANALYZER') AS d,\n\n-- TRUE: An exact match is found.\nSEARCH('', '', analyzer=>'NO_OP_ANALYZER') AS e;\n\n/*-------+-------+-------+-------+-------*\n| a     | b     | c     | d     | e     |\n+-------+-------+-------+-------+-------+\n| true  | false | false | false | true  |\n*-------+-------+-------+-------+-------*/\n\nConsider the following table called ` meals ` with columns ` breakfast ` , `\nlunch ` , and ` dinner ` :\n\n\n/*-------------------+-------------------------+------------------*\n| breakfast         | lunch                   | dinner           |\n+-------------------+-------------------------+------------------+\n| Potato pancakes   | Toasted cheese sandwich | Beef soup        |\n| Avocado toast     | Tomato soup             | Chicken soup     |\n*-------------------+-------------------------+------------------*/\n\nThe following query shows how to search single columns, multiple columns, and whole tables, using the default [ ` LOG_ANALYZER `\n](/bigquery/docs/reference/standard-sql/text-analysis#log_analyzer) text analyzer with the default analyzer options:\n\n\nWITH meals AS ( SELECT\n'Potato pancakes' AS breakfast,\n'Toasted cheese sandwich' AS lunch,\n'Beef soup' AS dinner UNION ALL SELECT\n'Avocado toast' AS breakfast,\n'Tomato soup' AS lunch,\n'Chicken soup' AS dinner ) SELECT SEARCH(lunch, 'soup') AS lunch_soup,\nSEARCH((breakfast, dinner), 'soup') AS breakfast_or_dinner_soup,\nSEARCH(meals, 'soup') AS anytime_soup FROM meals;\n\n/*------------+--------------------------+--------------*\n| lunch_soup | breakfast_or_dinner_soup | anytime_soup |\n+------------+--------------------------+--------------+\n| false      | true                     | true         |\n| true       | true                     | true         |\n*------------+--------------------------+--------------*/\n\nThe following query shows additional ways to search, using the default [ `\nLOG_ANALYZER ` ](/bigquery/docs/reference/standard-sql/text-\nanalysis#log_analyzer) text analyzer with default analyzer options:\n\n\nWITH data AS ( SELECT 'Please use foobar@example.com as your email.' AS email ) SELECT SEARCH(email, 'exam') AS a,\nSEARCH(email, 'foobar') AS b,\nSEARCH(email, 'example.com') AS c FROM data;\n\n/*-------+-------+-------*\n| a     | b     | c     |\n+-------+-------+-------+\n| false | true  | true  |\n*-------+-------+-------*/\n\nThe following query shows additional ways to search, using the default [ `\nLOG_ANALYZER ` ](/bigquery/docs/reference/standard-sql/text-\nanalysis#log_analyzer) text analyzer with custom analyzer options. Terms are only split when a space or ` @ ` symbol is encountered.\n\n\nWITH data AS ( SELECT 'Please use foobar@example.com as your email.' AS email ) SELECT SEARCH(email, 'foobar', analyzer_options=>'{\"delimiters\": [\" \", \"@\"]}') AS a,\nSEARCH(email, 'example', analyzer_options=>'{\"delimiters\": [\" \", \"@\"]}') AS b,\nSEARCH(email, 'example.com', analyzer_options=>'{\"delimiters\": [\" \", \"@\"]}') AS c,\nSEARCH(email, 'foobar@example.com', analyzer_options=>'{\"delimiters\": [\" \", \"@\"]}') AS d FROM data;\n\n/*-------+-------+-------+-------*\n| a     | b     | c     | d     |\n+-------+-------+-------+-------+\n| true  | false | true  | true  |\n*-------+-------+-------+-------*/\n\nThe following query shows how to search, using the [ ` NO_OP_ANALYZER `\n](/bigquery/docs/reference/standard-sql/text-analysis#no_op_analyzer) text analyzer:\n\n\nWITH meals AS ( SELECT 'Tomato soup' AS lunch ) SELECT SEARCH(lunch, 'Tomato soup', analyzer=>'NO_OP_ANALYZER') AS a,\nSEARCH(lunch, 'soup', analyzer=>'NO_OP_ANALYZER') AS b,\nSEARCH(lunch, 'tomato soup', analyzer=>'NO_OP_ANALYZER') AS c FROM meals;\n\n/*-------+-------+-------*\n| a     | b     | c     |\n+-------+-------+-------+\n| true  | false | false |\n*-------+-------+-------*/\n\nThe following query shows how to use the [ ` PATTERN_ANALYZER `\n](/bigquery/docs/reference/standard-sql/text-analysis#pattern_analyzer) text analyzer with default analyzer options:\n\n\nWITH data AS ( SELECT 'Please use foobar@example.com as your email.' AS email ) SELECT SEARCH(email, 'exam', analyzer=>'PATTERN_ANALYZER') AS a,\nSEARCH(email, 'foobar', analyzer=>'PATTERN_ANALYZER') AS b,\nSEARCH(email, 'example.com', analyzer=>'PATTERN_ANALYZER') AS c FROM data;\n\n/*-------+-------+-------*\n| a     | b     | c     |\n+-------+-------+-------+\n| false | true  | true  |\n*-------+-------+-------*/\n\nThe following query shows additional ways to search, using the [ `\nPATTERN_ANALYZER ` ](/bigquery/docs/reference/standard-sql/text-\nanalysis#pattern_analyzer) text analyzer with custom analyzer options:\n\n\nWITH data AS ( SELECT 'Please use foobar@EXAMPLE.com as your email.' AS email ) SELECT SEARCH(email, 'EXAMPLE', analyzer=>'PATTERN_ANALYZER', analyzer_options=>'{\"patterns\": [\"[A-Z]*\"]}') AS a,\nSEARCH(email, 'example', analyzer=>'PATTERN_ANALYZER', analyzer_options=>'{\"patterns\": [\"[a-z]*\"]}') AS b,\nSEARCH(email, 'example.com', analyzer=>'PATTERN_ANALYZER', analyzer_options=>'{\"patterns\": [\"[a-z]*\"]}') AS c,\nSEARCH(email, 'example.com', analyzer=>'PATTERN_ANALYZER', analyzer_options=>'{\"patterns\": [\"[a-zA-Z.]*\"]}') AS d FROM data;\n\n/*-------+-------+-------+-------*\n| a     | b     | c     | d     |\n+-------+-------+-------+-------+\n| true  | false | false | true  |\n*-------+-------+-------+-------*/\n\nFor additional examples that include analyzer options, see the [ Text analysis\n](/bigquery/docs/reference/standard-sql/text-analysis) reference guide.\n\nFor helpful analyzer recipes that you can use to enhance analyzer-supported queries, see the [ Search with text analyzers ](/bigquery/docs/text-analysis-\nsearch) user guide."
            },
            "VECTOR_SEARCH": {
                "name": "VECTOR_SEARCH",
                "summary": "Performs a vector search on embeddings to find semantically similar entities.",
                "description": "**Preview**\n\nThis product or feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of the [ Service Specific Terms\n](/terms/service-terms) . Pre-GA products and features are available \"as is\"\nand might have limited support. For more information, see the [ launch stage descriptions ](/products#product-launch-stages) .\n\nTo provide feedback or request support for this feature, send email to [ bq-\nvector-search@google.com ](mailto:bq-vector-search@google.com) .\n\n\nVECTOR_SEARCH( TABLE base_table,\ncolumn_to_search,\nTABLE query_table\n[, query_column_to_search => query_column_to_search_value]\n[, top_k => top_k_value ]\n[, distance_type => distance_type_value ]\n[, options => options_value ]\n)\n\n\nVECTOR_SEARCH( TABLE base_table,\ncolumn_to_search,\n(query_statement)\n[, query_column_to_search => query_column_to_search_value]\n[, top_k => top_k_value ]\n[, distance_type => distance_type_value ]\n[, options => options_value ]\n)\n\n**Description**\n\nThe ` VECTOR_SEARCH ` function lets you search embeddings to find semantically similar entities.\n\nEmbeddings are high-dimensional numerical vectors that represent a given entity, like a piece of text or an audio file. Machine learning (ML) models use embeddings to encode semantics about such entities to make it easier to reason about and compare them. For example, a common operation in clustering,\nclassification, and recommendation models is to measure the distance between vectors in an [ embedding space ](https://en.wikipedia.org/wiki/Latent_space) to find items that are most semantically similar.\n\n**Definitions**\n\n* ` base_table ` : The table to search for nearest neighbor embeddings.\n* ` column_to_search ` : The name of the base table column to search for nearest neighbor embeddings. The column must have a type of ` ARRAY<FLOAT64> ` . All elements in the array must be non- ` NULL ` , and all values in the column must have the same array dimensions. If the column has a vector index, BigQuery attempts to use it. To determine if an index was used in the vector search, see [ Vector index usage ](/bigquery/docs/vector-index#vector_index_usage) .\n* ` query_table ` : The table that provides the embeddings for which to find nearest neighbors. All columns are passed through as output columns.\n* ` query_statement ` : A query that provides the embeddings for which to find nearest neighbors. All columns are passed through as output columns.\n* ` query_column_to_search ` : An optional ` STRING ` positional-named argument. ` query_column_to_search_value ` specifies the name of the column in the query table or statement that contains the embeddings for which to find nearest neighbors. The column must have a type of ` ARRAY<FLOAT64> ` . All elements in the array must be non- ` NULL ` and all values in the column must have the same array dimensions as the values in the ` column_to_search ` column. If you don't specify ` query_column_to_search_value ` , the function uses the ` column_to_search ` value.\n* ` top_k ` : An optional ` INT64 ` mandatory-named argument. ` top_k_value ` specifies the number of nearest neighbors to return. The default is ` 10 ` . A negative value is treated as infinity, meaning that all values are counted as neighbors and returned.\n* ` distance_type ` : An optional ` STRING ` mandatory-named argument. ` distance_type_value ` specifies the type of metric to use to compute the distance between two vectors. Supported distance types are [ ` EUCLIDEAN ` ](https://en.wikipedia.org/wiki/Euclidean_distance) and [ ` COSINE ` ](https://en.wikipedia.org/wiki/Cosine_similarity#Cosine_Distance) . The default is ` EUCLIDEAN ` .\n\nIf you don't specify ` distance_type_value ` and the ` column_to_search `\ncolumn has a vector index that is used, ` VECTOR_SEARCH ` uses the distance type specified in the [ ` distance_type ` option\n](/bigquery/docs/reference/standard-sql/data-definition-\nlanguage#vector_index_option_list) of the ` CREATE VECTOR INDEX ` statement.\n\n* ` options ` : An optional JSON-formatted ` STRING ` mandatory-named argument. ` options_value ` is a literal that specifies the following vector search options:\n\n* ` fraction_lists_to_search ` : A JSON number that specifies the percentage of lists to search. For example, ` options => '{\"fraction_lists_to_search\":0.15}' ` . The ` fraction_lists_to_search ` value must be in the range ` 0.0 ` to ` 1.0 ` , exclusive.\n\nSpecifying a higher percentage leads to higher recall and slower performance,\nand the converse is true when specifying a lower percentage.\n\n` fraction_lists_to_search ` is only used when a vector index is also used. If you don't specify a ` fraction_lists_to_search ` value but an index is matched, the default number of lists to scan is calculated as ` min(0.002 *\nnumber_of_lists, 10) ` .\n\nThe number of available lists to search is determined by the [ ` num_lists `\noption ](/bigquery/docs/reference/standard-sql/data-definition-\nlanguage#vector_index_option_list) in the ` ivf_options ` option of the `\nCREATE VECTOR INDEX ` statement if that is specified. Otherwise, BigQuery calculates an appropriate number.\n\nYou can't specify ` fraction_lists_to_search ` when ` use_brute_force ` is set to ` true ` .\n\n* ` use_brute_force ` : A JSON boolean that determines whether to use brute force search by skipping the vector index if one is available. For example, ` options => '{\"use_brute_force\":true}' ` . The default is ` false ` . If you specify ` use_brute_force=false ` and there is no useable vector index available, brute force is used anyway.\n\n` options ` defaults to ` '{}' ` to denote that all underlying options use their corresponding default values.\n\n**Details**\n\nYou can optionally use ` VECTOR_SEARCH ` with a [ vector index\n](/bigquery/docs/vector-index) . When a vector index is used, ` VECTOR_SEARCH\n` uses the [ Approximate Nearest Neighbor\n](https://en.wikipedia.org/wiki/Nearest_neighbor_search#Approximation_methods) search technique to help improve vector search performance, with the trade-off of reducing [ recall ](https://developers.google.com/machine-learning/crash-\ncourse/classification/precision-and-recall#recallsearch_term_rules) and so returning more approximate results. Brute force is used to return exact results when a vector index isn't available, and you can choose to use brute force to get exact results even when a vector index is available.\n\n**Output**\n\nFor each row in the query data, the output contains multiple rows from the base table that satisfy the search criteria. The number of results rows per query table row is either 10 or the ` top_k ` value if it is specified. The order of the output isn't guaranteed.\n\nThe output includes the following columns:\n\n* ` query ` : A ` STRUCT ` value that contains all selected columns from the query data.\n* ` base ` : A ` STRUCT ` value that contains all columns from the base table.\n* ` distance ` : A ` FLOAT64 ` value that represents the distance between the base data and the query data.\n\n**Limitations**\n\nBigQuery data security and governance rules apply to the use of `\nVECTOR_SEARCH ` , which results in the following behavior:\n\n* If the base table has [ row-level security policies ](/bigquery/docs/row-level-security-intro) , ` VECTOR_SEARCH ` applies the row-level access policies to the query results.\n* If the indexed column from the base table has [ data masking policies ](/bigquery/docs/column-data-masking-intro) , ` VECTOR_SEARCH ` succeeds only if the user running the query has the [ ` Fine-Grained Reader ` ](/iam/docs/understanding-roles#datacatalog.categoryFineGrainedReader) role on the policy tags that are used. Otherwise, ` VECTOR_SEARCH ` fails with an invalid query error.\n* If any base table column or any column in the query table or statement has [ column-level security policies ](/bigquery/docs/column-level-security) and you don't have appropriate permissions to access the column, ` VECTOR_SEARCH ` fails with a permission denied error.\n\n**Examples**\n\nThe following queries create test tables ` table1 ` and ` table2 ` to use in subsequent query examples :\n\n\nCREATE OR REPLACE TABLE mydataset.table1 ( id INT64,\nmy_embedding ARRAY<FLOAT64>\n);\n\nINSERT mydataset.table1 (id, my_embedding) VALUES(1, [1.0, 2.0]),\n(2, [2.0, 4.0]),\n(3, [1.5, 7.0]),\n(4, [1.0, 3.2]),\n(5, [5.0, 5.4]),\n(6, [3.7, 1.8]),\n(7, [4.4, 2.9]);\n\n\nCREATE OR REPLACE TABLE mydataset.table2 ( query_id STRING,\nembedding ARRAY<FLOAT64>\n);\n\nINSERT mydataset.table2 (query_id, embedding) VALUES('dog', [1.0, 2.0]),\n('cat', [3.0, 5.2]);\n\nThe following example searches the ` my_embedding ` column of ` table1 ` for the top two embeddings that match each row of data in the ` embedding ` column of ` table2 ` :\n\n\nSELECT *\nFROM VECTOR_SEARCH( TABLE mydataset.table1,\n'my_embedding',\n(SELECT query_id, embedding FROM mydataset.table2),\n'embedding',\ntop_k => 2);\n\n/*------  --------+-----------------+---------+----------------------------------------*\n| query.query_id | query.embedding | base.id | base.my_embedding | distance           |\n+----------------+-----------------+---------+-------------------+--------------------+\n| dog            | 1.0             | 1       | 1.0               | 0                  |\n|                | 2.0             |         | 2.0               |                    |\n+----------------+-----------------+---------+-------------------+--------------------+\n| dog            | 1.0             | 4       | 1.0               | 1.2000000000000002 |\n|                | 2.0             |         | 3.2               |                    |\n+----------------+-----------------+---------+-------------------+--------------------+\n| cat            | 3.0             | 2       | 2.0               | 1.5620499351813311 |\n|                | 5.2             |         | 4.0               |                    |\n+----------------+-----------------+---------+-------------------+--------------------+\n| cat            | 3.0             | 5       | 5.0               | 2.0099751242241779 |\n|                | 5.2             |         | 5.4               |                    |\n*----------------+-----------------+---------+-------------------+--------------------*/\n\nThe following example searches the ` my_embedding ` column of ` table1 ` for the top two embeddings that match each row of data in the ` embedding ` column of ` table2 ` , and uses the ` COSINE ` distance type to measure the distance between the embeddings:\n\n\nSELECT *\nFROM VECTOR_SEARCH( TABLE mydataset.table1,\n'my_embedding',\nTABLE mydataset.table2,\n'embedding',\ntop_k => 2,\ndistance_type => 'COSINE');\n\n/*------  --------+-----------------+---------+-------------------------------------------+\n| query.query_id | query.embedding | base.id | base.my_embedding | distance              |\n+----------------+-----------------+---------+-------------------+-----------------------+\n| dog            | 1.0             | 2       | 2.0               | 0                     |\n|                | 2.0             |         | 4.0               |                       |\n+----------------+-----------------+---------+-------------------+-----------------------+\n| dog            | 1.0             | 1       | 1.0               | 0                     |\n|                | 2.0             |         | 2.0               |                       |\n+----------------+-----------------+---------+-------------------+-----------------------+\n| cat            | 3.0             | 2       | 2.0               | 0.0017773842088002478 |\n|                | 5.2             |         | 4.0               |                       |\n+----------------+-----------------+---------+-------------------+-----------------------+\n| cat            | 3.0             | 1       | 1.0               | 0.0017773842088002478 |\n|                | 5.2             |         | 2.0               |                       |\n*----------------+-----------------+---------+-------------------+-----------------------*/"
            }
        }
    },
    {
        "category": "security-functions",
        "description": "GoogleSQL for BigQuery supports the following security functions.",
        "source": "security_functions.txt",
        "functions": {
            "SESSION_USER": {
                "name": "SESSION_USER",
                "summary": "Get the email address or principal identifier of the user that is running the query.",
                "description": "SESSION_USER()\n\n**Description**\n\nFor first-party users, returns the email address of the user that is running the query. For third-party users, returns the [ principal identifier\n](https://cloud.google.com/iam/docs/principal-identifiers) of the user that is running the query. For more information about identities, see [ Principals\n](https://cloud.google.com/docs/authentication#principal) .\n\n**Return Data Type**\n\n` STRING `\n\n**Example**\n\n\nSELECT SESSION_USER() as user;\n\n/*----------------------*\n| user                 |\n+----------------------+\n| jdoe@example.com     |\n*----------------------*/"
            }
        }
    },
    {
        "category": "statistical-aggregate-functions",
        "description": "GoogleSQL for BigQuery supports statistical aggregate functions. To learn about the syntax for aggregate function calls, see [ Aggregate function calls\n](/bigquery/docs/reference/standard-sql/aggregate-function-calls) .",
        "source": "statistical_aggregate_functions.txt",
        "functions": {
            "CORR": {
                "name": "CORR",
                "summary": "Computes the Pearson coefficient of correlation of a set of number pairs.",
                "description": "CORR( X1, X2 )\n[ OVER over_clause ]\n\nover_clause:\n{ named_window | ( [ window_specification ] ) }\n\nwindow_specification:\n[ named_window ]\n[ PARTITION BY partition_expression [, ...] ]\n[ ORDER BY expression [ { ASC | DESC }  ] [, ...] ]\n[ window_frame_clause ]\n\n\n**Description**\n\nReturns the [ Pearson coefficient\n](https://en.wikipedia.org/wiki/Pearson_product-\nmoment_correlation_coefficient) of correlation of a set of number pairs. For each number pair, the first number is the dependent variable and the second number is the independent variable. The return result is between ` -1 ` and `\n1 ` . A result of ` 0 ` indicates no correlation.\n\nAll numeric types are supported. If the input is ` NUMERIC ` or ` BIGNUMERIC `\nthen the internal aggregation is stable with the final output converted to a `\nFLOAT64 ` . Otherwise the input is converted to a ` FLOAT64 ` before aggregation, resulting in a potentially unstable result.\n\nThis function ignores any input pairs that contain one or more ` NULL `\nvalues. If there are fewer than two input pairs without ` NULL ` values, this function returns ` NULL ` .\n\n` NaN ` is produced if:\n\n* Any input value is ` NaN `\n* Any input value is positive infinity or negative infinity.\n* The variance of ` X1 ` or ` X2 ` is ` 0 ` .\n* The covariance of ` X1 ` and ` X2 ` is ` 0 ` .\n\nTo learn more about the optional aggregate clauses that you can pass into this function, see [ Aggregate function calls ](/bigquery/docs/reference/standard-\nsql/aggregate-function-calls) .\n\nTo learn more about the ` OVER ` clause and how to use it, see [ Window function calls ](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n**Return Data Type**\n\n` FLOAT64 `\n\n**Examples**\n\n\nSELECT CORR(y, x) AS results FROM UNNEST(\n[\nSTRUCT(1.0 AS y, 5.0 AS x),\n(3.0, 9.0),\n(4.0, 7.0)]);\n\n/*--------------------*\n| results            |\n+--------------------+\n| 0.6546536707079772 |\n*--------------------*/\n\n\nSELECT CORR(y, x) AS results FROM UNNEST(\n[\nSTRUCT(1.0 AS y, 5.0 AS x),\n(3.0, 9.0),\n(4.0, NULL)]);\n\n/*---------*\n| results |\n+---------+\n| 1       |\n*---------*/\n\n\nSELECT CORR(y, x) AS results FROM UNNEST([STRUCT(1.0 AS y, NULL AS x),(9.0, 3.0)])\n\n/*---------*\n| results |\n+---------+\n| NULL    |\n*---------*/\n\n\nSELECT CORR(y, x) AS results FROM UNNEST([STRUCT(1.0 AS y, NULL AS x),(9.0, NULL)])\n\n/*---------*\n| results |\n+---------+\n| NULL    |\n*---------*/\n\n\nSELECT CORR(y, x) AS results FROM UNNEST(\n[\nSTRUCT(1.0 AS y, 5.0 AS x),\n(3.0, 9.0),\n(4.0, 7.0),\n(5.0, 1.0),\n(7.0, CAST('Infinity' as FLOAT64))])\n\n/*---------*\n| results |\n+---------+\n| NaN     |\n*---------*/\n\n\nSELECT CORR(x, y) AS results FROM ( SELECT 0 AS x, 0 AS y UNION ALL SELECT 0 AS x, 0 AS y )\n\n/*---------*\n| results |\n+---------+\n| NaN     |\n*---------*/"
            },
            "COVAR_POP": {
                "name": "COVAR_POP",
                "summary": "Computes the population covariance of a set of number pairs.",
                "description": "COVAR_POP( X1, X2 )\n[ OVER over_clause ]\n\nover_clause:\n{ named_window | ( [ window_specification ] ) }\n\nwindow_specification:\n[ named_window ]\n[ PARTITION BY partition_expression [, ...] ]\n[ ORDER BY expression [ { ASC | DESC }  ] [, ...] ]\n[ window_frame_clause ]\n\n\n**Description**\n\nReturns the population [ covariance\n](https://en.wikipedia.org/wiki/Covariance) of a set of number pairs. The first number is the dependent variable; the second number is the independent variable. The return result is between ` -Inf ` and ` +Inf ` .\n\nAll numeric types are supported. If the input is ` NUMERIC ` or ` BIGNUMERIC `\nthen the internal aggregation is stable with the final output converted to a `\nFLOAT64 ` . Otherwise the input is converted to a ` FLOAT64 ` before aggregation, resulting in a potentially unstable result.\n\nThis function ignores any input pairs that contain one or more ` NULL `\nvalues. If there is no input pair without ` NULL ` values, this function returns ` NULL ` . If there is exactly one input pair without ` NULL ` values,\nthis function returns ` 0 ` .\n\n` NaN ` is produced if:\n\n* Any input value is ` NaN `\n* Any input value is positive infinity or negative infinity.\n\nTo learn more about the optional aggregate clauses that you can pass into this function, see [ Aggregate function calls ](/bigquery/docs/reference/standard-\nsql/aggregate-function-calls) .\n\nThis function can be used with the [ ` AGGREGATION_THRESHOLD ` clause\n](/bigquery/docs/reference/standard-sql/query-syntax#agg_threshold_clause) .\n\nTo learn more about the ` OVER ` clause and how to use it, see [ Window function calls ](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n**Return Data Type**\n\n` FLOAT64 `\n\n**Examples**\n\n\nSELECT COVAR_POP(y, x) AS results FROM UNNEST(\n[\nSTRUCT(1.0 AS y, 1.0 AS x),\n(2.0, 6.0),\n(9.0, 3.0),\n(2.0, 6.0),\n(9.0, 3.0)])\n\n/*---------------------*\n| results             |\n+---------------------+\n| -1.6800000000000002 |\n*---------------------*/\n\n\nSELECT COVAR_POP(y, x) AS results FROM UNNEST([STRUCT(1.0 AS y, NULL AS x),(9.0, 3.0)])\n\n/*---------*\n| results |\n+---------+\n| 0       |\n*---------*/\n\n\nSELECT COVAR_POP(y, x) AS results FROM UNNEST([STRUCT(1.0 AS y, NULL AS x),(9.0, NULL)])\n\n/*---------*\n| results |\n+---------+\n| NULL    |\n*---------*/\n\n\nSELECT COVAR_POP(y, x) AS results FROM UNNEST(\n[\nSTRUCT(1.0 AS y, 1.0 AS x),\n(2.0, 6.0),\n(9.0, 3.0),\n(2.0, 6.0),\n(NULL, 3.0)])\n\n/*---------*\n| results |\n+---------+\n| -1      |\n*---------*/\n\n\nSELECT COVAR_POP(y, x) AS results FROM UNNEST(\n[\nSTRUCT(1.0 AS y, 1.0 AS x),\n(2.0, 6.0),\n(9.0, 3.0),\n(2.0, 6.0),\n(CAST('Infinity' as FLOAT64), 3.0)])\n\n/*---------*\n| results |\n+---------+\n| NaN     |\n*---------*/"
            },
            "COVAR_SAMP": {
                "name": "COVAR_SAMP",
                "summary": "Computes the sample covariance of a set of number pairs.",
                "description": "COVAR_SAMP( X1, X2 )\n[ OVER over_clause ]\n\nover_clause:\n{ named_window | ( [ window_specification ] ) }\n\nwindow_specification:\n[ named_window ]\n[ PARTITION BY partition_expression [, ...] ]\n[ ORDER BY expression [ { ASC | DESC }  ] [, ...] ]\n[ window_frame_clause ]\n\n\n**Description**\n\nReturns the sample [ covariance ](https://en.wikipedia.org/wiki/Covariance) of a set of number pairs. The first number is the dependent variable; the second number is the independent variable. The return result is between ` -Inf ` and\n` +Inf ` .\n\nAll numeric types are supported. If the input is ` NUMERIC ` or ` BIGNUMERIC `\nthen the internal aggregation is stable with the final output converted to a `\nFLOAT64 ` . Otherwise the input is converted to a ` FLOAT64 ` before aggregation, resulting in a potentially unstable result.\n\nThis function ignores any input pairs that contain one or more ` NULL `\nvalues. If there are fewer than two input pairs without ` NULL ` values, this function returns ` NULL ` .\n\n` NaN ` is produced if:\n\n* Any input value is ` NaN `\n* Any input value is positive infinity or negative infinity.\n\nTo learn more about the optional aggregate clauses that you can pass into this function, see [ Aggregate function calls ](/bigquery/docs/reference/standard-\nsql/aggregate-function-calls) .\n\nThis function can be used with the [ ` AGGREGATION_THRESHOLD ` clause\n](/bigquery/docs/reference/standard-sql/query-syntax#agg_threshold_clause) .\n\nTo learn more about the ` OVER ` clause and how to use it, see [ Window function calls ](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n**Return Data Type**\n\n` FLOAT64 `\n\n**Examples**\n\n\nSELECT COVAR_SAMP(y, x) AS results FROM UNNEST(\n[\nSTRUCT(1.0 AS y, 1.0 AS x),\n(2.0, 6.0),\n(9.0, 3.0),\n(2.0, 6.0),\n(9.0, 3.0)])\n\n/*---------*\n| results |\n+---------+\n| -2.1    |\n*---------*/\n\n\nSELECT COVAR_SAMP(y, x) AS results FROM UNNEST(\n[\nSTRUCT(1.0 AS y, 1.0 AS x),\n(2.0, 6.0),\n(9.0, 3.0),\n(2.0, 6.0),\n(NULL, 3.0)])\n\n/*----------------------*\n| results              |\n+----------------------+\n| --1.3333333333333333 |\n*----------------------*/\n\n\nSELECT COVAR_SAMP(y, x) AS results FROM UNNEST([STRUCT(1.0 AS y, NULL AS x),(9.0, 3.0)])\n\n/*---------*\n| results |\n+---------+\n| NULL    |\n*---------*/\n\n\nSELECT COVAR_SAMP(y, x) AS results FROM UNNEST([STRUCT(1.0 AS y, NULL AS x),(9.0, NULL)])\n\n/*---------*\n| results |\n+---------+\n| NULL    |\n*---------*/\n\n\nSELECT COVAR_SAMP(y, x) AS results FROM UNNEST(\n[\nSTRUCT(1.0 AS y, 1.0 AS x),\n(2.0, 6.0),\n(9.0, 3.0),\n(2.0, 6.0),\n(CAST('Infinity' as FLOAT64), 3.0)])\n\n/*---------*\n| results |\n+---------+\n| NaN     |\n*---------*/"
            },
            "STDDEV": {
                "name": "STDDEV",
                "summary": "An alias of the ` STDDEV_SAMP ` function.",
                "description": "STDDEV(\n[ DISTINCT ]\nexpression )\n[ OVER over_clause ]\n\nover_clause:\n{ named_window | ( [ window_specification ] ) }\n\nwindow_specification:\n[ named_window ]\n[ PARTITION BY partition_expression [, ...] ]\n[ ORDER BY expression [ { ASC | DESC }  ] [, ...] ]\n[ window_frame_clause ]\n\n\n**Description**\n\nAn alias of  STDDEV_SAMP  ."
            },
            "STDDEV_POP": {
                "name": "STDDEV_POP",
                "summary": "Computes the population (biased) standard deviation of the values.",
                "description": "STDDEV_POP(\n[ DISTINCT ]\nexpression )\n[ OVER over_clause ]\n\nover_clause:\n{ named_window | ( [ window_specification ] ) }\n\nwindow_specification:\n[ named_window ]\n[ PARTITION BY partition_expression [, ...] ]\n[ ORDER BY expression [ { ASC | DESC }  ] [, ...] ]\n[ window_frame_clause ]\n\n\n**Description**\n\nReturns the population (biased) standard deviation of the values. The return result is between ` 0 ` and ` +Inf ` .\n\nAll numeric types are supported. If the input is ` NUMERIC ` or ` BIGNUMERIC `\nthen the internal aggregation is stable with the final output converted to a `\nFLOAT64 ` . Otherwise the input is converted to a ` FLOAT64 ` before aggregation, resulting in a potentially unstable result.\n\nThis function ignores any ` NULL ` inputs. If all inputs are ignored, this function returns ` NULL ` . If this function receives a single non- ` NULL `\ninput, it returns ` 0 ` .\n\n` NaN ` is produced if:\n\n* Any input value is ` NaN `\n* Any input value is positive infinity or negative infinity.\n\nTo learn more about the optional aggregate clauses that you can pass into this function, see [ Aggregate function calls ](/bigquery/docs/reference/standard-\nsql/aggregate-function-calls) .\n\nThis function can be used with the [ ` AGGREGATION_THRESHOLD ` clause\n](/bigquery/docs/reference/standard-sql/query-syntax#agg_threshold_clause) .\n\nIf this function is used with the ` OVER ` clause, it's part of a window function call. In a window function call, aggregate function clauses can't be used. To learn more about the ` OVER ` clause and how to use it, see [ Window function calls ](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n**Return Data Type**\n\n` FLOAT64 `\n\n**Examples**\n\n\nSELECT STDDEV_POP(x) AS results FROM UNNEST([10, 14, 18]) AS x\n\n/*-------------------*\n| results           |\n+-------------------+\n| 3.265986323710904 |\n*-------------------*/\n\n\nSELECT STDDEV_POP(x) AS results FROM UNNEST([10, 14, NULL]) AS x\n\n/*---------*\n| results |\n+---------+\n| 2       |\n*---------*/\n\n\nSELECT STDDEV_POP(x) AS results FROM UNNEST([10, NULL]) AS x\n\n/*---------*\n| results |\n+---------+\n| 0       |\n*---------*/\n\n\nSELECT STDDEV_POP(x) AS results FROM UNNEST([NULL]) AS x\n\n/*---------*\n| results |\n+---------+\n| NULL    |\n*---------*/\n\n\nSELECT STDDEV_POP(x) AS results FROM UNNEST([10, 14, CAST('Infinity' as FLOAT64)]) AS x\n\n/*---------*\n| results |\n+---------+\n| NaN     |\n*---------*/"
            },
            "STDDEV_SAMP": {
                "name": "STDDEV_SAMP",
                "summary": "Computes the sample (unbiased) standard deviation of the values.",
                "description": "STDDEV_SAMP(\n[ DISTINCT ]\nexpression )\n[ OVER over_clause ]\n\nover_clause:\n{ named_window | ( [ window_specification ] ) }\n\nwindow_specification:\n[ named_window ]\n[ PARTITION BY partition_expression [, ...] ]\n[ ORDER BY expression [ { ASC | DESC }  ] [, ...] ]\n[ window_frame_clause ]\n\n\n**Description**\n\nReturns the sample (unbiased) standard deviation of the values. The return result is between ` 0 ` and ` +Inf ` .\n\nAll numeric types are supported. If the input is ` NUMERIC ` or ` BIGNUMERIC `\nthen the internal aggregation is stable with the final output converted to a `\nFLOAT64 ` . Otherwise the input is converted to a ` FLOAT64 ` before aggregation, resulting in a potentially unstable result.\n\nThis function ignores any ` NULL ` inputs. If there are fewer than two non- `\nNULL ` inputs, this function returns ` NULL ` .\n\n` NaN ` is produced if:\n\n* Any input value is ` NaN `\n* Any input value is positive infinity or negative infinity.\n\nTo learn more about the optional aggregate clauses that you can pass into this function, see [ Aggregate function calls ](/bigquery/docs/reference/standard-\nsql/aggregate-function-calls) .\n\nThis function can be used with the [ ` AGGREGATION_THRESHOLD ` clause\n](/bigquery/docs/reference/standard-sql/query-syntax#agg_threshold_clause) .\n\nIf this function is used with the ` OVER ` clause, it's part of a window function call. In a window function call, aggregate function clauses can't be used. To learn more about the ` OVER ` clause and how to use it, see [ Window function calls ](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n**Return Data Type**\n\n` FLOAT64 `\n\n**Examples**\n\n\nSELECT STDDEV_SAMP(x) AS results FROM UNNEST([10, 14, 18]) AS x\n\n/*---------*\n| results |\n+---------+\n| 4       |\n*---------*/\n\n\nSELECT STDDEV_SAMP(x) AS results FROM UNNEST([10, 14, NULL]) AS x\n\n/*--------------------*\n| results            |\n+--------------------+\n| 2.8284271247461903 |\n*--------------------*/\n\n\nSELECT STDDEV_SAMP(x) AS results FROM UNNEST([10, NULL]) AS x\n\n/*---------*\n| results |\n+---------+\n| NULL    |\n*---------*/\n\n\nSELECT STDDEV_SAMP(x) AS results FROM UNNEST([NULL]) AS x\n\n/*---------*\n| results |\n+---------+\n| NULL    |\n*---------*/\n\n\nSELECT STDDEV_SAMP(x) AS results FROM UNNEST([10, 14, CAST('Infinity' as FLOAT64)]) AS x\n\n/*---------*\n| results |\n+---------+\n| NaN     |\n*---------*/"
            },
            "VAR_POP": {
                "name": "VAR_POP",
                "summary": "Computes the population (biased) variance of the values.",
                "description": "VAR_POP(\n[ DISTINCT ]\nexpression )\n[ OVER over_clause ]\n\nover_clause:\n{ named_window | ( [ window_specification ] ) }\n\nwindow_specification:\n[ named_window ]\n[ PARTITION BY partition_expression [, ...] ]\n[ ORDER BY expression [ { ASC | DESC }  ] [, ...] ]\n[ window_frame_clause ]\n\n\n**Description**\n\nReturns the population (biased) variance of the values. The return result is between ` 0 ` and ` +Inf ` .\n\nAll numeric types are supported. If the input is ` NUMERIC ` or ` BIGNUMERIC `\nthen the internal aggregation is stable with the final output converted to a `\nFLOAT64 ` . Otherwise the input is converted to a ` FLOAT64 ` before aggregation, resulting in a potentially unstable result.\n\nThis function ignores any ` NULL ` inputs. If all inputs are ignored, this function returns ` NULL ` . If this function receives a single non- ` NULL `\ninput, it returns ` 0 ` .\n\n` NaN ` is produced if:\n\n* Any input value is ` NaN `\n* Any input value is positive infinity or negative infinity.\n\nIf this function is used with the ` OVER ` clause, it's part of a window function call. In a window function call, aggregate function clauses can't be used. To learn more about the ` OVER ` clause and how to use it, see [ Window function calls ](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n**Return Data Type**\n\n` FLOAT64 `\n\n**Examples**\n\n\nSELECT VAR_POP(x) AS results FROM UNNEST([10, 14, 18]) AS x\n\n/*--------------------*\n| results            |\n+--------------------+\n| 10.666666666666666 |\n*--------------------*/\n\n\nSELECT VAR_POP(x) AS results FROM UNNEST([10, 14, NULL]) AS x\n\n/*----------*\n| results |\n+---------+\n| 4       |\n*---------*/\n\n\nSELECT VAR_POP(x) AS results FROM UNNEST([10, NULL]) AS x\n\n/*----------*\n| results |\n+---------+\n| 0       |\n*---------*/\n\n\nSELECT VAR_POP(x) AS results FROM UNNEST([NULL]) AS x\n\n/*---------*\n| results |\n+---------+\n| NULL    |\n*---------*/\n\n\nSELECT VAR_POP(x) AS results FROM UNNEST([10, 14, CAST('Infinity' as FLOAT64)]) AS x\n\n/*---------*\n| results |\n+---------+\n| NaN     |\n*---------*/"
            },
            "VAR_SAMP": {
                "name": "VAR_SAMP",
                "summary": "Computes the sample (unbiased) variance of the values.",
                "description": "VAR_SAMP(\n[ DISTINCT ]\nexpression )\n[ OVER over_clause ]\n\nover_clause:\n{ named_window | ( [ window_specification ] ) }\n\nwindow_specification:\n[ named_window ]\n[ PARTITION BY partition_expression [, ...] ]\n[ ORDER BY expression [ { ASC | DESC }  ] [, ...] ]\n[ window_frame_clause ]\n\n\n**Description**\n\nReturns the sample (unbiased) variance of the values. The return result is between ` 0 ` and ` +Inf ` .\n\nAll numeric types are supported. If the input is ` NUMERIC ` or ` BIGNUMERIC `\nthen the internal aggregation is stable with the final output converted to a `\nFLOAT64 ` . Otherwise the input is converted to a ` FLOAT64 ` before aggregation, resulting in a potentially unstable result.\n\nThis function ignores any ` NULL ` inputs. If there are fewer than two non- `\nNULL ` inputs, this function returns ` NULL ` .\n\n` NaN ` is produced if:\n\n* Any input value is ` NaN `\n* Any input value is positive infinity or negative infinity.\n\nTo learn more about the optional aggregate clauses that you can pass into this function, see [ Aggregate function calls ](/bigquery/docs/reference/standard-\nsql/aggregate-function-calls) .\n\nThis function can be used with the [ ` AGGREGATION_THRESHOLD ` clause\n](/bigquery/docs/reference/standard-sql/query-syntax#agg_threshold_clause) .\n\nIf this function is used with the ` OVER ` clause, it's part of a window function call. In a window function call, aggregate function clauses can't be used. To learn more about the ` OVER ` clause and how to use it, see [ Window function calls ](/bigquery/docs/reference/standard-sql/window-function-calls) .\n\n**Return Data Type**\n\n` FLOAT64 `\n\n**Examples**\n\n\nSELECT VAR_SAMP(x) AS results FROM UNNEST([10, 14, 18]) AS x\n\n/*---------*\n| results |\n+---------+\n| 16      |\n*---------*/\n\n\nSELECT VAR_SAMP(x) AS results FROM UNNEST([10, 14, NULL]) AS x\n\n/*---------*\n| results |\n+---------+\n| 8       |\n*---------*/\n\n\nSELECT VAR_SAMP(x) AS results FROM UNNEST([10, NULL]) AS x\n\n/*---------*\n| results |\n+---------+\n| NULL    |\n*---------*/\n\n\nSELECT VAR_SAMP(x) AS results FROM UNNEST([NULL]) AS x\n\n/*---------*\n| results |\n+---------+\n| NULL    |\n*---------*/\n\n\nSELECT VAR_SAMP(x) AS results FROM UNNEST([10, 14, CAST('Infinity' as FLOAT64)]) AS x\n\n/*---------*\n| results |\n+---------+\n| NaN     |\n*---------*/"
            },
            "VARIANCE": {
                "name": "VARIANCE",
                "summary": "An alias of ` VAR_SAMP ` .",
                "description": "VARIANCE(\n[ DISTINCT ]\nexpression )\n[ OVER over_clause ]\n\nover_clause:\n{ named_window | ( [ window_specification ] ) }\n\nwindow_specification:\n[ named_window ]\n[ PARTITION BY partition_expression [, ...] ]\n[ ORDER BY expression [ { ASC | DESC }  ] [, ...] ]\n[ window_frame_clause ]\n\n\n**Description**\n\nAn alias of  VAR_SAMP  ."
            }
        }
    },
    {
        "category": "string-functions",
        "description": "GoogleSQL for BigQuery supports string functions. These string functions work on two different values: ` STRING ` and ` BYTES ` data types. ` STRING `\nvalues must be well-formed UTF-8.\n\nFunctions that return position values, such as  STRPOS  , encode those positions as ` INT64 ` . The value ` 1 ` refers to the first character (or byte), ` 2 ` refers to the second, and so on. The value ` 0 ` indicates an invalid position. When working on ` STRING ` types, the returned positions refer to character positions.\n\nAll string comparisons are done byte-by-byte, without regard to Unicode canonical equivalence.",
        "source": "string_functions.txt",
        "functions": {
            "ASCII": {
                "name": "ASCII",
                "summary": "Gets the ASCII code for the first character or byte in a ` STRING\n` or ` BYTES ` value.",
                "description": "ASCII(value)\n\n**Description**\n\nReturns the ASCII code for the first character or byte in ` value ` . Returns\n` 0 ` if ` value ` is empty or the ASCII code is ` 0 ` for the first character or byte.\n\n**Return type**\n\n` INT64 `\n\n**Examples**\n\n\nSELECT ASCII('abcd') as A, ASCII('a') as B, ASCII('') as C, ASCII(NULL) as D;\n\n/*-------+-------+-------+-------*\n| A     | B     | C     | D     |\n+-------+-------+-------+-------+\n| 97    | 97    | 0     | NULL  |\n*-------+-------+-------+-------*/"
            },
            "BYTE_LENGTH": {
                "name": "BYTE_LENGTH",
                "summary": "Gets the number of ` BYTES ` in a ` STRING ` or ` BYTES `\nvalue.",
                "description": "BYTE_LENGTH(value)\n\n**Description**\n\nGets the number of ` BYTES ` in a ` STRING ` or ` BYTES ` value, regardless of whether the value is a ` STRING ` or ` BYTES ` type.\n\n**Return type**\n\n` INT64 `\n\n**Examples**\n\n\nWITH example AS (SELECT '\u0430\u0431\u0432\u0433\u0434' AS characters, b'\u0430\u0431\u0432\u0433\u0434' AS bytes)\n\nSELECT characters,\nBYTE_LENGTH(characters) AS string_example,\nbytes,\nBYTE_LENGTH(bytes) AS bytes_example FROM example;\n\n/*------------+----------------+-------+---------------*\n| characters | string_example | bytes | bytes_example |\n+------------+----------------+-------+---------------+\n| \u0430\u0431\u0432\u0433\u0434      | 10             | \u0430\u0431\u0432\u0433\u0434 | 10            |\n*------------+----------------+-------+---------------*/"
            },
            "CHAR_LENGTH": {
                "name": "CHAR_LENGTH",
                "summary": "Gets the number of characters in a ` STRING ` value.",
                "description": "CHAR_LENGTH(value)\n\n**Description**\n\nGets the number of characters in a ` STRING ` value.\n\n**Return type**\n\n` INT64 `\n\n**Examples**\n\n\nWITH example AS (SELECT '\u0430\u0431\u0432\u0433\u0434' AS characters)\n\nSELECT characters,\nCHAR_LENGTH(characters) AS char_length_example FROM example;\n\n/*------------+---------------------*\n| characters | char_length_example |\n+------------+---------------------+\n| \u0430\u0431\u0432\u0433\u0434      |                   5 |\n*------------+---------------------*/"
            },
            "CHARACTER_LENGTH": {
                "name": "CHARACTER_LENGTH",
                "summary": "Synonym for ` CHAR_LENGTH ` .",
                "description": "CHARACTER_LENGTH(value)\n\n**Description**\n\nSynonym for  CHAR_LENGTH  .\n\n**Return type**\n\n` INT64 `\n\n**Examples**\n\n\nWITH example AS (SELECT '\u0430\u0431\u0432\u0433\u0434' AS characters)\n\nSELECT characters,\nCHARACTER_LENGTH(characters) AS char_length_example FROM example;\n\n/*------------+---------------------*\n| characters | char_length_example |\n+------------+---------------------+\n| \u0430\u0431\u0432\u0433\u0434      |                   5 |\n*------------+---------------------*/"
            },
            "CHR": {
                "name": "CHR",
                "summary": "Converts a Unicode code point to a character.",
                "description": "CHR(value)\n\n**Description**\n\nTakes a Unicode [ code point ](https://en.wikipedia.org/wiki/Code_point) and returns the character that matches the code point. Each valid code point should fall within the range of [0, 0xD7FF] and [0xE000, 0x10FFFF]. Returns an empty string if the code point is ` 0 ` . If an invalid Unicode code point is specified, an error is returned.\n\nTo work with an array of Unicode code points, see  ` CODE_POINTS_TO_STRING `\n\n**Return type**\n\n` STRING `\n\n**Examples**\n\n\nSELECT CHR(65) AS A, CHR(255) AS B, CHR(513) AS C, CHR(1024)  AS D;\n\n/*-------+-------+-------+-------*\n| A     | B     | C     | D     |\n+-------+-------+-------+-------+\n| A     | \u00ff     | \u0201     | \u0400     |\n*-------+-------+-------+-------*/\n\n\nSELECT CHR(97) AS A, CHR(0xF9B5) AS B, CHR(0) AS C, CHR(NULL) AS D;\n\n/*-------+-------+-------+-------*\n| A     | B     | C     | D     |\n+-------+-------+-------+-------+\n| a     | \uf9b5    |       | NULL  |\n*-------+-------+-------+-------*/"
            },
            "CODE_POINTS_TO_BYTES": {
                "name": "CODE_POINTS_TO_BYTES",
                "summary": "Converts an array of extended ASCII code points to a ` BYTES ` value.",
                "description": "CODE_POINTS_TO_BYTES(ascii_code_points)\n\n**Description**\n\nTakes an array of extended ASCII [ code points\n](https://en.wikipedia.org/wiki/Code_point) as ` ARRAY<INT64> ` and returns `\nBYTES ` .\n\nTo convert from ` BYTES ` to an array of code points, see  TO_CODE_POINTS  .\n\n**Return type**\n\n` BYTES `\n\n**Examples**\n\nThe following is a basic example using ` CODE_POINTS_TO_BYTES ` .\n\n\nSELECT CODE_POINTS_TO_BYTES([65, 98, 67, 100]) AS bytes;\n\n-- Note that the result of CODE_POINTS_TO_BYTES is of type BYTES, displayed as a base64-encoded string.\n-- In BYTES format, b'AbCd' is the result.\n/*----------*\n| bytes    |\n+----------+\n| QWJDZA== |\n*----------*/\n\nThe following example uses a rotate-by-13 places (ROT13) algorithm to encode a string.\n\n\nSELECT CODE_POINTS_TO_BYTES(ARRAY_AGG( (SELECT CASE WHEN chr BETWEEN b'a' and b'z'\nTHEN TO_CODE_POINTS(b'a')[offset(0)] +\nMOD(code+13-TO_CODE_POINTS(b'a')[offset(0)],26) WHEN chr BETWEEN b'A' and b'Z'\nTHEN TO_CODE_POINTS(b'A')[offset(0)] +\nMOD(code+13-TO_CODE_POINTS(b'A')[offset(0)],26) ELSE code END FROM (SELECT code, CODE_POINTS_TO_BYTES([code]) chr) ) ORDER BY OFFSET)) AS encoded_string FROM UNNEST(TO_CODE_POINTS(b'Test String!')) code WITH OFFSET;\n\n-- Note that the result of CODE_POINTS_TO_BYTES is of type BYTES, displayed as a base64-encoded string.\n-- In BYTES format, b'Grfg Fgevat!' is the result.\n/*------------------*\n| encoded_string   |\n+------------------+\n| R3JmZyBGZ2V2YXQh |\n*------------------*/"
            },
            "CODE_POINTS_TO_STRING": {
                "name": "CODE_POINTS_TO_STRING",
                "summary": "Converts an array of extended ASCII code points to a ` STRING ` value.",
                "description": "CODE_POINTS_TO_STRING(unicode_code_points)\n\n**Description**\n\nTakes an array of Unicode [ code points\n](https://en.wikipedia.org/wiki/Code_point) as ` ARRAY<INT64> ` and returns a\n` STRING ` .\n\nTo convert from a string to an array of code points, see  TO_CODE_POINTS  .\n\n**Return type**\n\n` STRING `\n\n**Examples**\n\nThe following are basic examples using ` CODE_POINTS_TO_STRING ` .\n\n\nSELECT CODE_POINTS_TO_STRING([65, 255, 513, 1024]) AS string;\n\n/*--------*\n| string |\n+--------+\n| A\u00ff\u0201\u0400   |\n*--------*/\n\n\nSELECT CODE_POINTS_TO_STRING([97, 0, 0xF9B5]) AS string;\n\n/*--------*\n| string |\n+--------+\n| a\uf9b5    |\n*--------*/\n\n\nSELECT CODE_POINTS_TO_STRING([65, 255, NULL, 1024]) AS string;\n\n/*--------*\n| string |\n+--------+\n| NULL   |\n*--------*/\n\nThe following example computes the frequency of letters in a set of words.\n\n\nWITH Words AS ( SELECT word FROM UNNEST(['foo', 'bar', 'baz', 'giraffe', 'llama']) AS word ) SELECT CODE_POINTS_TO_STRING([code_point]) AS letter,\nCOUNT(*) AS letter_count FROM Words,\nUNNEST(TO_CODE_POINTS(word)) AS code_point GROUP BY 1 ORDER BY 2 DESC;\n\n/*--------+--------------*\n| letter | letter_count |\n+--------+--------------+\n| a      | 5            |\n| f      | 3            |\n| r      | 2            |\n| b      | 2            |\n| l      | 2            |\n| o      | 2            |\n| g      | 1            |\n| z      | 1            |\n| e      | 1            |\n| m      | 1            |\n| i      | 1            |\n*--------+--------------*/"
            },
            "COLLATE": {
                "name": "COLLATE",
                "summary": "Combines a ` STRING ` value and a collation specification into a collation specification-supported ` STRING ` value.",
                "description": "COLLATE(value, collate_specification)\n\nTakes a ` STRING ` and a [ collation specification\n](/bigquery/docs/reference/standard-sql/collation-\nconcepts#collate_spec_details) . Returns a ` STRING ` with a collation specification. If ` collate_specification ` is empty, returns a value with collation removed from the ` STRING ` .\n\nThe collation specification defines how the resulting ` STRING ` can be compared and sorted. To learn more, see [ Working with collation\n](/bigquery/docs/reference/standard-sql/collation-\nconcepts#working_with_collation) .\n\n* ` collation_specification ` must be a string literal, otherwise an error is thrown.\n* Returns ` NULL ` if ` value ` is ` NULL ` .\n\n**Return type**\n\n` STRING `\n\n**Examples**\n\nIn this example, the weight of ` a ` is less than the weight of ` Z ` . This is because the collate specification, ` und:ci ` assigns more weight to ` Z `\n.\n\n\nWITH Words AS ( SELECT COLLATE('a', 'und:ci') AS char1,\nCOLLATE('Z', 'und:ci') AS char2 ) SELECT ( Words.char1 < Words.char2 ) AS a_less_than_Z FROM Words;\n\n/*----------------*\n| a_less_than_Z  |\n+----------------+\n| TRUE           |\n*----------------*/\n\nIn this example, the weight of ` a ` is greater than the weight of ` Z ` .\nThis is because the default collate specification assigns more weight to ` a `\n.\n\n\nWITH Words AS ( SELECT\n'a' AS char1,\n'Z' AS char2 ) SELECT ( Words.char1 < Words.char2 ) AS a_less_than_Z FROM Words;\n\n/*----------------*\n| a_less_than_Z  |\n+----------------+\n| FALSE          |\n*----------------*/"
            },
            "CONCAT": {
                "name": "CONCAT",
                "summary": "Concatenates one or more ` STRING ` or ` BYTES ` values into a single result.",
                "description": "CONCAT(value1[, ...])\n\n**Description**\n\nConcatenates one or more values into a single result. All values must be `\nBYTES ` or data types that can be cast to ` STRING ` .\n\nThe function returns ` NULL ` if any input argument is ` NULL ` .\n\n**Note:** You can also use the [ || concatenation operator\n](/bigquery/docs/reference/standard-sql/operators) to concatenate values into a string.\n\n**Return type**\n\n` STRING ` or ` BYTES `\n\n**Examples**\n\n\nSELECT CONCAT('T.P.', ' ', 'Bar') as author;\n\n/*---------------------*\n| author              |\n+---------------------+\n| T.P. Bar            |\n*---------------------*/\n\n\nSELECT CONCAT('Summer', ' ', 1923) as release_date;\n\n/*---------------------*\n| release_date        |\n+---------------------+\n| Summer 1923         |\n*---------------------*/\n\n\nWith Employees AS (SELECT\n'John' AS first_name,\n'Doe' AS last_name UNION ALL SELECT\n'Jane' AS first_name,\n'Smith' AS last_name UNION ALL SELECT\n'Joe' AS first_name,\n'Jackson' AS last_name)\n\nSELECT CONCAT(first_name, ' ', last_name) AS full_name FROM Employees;\n\n/*---------------------*\n| full_name           |\n+---------------------+\n| John Doe            |\n| Jane Smith          |\n| Joe Jackson         |\n*---------------------*/"
            },
            "CONTAINS_SUBSTR": {
                "name": "CONTAINS_SUBSTR",
                "summary": "Performs a normalized, case-insensitive search to see if a value exists as a substring in an expression.",
                "description": "CONTAINS_SUBSTR(expression, search_value_literal[, json_scope=>json_scope_value])\n\njson_scope_value:\n{ 'JSON_VALUES' | 'JSON_KEYS' | 'JSON_KEYS_AND_VALUES' }\n\n**Description**\n\nPerforms a normalized, case-insensitive search to see if a value exists as a substring in an expression. Returns ` TRUE ` if the value exists, otherwise returns ` FALSE ` .\n\nBefore values are compared, they are  normalized and case folded with ` NFKC `\nnormalization  . Wildcard searches are not supported.\n\n**Arguments**\n\n* ` search_value_literal ` : The value to search for. It must be a ` STRING ` literal or a ` STRING ` constant expression.\n* ` expression ` : The data to search over. The expression can be a column or table reference. A table reference is evaluated as a ` STRUCT ` whose fields are the columns of the table. A column reference is evaluated as one the following data types:\n\n* ` STRING `\n* ` INT64 `\n* ` BOOL `\n* ` NUMERIC `\n* ` BIGNUMERIC `\n* ` TIMESTAMP `\n* ` TIME `\n* ` DATE `\n* ` DATETIME `\n* ` ARRAY `\n* ` STRUCT `\n\nWhen the expression is evaluated, the result is cast to a ` STRING ` , and then the function looks for the search value in the result.\n\nYou can perform a cross-field search on an expression that evaluates to a `\nSTRUCT ` or ` ARRAY ` . If the expression evaluates to a ` STRUCT ` , the cross-field search is recursive and includes all subfields inside the ` STRUCT\n` .\n\nIn a cross-field search, each field and subfield is individually converted to a string and searched for the value. The function returns ` TRUE ` if at least one field includes the search value; otherwise, if at least one field is `\nNULL ` , it returns ` NULL ` ; otherwise, if the search value is not found and all fields are non- ` NULL ` , it returns ` FALSE ` .\n\nIf the expression is ` NULL ` , the return value is ` NULL ` .\n\n* ` json_scope ` : This optional [ mandatory-named argument ](/bigquery/docs/reference/standard-sql/functions-reference#named_arguments) takes one of the following values to indicate the scope of ` JSON ` data to be searched. It has no effect if ` expression ` is not ` JSON ` or does not contain a ` JSON ` field.\n\n* ` 'JSON_VALUES' ` : Only the ` JSON ` values are searched. If ` json_scope ` is not provided, this is used by default.\n* ` 'JSON_KEYS' ` : Only the ` JSON ` keys are searched.\n* ` 'JSON_KEYS_AND_VALUES' ` : The ` JSON ` keys and values are searched.\n\n**Return type**\n\n` BOOL `\n\n**Examples**\n\nThe following query returns ` TRUE ` because this case-insensitive match was found: ` blue house ` and ` Blue house ` .\n\n\nSELECT CONTAINS_SUBSTR('the blue house', 'Blue house') AS result;\n\n/*--------*\n| result |\n+--------+\n| true   |\n*--------*/\n\nThe following query returns ` TRUE ` similar to the above example, but in this case the search value is a constant expression with CONCAT function.\n\n\nSELECT CONTAINS_SUBSTR('the blue house', CONCAT('Blue ', 'house')) AS result;\n\n/*--------*\n| result |\n+--------+\n| true   |\n*--------*/\n\nThe following query returns ` FALSE ` because ` blue ` was not found in ` the red house ` .\n\n\nSELECT CONTAINS_SUBSTR('the red house', 'blue') AS result;\n\n/*--------*\n| result |\n+--------+\n| false  |\n*--------*/\n\nThe following query returns ` TRUE ` because ` \u2168 ` and ` IX ` represent the same normalized value.\n\n\nSELECT '\\u2168 day' AS a, 'IX' AS b, CONTAINS_SUBSTR('\\u2168', 'IX') AS result;\n\n/*----------------------*\n| a      | b  | result |\n+----------------------+\n| \u2168 day | IX | true   |\n*----------------------*/\n\nThe following query returns ` TRUE ` because ` 35 ` was found inside a `\nSTRUCT ` field.\n\n\nSELECT CONTAINS_SUBSTR((23, 35, 41), '35') AS result;\n\n/*--------*\n| result |\n+--------+\n| true   |\n*--------*/\n\nThe following query returns ` TRUE ` because ` jk ` was found during a recursive search inside a ` STRUCT ` .\n\n\nSELECT CONTAINS_SUBSTR(('abc', ['def', 'ghi', 'jkl'], 'mno'), 'jk');\n\n/*--------*\n| result |\n+--------+\n| true   |\n*--------*/\n\nThe following query returns ` TRUE ` because ` NULL ` s are ignored when a match is found found inside a ` STRUCT ` or ` ARRAY ` .\n\n\nSELECT CONTAINS_SUBSTR((23, NULL, 41), '41') AS result;\n\n/*--------*\n| result |\n+--------+\n| true   |\n*--------*/\n\nThe following query returns ` NULL ` because a ` NULL ` existed in a ` STRUCT\n` that did not result in a match.\n\n\nSELECT CONTAINS_SUBSTR((23, NULL, 41), '35') AS result;\n\n/*--------*\n| result |\n+--------+\n| null   |\n*--------*/\n\nIn the following query, an error is thrown because the search value cannot be a literal ` NULL ` .\n\n\nSELECT CONTAINS_SUBSTR('hello', NULL) AS result;\n-- Throws an error\n\nThe following examples reference a table called ` Recipes ` that you can emulate with a ` WITH ` clause like this:\n\n\nWITH Recipes AS (SELECT 'Blueberry pancakes' as Breakfast, 'Egg salad sandwich' as Lunch, 'Potato dumplings' as Dinner UNION ALL SELECT 'Potato pancakes', 'Toasted cheese sandwich', 'Beef stroganoff' UNION ALL SELECT 'Ham scramble', 'Steak avocado salad', 'Tomato pasta' UNION ALL SELECT 'Avocado toast', 'Tomato soup', 'Blueberry salmon' UNION ALL SELECT 'Corned beef hash', 'Lentil potato soup', 'Glazed ham') SELECT * FROM Recipes;\n\n/*-------------------+-------------------------+------------------*\n| Breakfast         | Lunch                   | Dinner           |\n+-------------------+-------------------------+------------------+\n| Bluberry pancakes | Egg salad sandwich      | Potato dumplings |\n| Potato pancakes   | Toasted cheese sandwich | Beef stroganoff  |\n| Ham scramble      | Steak avocado salad     | Tomato pasta     |\n| Avocado toast     | Tomato soup             | Blueberry samon  |\n| Corned beef hash  | Lentil potato soup      | Glazed ham       |\n*-------------------+-------------------------+------------------*/\n\nThe following query searches across all columns of the ` Recipes ` table for the value ` toast ` and returns the rows that contain this value.\n\n\nSELECT * FROM Recipes WHERE CONTAINS_SUBSTR(Recipes, 'toast');\n\n/*-------------------+-------------------------+------------------*\n| Breakfast         | Lunch                   | Dinner           |\n+-------------------+-------------------------+------------------+\n| Potato pancakes   | Toasted cheese sandwich | Beef stroganoff  |\n| Avocado toast     | Tomato soup             | Blueberry samon  |\n*-------------------+-------------------------+------------------*/\n\nThe following query searches the ` Lunch ` and ` Dinner ` columns of the `\nRecipe ` table for the value ` potato ` and returns the row if either column contains this value.\n\n\nSELECT * FROM Recipes WHERE CONTAINS_SUBSTR((Lunch, Dinner), 'potato');\n\n/*-------------------+-------------------------+------------------*\n| Breakfast         | Lunch                   | Dinner           |\n+-------------------+-------------------------+------------------+\n| Bluberry pancakes | Egg salad sandwich      | Potato dumplings |\n| Corned beef hash  | Lentil potato soup      | Glazed ham       |\n*-------------------+-------------------------+------------------*/\n\nThe following query searches across all columns of the ` Recipes ` table except for the ` Lunch ` and ` Dinner ` columns. It returns the rows of any columns other than ` Lunch ` or ` Dinner ` that contain the value ` potato ` .\n\n\nSELECT *\nFROM Recipes WHERE CONTAINS_SUBSTR( (SELECT AS STRUCT Recipes.* EXCEPT (Lunch, Dinner)),\n'potato'\n);\n\n/*-------------------+-------------------------+------------------*\n| Breakfast         | Lunch                   | Dinner           |\n+-------------------+-------------------------+------------------+\n| Potato pancakes   | Toasted cheese sandwich | Beef stroganoff  |\n*-------------------+-------------------------+------------------*/\n\nThe following query searches for the value ` lunch ` in the JSON `\n{\"lunch\":\"soup\"} ` and returns ` FALSE ` because the default ` json_scope ` is\n` \"JSON_VALUES\" ` , and ` lunch ` is a ` JSON ` key, not a ` JSON ` value.\n\n\nSELECT CONTAINS_SUBSTR(JSON '{\"lunch\":\"soup\"}',\"lunch\") AS result;\n\n/*--------*\n| result |\n+--------+\n| FALSE  |\n*--------*/\n\nThe following query searches for the value ` lunch ` in the values of the JSON\n` {\"lunch\":\"soup\"} ` and returns ` FALSE ` because ` lunch ` is a ` JSON `\nkey, not a ` JSON ` value.\n\n\nSELECT CONTAINS_SUBSTR(JSON '{\"lunch\":\"soup\"}',\n\"lunch\",\njson_scope=>\"JSON_VALUES\") AS result;\n\n/*--------*\n| result |\n+--------+\n| FALSE  |\n*--------*/\n\nThe following query searches for the value ` lunch ` in the keys and values of the JSON ` {\"lunch\":\"soup\"} ` and returns ` TRUE ` because ` lunch ` is a `\nJSON ` key.\n\n\nSELECT CONTAINS_SUBSTR(JSON '{\"lunch\":\"soup\"}',\n\"lunch\",\njson_scope=>\"JSON_KEYS_AND_VALUES\") AS result;\n\n/*--------*\n| result |\n+--------+\n| TRUE   |\n*--------*/\n\nThe following query searches for the value ` lunch ` in the keys of the JSON `\n{\"lunch\":\"soup\"} ` and returns ` TRUE ` because ` lunch ` is a ` JSON ` key.\n\n\nSELECT CONTAINS_SUBSTR(JSON '{\"lunch\":\"soup\"}',\n\"lunch\",\njson_scope=>\"JSON_KEYS\") AS result;\n\n/*--------*\n| result |\n+--------+\n| TRUE   |\n*--------*/"
            },
            "EDIT_DISTANCE": {
                "name": "EDIT_DISTANCE",
                "summary": "Computes the Levenshtein distance between two ` STRING `\nor ` BYTES ` values.",
                "description": "EDIT_DISTANCE(value1, value2, [max_distance => max_distance_value])\n\n**Description**\n\nComputes the [ Levenshtein distance\n](https://en.wikipedia.org/wiki/Levenshtein_distance) between two ` STRING `\nor ` BYTES ` values.\n\n**Definitions**\n\n* ` value1 ` : The first ` STRING ` or ` BYTES ` value to compare.\n* ` value2 ` : The second ` STRING ` or ` BYTES ` value to compare.\n* ` max_distance ` : Optional mandatory-named argument. Takes a non-negative ` INT64 ` value that represents the maximum distance between the two values to compute.\n\nIf this distance is exceeded, the function returns this value. The default value for this argument is the maximum size of ` value1 ` and ` value2 ` .\n\n**Details**\n\nIf ` value1 ` or ` value2 ` is ` NULL ` , ` NULL ` is returned.\n\nYou can only compare values of the same type. Otherwise, an error is produced.\n\n**Return type**\n\n` INT64 `\n\n**Examples**\n\nIn the following example, the first character in both strings is different:\n\n\nSELECT EDIT_DISTANCE('a', 'b') AS results;\n\n/*---------*\n| results |\n+---------+\n| 1       |\n*---------*/\n\nIn the following example, the first and second characters in both strings are different:\n\n\nSELECT EDIT_DISTANCE('aa', 'b') AS results;\n\n/*---------*\n| results |\n+---------+\n| 2       |\n*---------*/\n\nIn the following example, only the first character in both strings is different:\n\n\nSELECT EDIT_DISTANCE('aa', 'ba') AS results;\n\n/*---------*\n| results |\n+---------+\n| 1       |\n*---------*/\n\nIn the following example, the last six characters are different, but because the maximum distance is ` 2 ` , this function exits early and returns ` 2 ` ,\nthe maximum distance:\n\n\nSELECT EDIT_DISTANCE('abcdefg', 'a', max_distance => 2) AS results;\n\n/*---------*\n| results |\n+---------+\n| 2       |\n*---------*/"
            },
            "ENDS_WITH": {
                "name": "ENDS_WITH",
                "summary": "Checks if a ` STRING ` or ` BYTES ` value is the suffix of another value.",
                "description": "ENDS_WITH(value, suffix)\n\n**Description**\n\nTakes two ` STRING ` or ` BYTES ` values. Returns ` TRUE ` if ` suffix ` is a suffix of ` value ` .\n\nThis function supports specifying [ collation\n](/bigquery/docs/reference/standard-sql/collation-concepts#collate_about) .\n\n**Return type**\n\n` BOOL `\n\n**Examples**\n\n\nWITH items AS (SELECT 'apple' as item UNION ALL SELECT 'banana' as item UNION ALL SELECT 'orange' as item)\n\nSELECT ENDS_WITH(item, 'e') as example FROM items;\n\n/*---------*\n| example |\n+---------+\n|    True |\n|   False |\n|    True |\n*---------*/"
            },
            "FORMAT": {
                "name": "FORMAT",
                "summary": "Formats data and produces the results as a ` STRING ` value.",
                "description": "FORMAT(format_string_expression, data_type_expression[, ...])\n\n**Description**\n\n` FORMAT ` formats a data type expression as a string.\n\n* ` format_string_expression ` : Can contain zero or more  format specifiers  . Each format specifier is introduced by the ` % ` symbol, and must map to one or more of the remaining arguments. In general, this is a one-to-one mapping, except when the ` * ` specifier is present. For example, ` %.*i ` maps to two arguments\u2014a length argument and a signed integer argument. If the number of arguments related to the format specifiers is not the same as the number of arguments, an error occurs.\n* ` data_type_expression ` : The value to format as a string. This can be any GoogleSQL data type.\n\n**Return type**\n\n` STRING `\n\n**Examples**\n\nDescription  |  Statement  |  Result\n---|---|---\nSimple integer  |  FORMAT('%d', 10)  |  10 Integer with left blank padding  |  FORMAT('|%10d|', 11)  |  | 11|\nInteger with left zero padding  |  FORMAT('+%010d+', 12)  |  +0000000012+\nInteger with commas  |  FORMAT(\"%'d\", 123456789)  |  123,456,789 STRING  |  FORMAT('-%s-', 'abcd efg')  |  -abcd efg-\nFLOAT64  |  FORMAT('%f %E', 1.1, 2.2)  |  1.100000 2.200000E+00 DATE  |  FORMAT('%t', date '2015-09-01')  |  2015-09-01 TIMESTAMP  |  FORMAT('%t', timestamp '2015-09-01 12:34:56 America/Los_Angeles')  |  2015\u201109\u201101 19:34:56+00\n\nThe ` FORMAT() ` function does not provide fully customizable formatting for all types and values, nor formatting that is sensitive to locale.\n\nIf custom formatting is necessary for a type, you must first format it using type-specific format functions, such as ` FORMAT_DATE() ` or `\nFORMAT_TIMESTAMP() ` . For example:\n\n\nSELECT FORMAT('date: %s!', FORMAT_DATE('%B %d, %Y', date '2015-01-02'));\n\nReturns\n\n\ndate: January 02, 2015!\n\n####  Supported format specifiers\n\n\n%[flags][width][.precision]specifier\n\nA  format specifier  adds formatting when casting a value to a string. It can optionally contain these sub-specifiers:\n\n* Flags\n* Width\n* Precision\n\nAdditional information about format specifiers:\n\n* %g and %G behavior\n* %p and %P behavior\n* %t and %T behavior\n* Error conditions\n* NULL argument handling\n* Additional semantic rules\n\n#####  Format specifiers\n\nSpecifier  |  Description  |  Examples  |  Types\n---|---|---|---\n` d ` or ` i ` |  Decimal integer  |  392  |  ` INT64 `\n\n` o ` |  Octal\n\nNote: If an ` INT64 ` value is negative, an error is produced.  |  610  |  `\nINT64 `\n\n` x ` |  Hexadecimal integer\n\nNote: If an ` INT64 ` value is negative, an error is produced.  |  7fa  |  `\nINT64 `\n\n` X ` |  Hexadecimal integer (uppercase)\n\nNote: If an ` INT64 ` value is negative, an error is produced.  |  7FA  |  `\nINT64 `\n\n` f ` |  Decimal notation, in [-](integer part).(fractional part) for finite values, and in lowercase for non-finite values  |  392.650000 inf nan  |  ` NUMERIC `\n` BIGNUMERIC `\n` FLOAT64 `\n\n` F ` |  Decimal notation, in [-](integer part).(fractional part) for finite values, and in uppercase for non-finite values  |  392.650000 INF NAN  |  ` NUMERIC `\n` BIGNUMERIC `\n` FLOAT64 `\n\n` e ` |  Scientific notation (mantissa/exponent), lowercase  |  3.926500e+02 inf nan  |  ` NUMERIC `\n` BIGNUMERIC `\n` FLOAT64 `\n\n` E ` |  Scientific notation (mantissa/exponent), uppercase  |  3.926500E+02 INF NAN  |  ` NUMERIC `\n` BIGNUMERIC `\n` FLOAT64 `\n\n` g ` |  Either decimal notation or scientific notation, depending on the input value's exponent and the specified precision. Lowercase. See  %g and %G behavior  for details.  |  392.65 3.9265e+07 inf nan  |  ` NUMERIC `\n` BIGNUMERIC `\n` FLOAT64 `\n\n` G ` |  Either decimal notation or scientific notation, depending on the input value's exponent and the specified precision. Uppercase. See  %g and %G behavior  for details.  |  392.65 3.9265E+07 INF NAN  |  ` NUMERIC `\n` BIGNUMERIC `\n` FLOAT64 `\n\n` p ` |  Produces a one-line printable string representing JSON. See  %p and\n%P behavior  .  |\n\n\n{\"month\":10,\"year\":2019}\n\n|  ` JSON `\n\n` P ` |  Produces a multi-line printable string representing JSON. See  %p and\n%P behavior  .  |\n\n\n{\n\"month\": 10,\n\"year\": 2019\n}\n\n|  ` JSON `\n\n` s ` |  String of characters  |  sample  |  ` STRING `\n\n` t ` |  Returns a printable string representing the value. Often looks similar to casting the argument to ` STRING ` . See  %t and %T behavior  .  |\nsample 2014\u201101\u201101  |  Any type\n` T ` |  Produces a string that is a valid GoogleSQL constant with a similar type to the value's type (maybe wider, or maybe string). See  %t and %T behavior  .  |  'sample'\nb'bytes sample'\n1234 2.3 date '2014\u201101\u201101'  |  Any type\n` % ` |  '%%' produces a single '%'  |  %  |  n/a\n\nThe format specifier can optionally contain the sub-specifiers identified above in the specifier prototype.\n\nThese sub-specifiers must comply with the following specifications.\n\n#####  Flags\n\nFlags  |  Description\n---|---\n` - ` |  Left-justify within the given field width; Right justification is the default (see width sub-specifier)\n` + ` |  Forces to precede the result with a plus or minus sign ( ` + ` or ` -\n` ) even for positive numbers. By default, only negative numbers are preceded with a ` - ` sign\n<space> |  If no sign is going to be written, a blank space is inserted before the value\n` # ` |\n\n* For `%o`, `%x`, and `%X`, this flag means to precede the value with 0, 0x or 0X respectively for values different than zero.\n* For `%f`, `%F`, `%e`, and `%E`, this flag means to add the decimal point even when there is no fractional part, unless the value is non-finite.\n* For `%g` and `%G`, this flag means to add the decimal point even when there is no fractional part unless the value is non-finite, and never remove the trailing zeros after the decimal point.\n\n` 0 ` |  Left-pads the number with zeroes (0) instead of spaces when padding is specified (see width sub-specifier)\n` ' ` |\n\nFormats integers using the appropriating grouping character. For example:\n\n* ` FORMAT(\"%'d\", 12345678) ` returns ` 12,345,678 `\n* ` FORMAT(\"%'x\", 12345678) ` returns ` bc:614e `\n* ` FORMAT(\"%'o\", 55555) ` returns ` 15,4403 `\n\nThis flag is only relevant for decimal, hex, and octal values.\n\n\nFlags may be specified in any order. Duplicate flags are not an error. When flags are not relevant for some element type, they are ignored.\n\n#####  Width\n\nWidth  |  Description\n---|---\n<number> |  Minimum number of characters to be printed. If the value to be printed is shorter than this number, the result is padded with blank spaces.\nThe value is not truncated even if the result is larger\n` * ` |  The width is not specified in the format string, but as an additional integer value argument preceding the argument that has to be formatted\n\n#####  Precision\n\nPrecision  |  Description\n---|---\n` . ` <number> |\n\n* For integer specifiers `%d`, `%i`, `%o`, `%u`, `%x`, and `%X`: precision specifies the minimum number of digits to be written. If the value to be written is shorter than this number, the result is padded with trailing zeros. The value is not truncated even if the result is longer. A precision of 0 means that no character is written for the value 0.\n* For specifiers `%a`, `%A`, `%e`, `%E`, `%f`, and `%F`: this is the number of digits to be printed after the decimal point. The default value is 6.\n* For specifiers `%g` and `%G`: this is the number of significant digits to be printed, before the removal of the trailing zeros after the decimal point. The default value is 6.\n\n` .* ` |  The precision is not specified in the format string, but as an additional integer value argument preceding the argument that has to be formatted\n\n#####  %g and %G behavior\n\nThe ` %g ` and ` %G ` format specifiers choose either the decimal notation (like the ` %f ` and ` %F ` specifiers) or the scientific notation (like the `\n%e ` and ` %E ` specifiers), depending on the input value's exponent and the specified  precision  .\n\nLet p stand for the specified  precision  (defaults to 6; 1 if the specified precision is less than 1). The input value is first converted to scientific notation with precision = (p - 1). If the resulting exponent part x is less than -4 or no less than p, the scientific notation with precision = (p - 1) is used; otherwise the decimal notation with precision = (p - 1 - x) is used.\n\nUnless  ` # ` flag  is present, the trailing zeros after the decimal point are removed, and the decimal point is also removed if there is no digit after it.\n\n#####  %p and %P behavior\n\nThe ` %p ` format specifier produces a one-line printable string. The ` %P `\nformat specifier produces a multi-line printable string. You can use these format specifiers with the following data types:\n\n**Type** |  **%p** |  **%P**\n---|---|---\nJSON  |\n\nJSON input:\n\n\nJSON '\n{\n\"month\": 10,\n\"year\": 2019\n}\n'\n\nProduces a one-line printable string representing JSON:\n\n\n{\"month\":10,\"year\":2019}\n\n|\n\nJSON input:\n\n\nJSON '\n{\n\"month\": 10,\n\"year\": 2019\n}\n'\n\nProduces a multi-line printable string representing JSON:\n\n\n{\n\"month\": 10,\n\"year\": 2019\n}\n\n#####  %t and %T behavior\n\nThe ` %t ` and ` %T ` format specifiers are defined for all types. The  width\n,  precision  , and  flags  act as they do for ` %s ` : the  width  is the minimum width and the ` STRING ` will be padded to that size, and  precision is the maximum width of content to show and the ` STRING ` will be truncated to that size, prior to padding to width.\n\nThe ` %t ` specifier is always meant to be a readable form of the value.\n\nThe ` %T ` specifier is always a valid SQL literal of a similar type, such as a wider numeric type. The literal will not include casts or a type name,\nexcept for the special case of non-finite floating point values.\n\nThe ` STRING ` is formatted as follows:\n\n**Type** |  **%t** |  **%T**\n---|---|---\n` NULL ` of any type  |  NULL  |  NULL\n` INT64 `\n|  123  |  123 NUMERIC  |  123.0 _(always with .0)_ |  NUMERIC \"123.0\"\nFLOAT64  |  123.0 _(always with .0)_\n123e+10\n` inf `\n` -inf `\n` NaN ` |  123.0 _(always with .0)_\n123e+10 CAST(\"inf\" AS <type>) CAST(\"-inf\" AS <type>) CAST(\"nan\" AS <type>) STRING  |  unquoted string value  |  quoted string literal BYTES  |  unquoted escaped bytes e.g., abc\\x01\\x02  |  quoted bytes literal e.g., b\"abc\\x01\\x02\"\nBOOL  |  boolean value  |  boolean value DATE  |  2011-02-03  |  DATE \"2011-02-03\"\nTIMESTAMP  |  2011-02-03 04:05:06+00  |  TIMESTAMP \"2011-02-03 04:05:06+00\"\nINTERVAL  |  1-2 3 4:5:6.789  |  INTERVAL \"1-2 3 4:5:6.789\" YEAR TO SECOND ARRAY  |  [value, value, ...]\nwhere values are formatted with %t  |  [value, value, ...]\nwhere values are formatted with %T STRUCT  |  (value, value, ...) where fields are formatted with %t  |  (value, value, ...) where fields are formatted with %T\n\nSpecial cases:\nZero fields: STRUCT() One field: STRUCT(value) JSON  |  one-line printable string representing JSON.\n\n\n{\"name\":\"apple\",\"stock\":3}\n\n|  one-line printable string representing a JSON literal.\n\n\nJSON '{\"name\":\"apple\",\"stock\":3}'\n\n#####  Error conditions\n\nIf a format specifier is invalid, or is not compatible with the related argument type, or the wrong number or arguments are provided, then an error is produced. For example, the following ` <format_string> ` expressions are invalid:\n\n\nFORMAT('%s', 1)\n\n\nFORMAT('%')\n\n#####  NULL argument handling\n\nA ` NULL ` format string results in a ` NULL ` output ` STRING ` . Any other arguments are ignored in this case.\n\nThe function generally produces a ` NULL ` value if a ` NULL ` argument is present. For example, ` FORMAT('%i', NULL_expression) ` produces a ` NULL STRING ` as output.\n\nHowever, there are some exceptions: if the format specifier is %t or %T (both of which produce ` STRING ` s that effectively match CAST and literal value semantics), a ` NULL ` value produces 'NULL' (without the quotes) in the result ` STRING ` . For example, the function:\n\n\nFORMAT('00-%t-00', NULL_expression);\n\nReturns\n\n\n00-NULL-00\n\n#####  Additional semantic rules\n\n` FLOAT64 ` values can be ` +/-inf ` or ` NaN ` . When an argument has one of those values, the result of the format specifiers ` %f ` , ` %F ` , ` %e ` , `\n%E ` , ` %g ` , ` %G ` , and ` %t ` are ` inf ` , ` -inf ` , or ` nan ` (or the same in uppercase) as appropriate. This is consistent with how GoogleSQL casts these values to ` STRING ` . For ` %T ` , GoogleSQL returns quoted strings for ` FLOAT64 ` values that don't have non-string literal representations."
            },
            "FROM_BASE32": {
                "name": "FROM_BASE32",
                "summary": "Converts a base32-encoded ` STRING ` value into a ` BYTES `\nvalue.",
                "description": "FROM_BASE32(string_expr)\n\n**Description**\n\nConverts the base32-encoded input ` string_expr ` into ` BYTES ` format. To convert ` BYTES ` to a base32-encoded ` STRING ` , use  TO_BASE32  .\n\n**Return type**\n\n` BYTES `\n\n**Example**\n\n\nSELECT FROM_BASE32('MFRGGZDF74======') AS byte_data;\n\n-- Note that the result of FROM_BASE32 is of type BYTES, displayed as a base64-encoded string.\n/*-----------*\n| byte_data |\n+-----------+\n| YWJjZGX/  |\n*-----------*/"
            },
            "FROM_BASE64": {
                "name": "FROM_BASE64",
                "summary": "Converts a base64-encoded ` STRING ` value into a ` BYTES `\nvalue.",
                "description": "FROM_BASE64(string_expr)\n\n**Description**\n\nConverts the base64-encoded input ` string_expr ` into ` BYTES ` format. To convert ` BYTES ` to a base64-encoded ` STRING ` , use [TO_BASE64][string-\nlink-to-base64].\n\nThere are several base64 encodings in common use that vary in exactly which alphabet of 65 ASCII characters are used to encode the 64 digits and padding.\nSee [ RFC 4648 ](https://tools.ietf.org/html/rfc4648#section-4) for details.\nThis function expects the alphabet ` [A-Za-z0-9+/=] ` .\n\n**Return type**\n\n` BYTES `\n\n**Example**\n\n\nSELECT FROM_BASE64('/+A=') AS byte_data;\n\n-- Note that the result of FROM_BASE64 is of type BYTES, displayed as a base64-encoded string.\n/*-----------*\n| byte_data |\n+-----------+\n| /+A=      |\n*-----------*/\n\nTo work with an encoding using a different base64 alphabet, you might need to compose ` FROM_BASE64 ` with the ` REPLACE ` function. For instance, the `\nbase64url ` url-safe and filename-safe encoding commonly used in web programming uses ` -_= ` as the last characters rather than ` +/= ` . To decode a ` base64url ` -encoded string, replace ` - ` and ` _ ` with ` + ` and\n` / ` respectively.\n\n\nSELECT FROM_BASE64(REPLACE(REPLACE('_-A=', '-', '+'), '_', '/')) AS binary;\n\n-- Note that the result of FROM_BASE64 is of type BYTES, displayed as a base64-encoded string.\n/*--------*\n| binary |\n+--------+\n| /+A=   |\n*--------*/"
            },
            "FROM_HEX": {
                "name": "FROM_HEX",
                "summary": "Converts a hexadecimal-encoded ` STRING ` value into a ` BYTES\n` value.",
                "description": "FROM_HEX(string)\n\n**Description**\n\nConverts a hexadecimal-encoded ` STRING ` into ` BYTES ` format. Returns an error if the input ` STRING ` contains characters outside the range ` (0..9,\nA..F, a..f) ` . The lettercase of the characters does not matter. If the input\n` STRING ` has an odd number of characters, the function acts as if the input has an additional leading ` 0 ` . To convert ` BYTES ` to a hexadecimal-\nencoded ` STRING ` , use  TO_HEX  .\n\n**Return type**\n\n` BYTES `\n\n**Example**\n\n\nWITH Input AS ( SELECT '00010203aaeeefff' AS hex_str UNION ALL SELECT '0AF' UNION ALL SELECT '666f6f626172'\n) SELECT hex_str, FROM_HEX(hex_str) AS bytes_str FROM Input;\n\n-- Note that the result of FROM_HEX is of type BYTES, displayed as a base64-encoded string.\n/*------------------+--------------*\n| hex_str          | bytes_str    |\n+------------------+--------------+\n| 0AF              | AAECA6ru7/8= |\n| 00010203aaeeefff | AK8=         |\n| 666f6f626172     | Zm9vYmFy     |\n*------------------+--------------*/"
            },
            "INITCAP": {
                "name": "INITCAP",
                "summary": "Formats a ` STRING ` as proper case, which means that the first character in each word is uppercase and all other characters are lowercase.",
                "description": "INITCAP(value[, delimiters])\n\n**Description**\n\nTakes a ` STRING ` and returns it with the first character in each word in uppercase and all other characters in lowercase. Non-alphabetic characters remain the same.\n\n` delimiters ` is an optional string argument that is used to override the default set of characters used to separate words. If ` delimiters ` is not specified, it defaults to the following characters:\n` <whitespace> [ ] ( ) { } / | \\ < > ! ? @ \" ^ # $ & ~ _ , . : ; * % + - `\n\nIf ` value ` or ` delimiters ` is ` NULL ` , the function returns ` NULL ` .\n\n**Return type**\n\n` STRING `\n\n**Examples**\n\n\nWITH example AS ( SELECT 'Hello World-everyone!' AS value UNION ALL SELECT 'tHe dog BARKS loudly+friendly' AS value UNION ALL SELECT 'apples&oranges;&pears' AS value UNION ALL SELECT '\u03ba\u03b1\u03b8\u03af\u03c3\u03bc\u03b1\u03c4\u03b1 \u03c4\u03b1\u03b9\u03bd\u03b9\u03ce\u03bd' AS value ) SELECT value, INITCAP(value) AS initcap_value FROM example\n\n/*-------------------------------+-------------------------------*\n| value                         | initcap_value                 |\n+-------------------------------+-------------------------------+\n| Hello World-everyone!         | Hello World-Everyone!         |\n| tHe dog BARKS loudly+friendly | The Dog Barks Loudly+Friendly |\n| apples&oranges;&pears         | Apples&Oranges;&Pears         |\n| \u03ba\u03b1\u03b8\u03af\u03c3\u03bc\u03b1\u03c4\u03b1 \u03c4\u03b1\u03b9\u03bd\u03b9\u03ce\u03bd             | \u039a\u03b1\u03b8\u03af\u03c3\u03bc\u03b1\u03c4\u03b1 \u03a4\u03b1\u03b9\u03bd\u03b9\u03ce\u03bd             |\n*-------------------------------+-------------------------------*/\n\nWITH example AS ( SELECT 'hello WORLD!' AS value, '' AS delimiters UNION ALL SELECT '\u03ba\u03b1\u03b8\u03af\u03c3\u03bc\u03b1\u03c4\u03b1 \u03c4\u03b1\u03b9\u03bd\u03c4\u03b9\u03ce@\u03bd' AS value, '\u03c4@' AS delimiters UNION ALL SELECT 'Apples1oranges2pears' AS value, '12' AS delimiters UNION ALL SELECT 'tHisEisEaESentence' AS value, 'E' AS delimiters ) SELECT value, delimiters, INITCAP(value, delimiters) AS initcap_value FROM example;\n\n/*----------------------+------------+----------------------*\n| value                | delimiters | initcap_value        |\n+----------------------+------------+----------------------+\n| hello WORLD!         |            | Hello world!         |\n| \u03ba\u03b1\u03b8\u03af\u03c3\u03bc\u03b1\u03c4\u03b1 \u03c4\u03b1\u03b9\u03bd\u03c4\u03b9\u03ce@\u03bd  | \u03c4@         | \u039a\u03b1\u03b8\u03af\u03c3\u03bc\u03b1\u03c4\u0391 \u03c4\u0391\u03b9\u03bd\u03c4\u0399\u03ce@\u039d  |\n| Apples1oranges2pears | 12         | Apples1Oranges2Pears |\n| tHisEisEaESentence   | E          | ThisEIsEAESentence   |\n*----------------------+------------+----------------------*/"
            },
            "INSTR": {
                "name": "INSTR",
                "summary": "Finds the position of a subvalue inside another value, optionally starting the search at a given offset or occurrence.",
                "description": "INSTR(value, subvalue[, position[, occurrence]])\n\n**Description**\n\nReturns the lowest 1-based position of ` subvalue ` in ` value ` . ` value `\nand ` subvalue ` must be the same type, either ` STRING ` or ` BYTES ` .\n\nIf ` position ` is specified, the search starts at this position in ` value `\n, otherwise it starts at ` 1 ` , which is the beginning of ` value ` . If `\nposition ` is negative, the function searches backwards from the end of `\nvalue ` , with ` -1 ` indicating the last character. ` position ` is of type `\nINT64 ` and cannot be ` 0 ` .\n\nIf ` occurrence ` is specified, the search returns the position of a specific instance of ` subvalue ` in ` value ` . If not specified, ` occurrence `\ndefaults to ` 1 ` and returns the position of the first occurrence. For `\noccurrence ` > ` 1 ` , the function includes overlapping occurrences. `\noccurrence ` is of type ` INT64 ` and must be positive.\n\nThis function supports specifying [ collation\n](/bigquery/docs/reference/standard-sql/collation-concepts#collate_about) .\n\nReturns ` 0 ` if:\n\n* No match is found.\n* If ` occurrence ` is greater than the number of matches found.\n* If ` position ` is greater than the length of ` value ` .\n\nReturns ` NULL ` if:\n\n* Any input argument is ` NULL ` .\n\nReturns an error if:\n\n* ` position ` is ` 0 ` .\n* ` occurrence ` is ` 0 ` or negative.\n\n**Return type**\n\n` INT64 `\n\n**Examples**\n\n\nWITH example AS (SELECT 'banana' as value, 'an' as subvalue, 1 as position, 1 as occurrence UNION ALL SELECT 'banana' as value, 'an' as subvalue, 1 as position, 2 as occurrence UNION ALL SELECT 'banana' as value, 'an' as subvalue, 1 as position, 3 as occurrence UNION ALL SELECT 'banana' as value, 'an' as subvalue, 3 as position, 1 as occurrence UNION ALL SELECT 'banana' as value, 'an' as subvalue, -1 as position, 1 as occurrence UNION ALL SELECT 'banana' as value, 'an' as subvalue, -3 as position, 1 as occurrence UNION ALL SELECT 'banana' as value, 'ann' as subvalue, 1 as position, 1 as occurrence UNION ALL SELECT 'helloooo' as value, 'oo' as subvalue, 1 as position, 1 as occurrence UNION ALL SELECT 'helloooo' as value, 'oo' as subvalue, 1 as position, 2 as occurrence ) SELECT value, subvalue, position, occurrence, INSTR(value,\nsubvalue, position, occurrence) AS instr FROM example;\n\n/*--------------+--------------+----------+------------+-------*\n| value        | subvalue     | position | occurrence | instr |\n+--------------+--------------+----------+------------+-------+\n| banana       | an           | 1        | 1          | 2     |\n| banana       | an           | 1        | 2          | 4     |\n| banana       | an           | 1        | 3          | 0     |\n| banana       | an           | 3        | 1          | 4     |\n| banana       | an           | -1       | 1          | 4     |\n| banana       | an           | -3       | 1          | 4     |\n| banana       | ann          | 1        | 1          | 0     |\n| helloooo     | oo           | 1        | 1          | 5     |\n| helloooo     | oo           | 1        | 2          | 6     |\n*--------------+--------------+----------+------------+-------*/"
            },
            "LEFT": {
                "name": "LEFT",
                "summary": "Gets the specified leftmost portion from a ` STRING ` or ` BYTES `\nvalue.",
                "description": "LEFT(value, length)\n\n**Description**\n\nReturns a ` STRING ` or ` BYTES ` value that consists of the specified number of leftmost characters or bytes from ` value ` . The ` length ` is an ` INT64\n` that specifies the length of the returned value. If ` value ` is of type `\nBYTES ` , ` length ` is the number of leftmost bytes to return. If ` value `\nis ` STRING ` , ` length ` is the number of leftmost characters to return.\n\nIf ` length ` is 0, an empty ` STRING ` or ` BYTES ` value will be returned.\nIf ` length ` is negative, an error will be returned. If ` length ` exceeds the number of characters or bytes from ` value ` , the original ` value ` will be returned.\n\n**Return type**\n\n` STRING ` or ` BYTES `\n\n**Examples**\n\n\nWITH examples AS (SELECT 'apple' as example UNION ALL SELECT 'banana' as example UNION ALL SELECT '\u0430\u0431\u0432\u0433\u0434' as example ) SELECT example, LEFT(example, 3) AS left_example FROM examples;\n\n/*---------+--------------*\n| example | left_example |\n+---------+--------------+\n| apple   | app          |\n| banana  | ban          |\n| \u0430\u0431\u0432\u0433\u0434   | \u0430\u0431\u0432          |\n*---------+--------------*/\n\n\nWITH examples AS (SELECT b'apple' as example UNION ALL SELECT b'banana' as example UNION ALL SELECT b'\\xab\\xcd\\xef\\xaa\\xbb' as example ) SELECT example, LEFT(example, 3) AS left_example FROM examples;\n\n-- Note that the result of LEFT is of type BYTES, displayed as a base64-encoded string.\n/*----------+--------------*\n| example  | left_example |\n+----------+--------------+\n| YXBwbGU= | YXBw         |\n| YmFuYW5h | YmFu         |\n| q83vqrs= | q83v         |\n*----------+--------------*/"
            },
            "LENGTH": {
                "name": "LENGTH",
                "summary": "Gets the length of a ` STRING ` or ` BYTES ` value.",
                "description": "LENGTH(value)\n\n**Description**\n\nReturns the length of the ` STRING ` or ` BYTES ` value. The returned value is in characters for ` STRING ` arguments and in bytes for the ` BYTES `\nargument.\n\n**Return type**\n\n` INT64 `\n\n**Examples**\n\n\nWITH example AS (SELECT '\u0430\u0431\u0432\u0433\u0434' AS characters)\n\nSELECT characters,\nLENGTH(characters) AS string_example,\nLENGTH(CAST(characters AS BYTES)) AS bytes_example FROM example;\n\n/*------------+----------------+---------------*\n| characters | string_example | bytes_example |\n+------------+----------------+---------------+\n| \u0430\u0431\u0432\u0433\u0434      |              5 |            10 |\n*------------+----------------+---------------*/"
            },
            "LOWER": {
                "name": "LOWER",
                "summary": "Formats alphabetic characters in a ` STRING ` value as lowercase.\n\nFormats ASCII characters in a ` BYTES ` value as lowercase.",
                "description": "LOWER(value)\n\n**Description**\n\nFor ` STRING ` arguments, returns the original string with all alphabetic characters in lowercase. Mapping between lowercase and uppercase is done according to the [ Unicode Character Database ](http://unicode.org/ucd/) without taking into account language-specific mappings.\n\nFor ` BYTES ` arguments, the argument is treated as ASCII text, with all bytes greater than 127 left intact.\n\n**Return type**\n\n` STRING ` or ` BYTES `\n\n**Examples**\n\n\nWITH items AS (SELECT\n'FOO' as item UNION ALL SELECT\n'BAR' as item UNION ALL SELECT\n'BAZ' as item)\n\nSELECT LOWER(item) AS example FROM items;\n\n/*---------*\n| example |\n+---------+\n| foo     |\n| bar     |\n| baz     |\n*---------*/"
            },
            "LPAD": {
                "name": "LPAD",
                "summary": "Prepends a ` STRING ` or ` BYTES ` value with a pattern.",
                "description": "LPAD(original_value, return_length[, pattern])\n\n**Description**\n\nReturns a ` STRING ` or ` BYTES ` value that consists of ` original_value `\nprepended with ` pattern ` . The ` return_length ` is an ` INT64 ` that specifies the length of the returned value. If ` original_value ` is of type `\nBYTES ` , ` return_length ` is the number of bytes. If ` original_value ` is of type ` STRING ` , ` return_length ` is the number of characters.\n\nThe default value of ` pattern ` is a blank space.\n\nBoth ` original_value ` and ` pattern ` must be the same data type.\n\nIf ` return_length ` is less than or equal to the ` original_value ` length,\nthis function returns the ` original_value ` value, truncated to the value of\n` return_length ` . For example, ` LPAD('hello world', 7); ` returns ` 'hello w' ` .\n\nIf ` original_value ` , ` return_length ` , or ` pattern ` is ` NULL ` , this function returns ` NULL ` .\n\nThis function returns an error if:\n\n* ` return_length ` is negative\n* ` pattern ` is empty\n\n**Return type**\n\n` STRING ` or ` BYTES `\n\n**Examples**\n\n\nSELECT t, len, FORMAT('%T', LPAD(t, len)) AS LPAD FROM UNNEST([\nSTRUCT('abc' AS t, 5 AS len),\n('abc', 2),\n('\u4f8b\u5b50', 4)\n]);\n\n/*------+-----+----------*\n| t    | len | LPAD     |\n|------|-----|----------|\n| abc  | 5   | \"  abc\"  |\n| abc  | 2   | \"ab\"     |\n| \u4f8b\u5b50  | 4   | \"  \u4f8b\u5b50\" |\n*------+-----+----------*/\n\n\nSELECT t, len, pattern, FORMAT('%T', LPAD(t, len, pattern)) AS LPAD FROM UNNEST([\nSTRUCT('abc' AS t, 8 AS len, 'def' AS pattern),\n('abc', 5, '-'),\n('\u4f8b\u5b50', 5, '\u4e2d\u6587')\n]);\n\n/*------+-----+---------+--------------*\n| t    | len | pattern | LPAD         |\n|------|-----|---------|--------------|\n| abc  | 8   | def     | \"defdeabc\"   |\n| abc  | 5   | -       | \"--abc\"      |\n| \u4f8b\u5b50  | 5   | \u4e2d\u6587    | \"\u4e2d\u6587\u4e2d\u4f8b\u5b50\"   |\n*------+-----+---------+--------------*/\n\n\nSELECT FORMAT('%T', t) AS t, len, FORMAT('%T', LPAD(t, len)) AS LPAD FROM UNNEST([\nSTRUCT(b'abc' AS t, 5 AS len),\n(b'abc', 2),\n(b'\\xab\\xcd\\xef', 4)\n]);\n\n/*-----------------+-----+------------------*\n| t               | len | LPAD             |\n|-----------------|-----|------------------|\n| b\"abc\"          | 5   | b\"  abc\"         |\n| b\"abc\"          | 2   | b\"ab\"            |\n| b\"\\xab\\xcd\\xef\" | 4   | b\" \\xab\\xcd\\xef\" |\n*-----------------+-----+------------------*/\n\n\nSELECT FORMAT('%T', t) AS t,\nlen,\nFORMAT('%T', pattern) AS pattern,\nFORMAT('%T', LPAD(t, len, pattern)) AS LPAD FROM UNNEST([\nSTRUCT(b'abc' AS t, 8 AS len, b'def' AS pattern),\n(b'abc', 5, b'-'),\n(b'\\xab\\xcd\\xef', 5, b'\\x00')\n]);\n\n/*-----------------+-----+---------+-------------------------*\n| t               | len | pattern | LPAD                    |\n|-----------------|-----|---------|-------------------------|\n| b\"abc\"          | 8   | b\"def\"  | b\"defdeabc\"             |\n| b\"abc\"          | 5   | b\"-\"    | b\"--abc\"                |\n| b\"\\xab\\xcd\\xef\" | 5   | b\"\\x00\" | b\"\\x00\\x00\\xab\\xcd\\xef\" |\n*-----------------+-----+---------+-------------------------*/"
            },
            "LTRIM": {
                "name": "LTRIM",
                "summary": "Identical to the ` TRIM ` function, but only removes leading characters.",
                "description": "LTRIM(value1[, value2])\n\n**Description**\n\nIdentical to  TRIM  , but only removes leading characters.\n\n**Return type**\n\n` STRING ` or ` BYTES `\n\n**Examples**\n\n\nWITH items AS (SELECT '   apple   ' as item UNION ALL SELECT '   banana   ' as item UNION ALL SELECT '   orange   ' as item)\n\nSELECT CONCAT('#', LTRIM(item), '#') as example FROM items;\n\n/*-------------*\n| example     |\n+-------------+\n| #apple   #  |\n| #banana   # |\n| #orange   # |\n*-------------*/\n\n\nWITH items AS (SELECT '***apple***' as item UNION ALL SELECT '***banana***' as item UNION ALL SELECT '***orange***' as item)\n\nSELECT LTRIM(item, '*') as example FROM items;\n\n/*-----------*\n| example   |\n+-----------+\n| apple***  |\n| banana*** |\n| orange*** |\n*-----------*/\n\n\nWITH items AS (SELECT 'xxxapplexxx' as item UNION ALL SELECT 'yyybananayyy' as item UNION ALL SELECT 'zzzorangezzz' as item UNION ALL SELECT 'xyzpearxyz' as item)\n\nSELECT LTRIM(item, 'xyz') as example FROM items;\n\n/*-----------*\n| example   |\n+-----------+\n| applexxx  |\n| bananayyy |\n| orangezzz |\n| pearxyz   |\n*-----------*/"
            },
            "NORMALIZE": {
                "name": "NORMALIZE",
                "summary": "Case-sensitively normalizes the characters in a ` STRING `\nvalue.",
                "description": "NORMALIZE(value[, normalization_mode])\n\n**Description**\n\nTakes a string value and returns it as a normalized string. If you do not provide a normalization mode, ` NFC ` is used.\n\n[ Normalization\n](https://en.wikipedia.org/wiki/Unicode_equivalence#Normalization) is used to ensure that two strings are equivalent. Normalization is often used in situations in which two strings render the same on the screen but have different Unicode code points.\n\n` NORMALIZE ` supports four optional normalization modes:\n\nValue  |  Name  |  Description\n---|---|---\n` NFC ` |  Normalization Form Canonical Composition  |  Decomposes and recomposes characters by canonical equivalence.\n` NFKC ` |  Normalization Form Compatibility Composition  |  Decomposes characters by compatibility, then recomposes them by canonical equivalence.\n` NFD ` |  Normalization Form Canonical Decomposition  |  Decomposes characters by canonical equivalence, and multiple combining characters are arranged in a specific order.\n` NFKD ` |  Normalization Form Compatibility Decomposition  |  Decomposes characters by compatibility, and multiple combining characters are arranged in a specific order.\n\n**Return type**\n\n` STRING `\n\n**Examples**\n\n\nSELECT a, b, a = b as normalized FROM (SELECT NORMALIZE('\\u00ea') as a, NORMALIZE('\\u0065\\u0302') as b);\n\n/*---+---+------------*\n| a | b | normalized |\n+---+---+------------+\n| \u00ea | \u00ea | true       |\n*---+---+------------*/\n\nThe following example normalizes different space characters.\n\n\nWITH EquivalentNames AS ( SELECT name FROM UNNEST([\n'Jane\\u2004Doe',\n'John\\u2004Smith',\n'Jane\\u2005Doe',\n'Jane\\u2006Doe',\n'John Smith']) AS name ) SELECT NORMALIZE(name, NFKC) AS normalized_name,\nCOUNT(*) AS name_count FROM EquivalentNames GROUP BY 1;\n\n/*-----------------+------------*\n| normalized_name | name_count |\n+-----------------+------------+\n| John Smith      | 2          |\n| Jane Doe        | 3          |\n*-----------------+------------*/"
            },
            "NORMALIZE_AND_CASEFOLD": {
                "name": "NORMALIZE_AND_CASEFOLD",
                "summary": "Case-insensitively normalizes the characters in a ` STRING ` value.",
                "description": "NORMALIZE_AND_CASEFOLD(value[, normalization_mode])\n\n**Description**\n\nTakes a string value and returns it as a normalized string. If you do not provide a normalization mode, ` NFC ` is used.\n\n[ Normalization\n](https://en.wikipedia.org/wiki/Unicode_equivalence#Normalization) is used to ensure that two strings are equivalent. Normalization is often used in situations in which two strings render the same on the screen but have different Unicode code points.\n\n[ Case folding ](https://en.wikipedia.org/wiki/Letter_case#Case_folding) is used for the caseless comparison of strings. If you need to compare strings and case should not be considered, use ` NORMALIZE_AND_CASEFOLD ` , otherwise use  ` NORMALIZE ` .\n\n` NORMALIZE_AND_CASEFOLD ` supports four optional normalization modes:\n\nValue  |  Name  |  Description\n---|---|---\n` NFC ` |  Normalization Form Canonical Composition  |  Decomposes and recomposes characters by canonical equivalence.\n` NFKC ` |  Normalization Form Compatibility Composition  |  Decomposes characters by compatibility, then recomposes them by canonical equivalence.\n` NFD ` |  Normalization Form Canonical Decomposition  |  Decomposes characters by canonical equivalence, and multiple combining characters are arranged in a specific order.\n` NFKD ` |  Normalization Form Compatibility Decomposition  |  Decomposes characters by compatibility, and multiple combining characters are arranged in a specific order.\n\n**Return type**\n\n` STRING `\n\n**Examples**\n\n\nSELECT a, b,\nNORMALIZE(a) = NORMALIZE(b) as normalized,\nNORMALIZE_AND_CASEFOLD(a) = NORMALIZE_AND_CASEFOLD(b) as normalized_with_case_folding FROM (SELECT 'The red barn' AS a, 'The Red Barn' AS b);\n\n/*--------------+--------------+------------+------------------------------*\n| a            | b            | normalized | normalized_with_case_folding |\n+--------------+--------------+------------+------------------------------+\n| The red barn | The Red Barn | false      | true                         |\n*--------------+--------------+------------+------------------------------*/\n\n\nWITH Strings AS ( SELECT '\\u2168' AS a, 'IX' AS b UNION ALL SELECT '\\u0041\\u030A', '\\u00C5'\n) SELECT a, b,\nNORMALIZE_AND_CASEFOLD(a, NFD)=NORMALIZE_AND_CASEFOLD(b, NFD) AS nfd,\nNORMALIZE_AND_CASEFOLD(a, NFC)=NORMALIZE_AND_CASEFOLD(b, NFC) AS nfc,\nNORMALIZE_AND_CASEFOLD(a, NFKD)=NORMALIZE_AND_CASEFOLD(b, NFKD) AS nkfd,\nNORMALIZE_AND_CASEFOLD(a, NFKC)=NORMALIZE_AND_CASEFOLD(b, NFKC) AS nkfc FROM Strings;\n\n/*---+----+-------+-------+------+------*\n| a | b  | nfd   | nfc   | nkfd | nkfc |\n+---+----+-------+-------+------+------+\n| \u2168 | IX | false | false | true | true |\n| A\u030a | \u00c5  | true  | true  | true | true |\n*---+----+-------+-------+------+------*/"
            },
            "OCTET_LENGTH": {
                "name": "OCTET_LENGTH",
                "summary": "Alias for ` BYTE_LENGTH ` .",
                "description": "OCTET_LENGTH(value)\n\nAlias for  ` BYTE_LENGTH ` ."
            },
            "REGEXP_CONTAINS": {
                "name": "REGEXP_CONTAINS",
                "summary": "Checks if a value is a partial match for a regular expression.",
                "description": "REGEXP_CONTAINS(value, regexp)\n\n**Description**\n\nReturns ` TRUE ` if ` value ` is a partial match for the regular expression, `\nregexp ` .\n\nIf the ` regexp ` argument is invalid, the function returns an error.\n\nYou can search for a full match by using ` ^ ` (beginning of text) and ` $ `\n(end of text). Due to regular expression operator precedence, it is good practice to use parentheses around everything between ` ^ ` and ` $ ` .\n\n**Note:** GoogleSQL provides regular expression support using the [ re2\n](https://github.com/google/re2/wiki/Syntax) library; see that documentation for its regular expression syntax.\n\n**Return type**\n\n` BOOL `\n\n**Examples**\n\n\nSELECT email,\nREGEXP_CONTAINS(email, r'@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+') AS is_valid FROM (SELECT\n['foo@example.com', 'bar@example.org', 'www.example.net']\nAS addresses),\nUNNEST(addresses) AS email;\n\n/*-----------------+----------*\n| email           | is_valid |\n+-----------------+----------+\n| foo@example.com | true     |\n| bar@example.org | true     |\n| www.example.net | false    |\n*-----------------+----------*/\n\n-- Performs a full match, using ^ and $. Due to regular expression operator\n-- precedence, it is good practice to use parentheses around everything between ^\n-- and $.\nSELECT email,\nREGEXP_CONTAINS(email, r'^([\\w.+-]+@foo\\.com|[\\w.+-]+@bar\\.org)$') AS valid_email_address,\nREGEXP_CONTAINS(email, r'^[\\w.+-]+@foo\\.com|[\\w.+-]+@bar\\.org$') AS without_parentheses FROM (SELECT\n['a@foo.com', 'a@foo.computer', 'b@bar.org', '!b@bar.org', 'c@buz.net']\nAS addresses),\nUNNEST(addresses) AS email;\n\n/*----------------+---------------------+---------------------*\n| email          | valid_email_address | without_parentheses |\n+----------------+---------------------+---------------------+\n| a@foo.com      | true                | true                |\n| a@foo.computer | false               | true                |\n| b@bar.org      | true                | true                |\n| !b@bar.org     | false               | true                |\n| c@buz.net      | false               | false               |\n*----------------+---------------------+---------------------*/"
            },
            "REGEXP_EXTRACT": {
                "name": "REGEXP_EXTRACT",
                "summary": "Produces a substring that matches a regular expression.",
                "description": "REGEXP_EXTRACT(value, regexp[, position[, occurrence]])\n\n**Description**\n\nReturns the substring in ` value ` that matches the [ re2 regular expression\n](https://github.com/google/re2/wiki/Syntax) , ` regexp ` . Returns ` NULL `\nif there is no match.\n\nIf the regular expression contains a capturing group ( ` (...) ` ), and there is a match for that capturing group, that match is returned. If there are multiple matches for a capturing group, the first match is returned.\n\nIf ` position ` is specified, the search starts at this position in ` value `\n, otherwise it starts at the beginning of ` value ` . The ` position ` must be a positive integer and cannot be 0. If ` position ` is greater than the length of ` value ` , ` NULL ` is returned.\n\nIf ` occurrence ` is specified, the search returns a specific occurrence of the ` regexp ` in ` value ` , otherwise returns the first match. If `\noccurrence ` is greater than the number of matches found, ` NULL ` is returned. For ` occurrence ` > 1, the function searches for additional occurrences beginning with the character following the previous occurrence.\n\nReturns an error if:\n\n* The regular expression is invalid\n* The regular expression has more than one capturing group\n* The ` position ` is not a positive integer\n* The ` occurrence ` is not a positive integer\n\n**Return type**\n\n` STRING ` or ` BYTES `\n\n**Examples**\n\n\nWITH email_addresses AS (SELECT 'foo@example.com' as email UNION ALL SELECT 'bar@example.org' as email UNION ALL SELECT 'baz@example.net' as email)\n\nSELECT REGEXP_EXTRACT(email, r'^[a-zA-Z0-9_.+-]+') AS user_name FROM email_addresses;\n\n/*-----------*\n| user_name |\n+-----------+\n| foo       |\n| bar       |\n| baz       |\n*-----------*/\n\n\nWITH email_addresses AS (SELECT 'foo@example.com' as email UNION ALL SELECT 'bar@example.org' as email UNION ALL SELECT 'baz@example.net' as email)\n\nSELECT REGEXP_EXTRACT(email, r'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.([a-zA-Z0-9-.]+$)') AS top_level_domain FROM email_addresses;\n\n/*------------------*\n| top_level_domain |\n+------------------+\n| com              |\n| org              |\n| net              |\n*------------------*/\n\n\nWITH characters AS ( SELECT 'ab' AS value, '.b' AS regex UNION ALL SELECT 'ab' AS value, '(.)b' AS regex UNION ALL SELECT 'xyztb' AS value, '(.)+b' AS regex UNION ALL SELECT 'ab' AS value, '(z)?b' AS regex ) SELECT value, regex, REGEXP_EXTRACT(value, regex) AS result FROM characters;\n\n/*-------+---------+----------*\n| value | regex   | result   |\n+-------+---------+----------+\n| ab    | .b      | ab       |\n| ab    | (.)b    | a        |\n| xyztb | (.)+b   | t        |\n| ab    | (z)?b   | NULL     |\n*-------+---------+----------*/\n\n\nWITH example AS (SELECT 'Hello Helloo and Hellooo' AS value, 'H?ello+' AS regex, 1 as position,\n1 AS occurrence UNION ALL SELECT 'Hello Helloo and Hellooo', 'H?ello+', 1, 2 UNION ALL SELECT 'Hello Helloo and Hellooo', 'H?ello+', 1, 3 UNION ALL SELECT 'Hello Helloo and Hellooo', 'H?ello+', 1, 4 UNION ALL SELECT 'Hello Helloo and Hellooo', 'H?ello+', 2, 1 UNION ALL SELECT 'Hello Helloo and Hellooo', 'H?ello+', 3, 1 UNION ALL SELECT 'Hello Helloo and Hellooo', 'H?ello+', 3, 2 UNION ALL SELECT 'Hello Helloo and Hellooo', 'H?ello+', 3, 3 UNION ALL SELECT 'Hello Helloo and Hellooo', 'H?ello+', 20, 1 UNION ALL SELECT 'cats&dogs&rabbits' ,'\\\\w+&', 1, 2 UNION ALL SELECT 'cats&dogs&rabbits', '\\\\w+&', 2, 3 ) SELECT value, regex, position, occurrence, REGEXP_EXTRACT(value, regex,\nposition, occurrence) AS regexp_value FROM example;\n\n/*--------------------------+---------+----------+------------+--------------*\n| value                    | regex   | position | occurrence | regexp_value |\n+--------------------------+---------+----------+------------+--------------+\n| Hello Helloo and Hellooo | H?ello+ | 1        | 1          | Hello        |\n| Hello Helloo and Hellooo | H?ello+ | 1        | 2          | Helloo       |\n| Hello Helloo and Hellooo | H?ello+ | 1        | 3          | Hellooo      |\n| Hello Helloo and Hellooo | H?ello+ | 1        | 4          | NULL         |\n| Hello Helloo and Hellooo | H?ello+ | 2        | 1          | ello         |\n| Hello Helloo and Hellooo | H?ello+ | 3        | 1          | Helloo       |\n| Hello Helloo and Hellooo | H?ello+ | 3        | 2          | Hellooo      |\n| Hello Helloo and Hellooo | H?ello+ | 3        | 3          | NULL         |\n| Hello Helloo and Hellooo | H?ello+ | 20       | 1          | NULL         |\n| cats&dogs&rabbits        | \\w+&    | 1        | 2          | dogs&        |\n| cats&dogs&rabbits        | \\w+&    | 2        | 3          | NULL         |\n*--------------------------+---------+----------+------------+--------------*/"
            },
            "REGEXP_EXTRACT_ALL": {
                "name": "REGEXP_EXTRACT_ALL",
                "summary": "Produces an array of all substrings that match a regular expression.",
                "description": "REGEXP_EXTRACT_ALL(value, regexp)\n\n**Description**\n\nReturns an array of all substrings of ` value ` that match the [ re2 regular expression ](https://github.com/google/re2/wiki/Syntax) , ` regexp ` . Returns an empty array if there is no match.\n\nIf the regular expression contains a capturing group ( ` (...) ` ), and there is a match for that capturing group, that match is added to the results.\n\nThe ` REGEXP_EXTRACT_ALL ` function only returns non-overlapping matches. For example, using this function to extract ` ana ` from ` banana ` returns only one substring, not two.\n\nReturns an error if:\n\n* The regular expression is invalid\n* The regular expression has more than one capturing group\n\n**Return type**\n\n` ARRAY<STRING> ` or ` ARRAY<BYTES> `\n\n**Examples**\n\n\nWITH code_markdown AS (SELECT 'Try `function(x)` or `function(y)`' as code)\n\nSELECT REGEXP_EXTRACT_ALL(code, '`(.+?)`') AS example FROM code_markdown;\n\n/*----------------------------*\n| example                    |\n+----------------------------+\n| [function(x), function(y)] |\n*----------------------------*/"
            },
            "REGEXP_INSTR": {
                "name": "REGEXP_INSTR",
                "summary": "Finds the position of a regular expression match in a value, optionally starting the search at a given offset or occurrence.",
                "description": "REGEXP_INSTR(source_value, regexp [, position[, occurrence, [occurrence_position]]])\n\n**Description**\n\nReturns the lowest 1-based position of a regular expression, ` regexp ` , in `\nsource_value ` . ` source_value ` and ` regexp ` must be the same type, either\n` STRING ` or ` BYTES ` .\n\nIf ` position ` is specified, the search starts at this position in `\nsource_value ` , otherwise it starts at ` 1 ` , which is the beginning of `\nsource_value ` . ` position ` is of type ` INT64 ` and must be positive.\n\nIf ` occurrence ` is specified, the search returns the position of a specific instance of ` regexp ` in ` source_value ` . If not specified, ` occurrence `\ndefaults to ` 1 ` and returns the position of the first occurrence. For `\noccurrence ` > 1, the function searches for the next, non-overlapping occurrence. ` occurrence ` is of type ` INT64 ` and must be positive.\n\nYou can optionally use ` occurrence_position ` to specify where a position in relation to an ` occurrence ` starts. Your choices are:\n\n* ` 0 ` : Returns the start position of ` occurrence ` .\n* ` 1 ` : Returns the end position of ` occurrence ` \\+ ` 1 ` . If the end of the occurrence is at the end of ` source_value ` , ` LENGTH(source_value) + 1 ` is returned.\n\nReturns ` 0 ` if:\n\n* No match is found.\n* If ` occurrence ` is greater than the number of matches found.\n* If ` position ` is greater than the length of ` source_value ` .\n* The regular expression is empty.\n\nReturns ` NULL ` if:\n\n* ` position ` is ` NULL ` .\n* ` occurrence ` is ` NULL ` .\n\nReturns an error if:\n\n* ` position ` is ` 0 ` or negative.\n* ` occurrence ` is ` 0 ` or negative.\n* ` occurrence_position ` is neither ` 0 ` nor ` 1 ` .\n* The regular expression is invalid.\n* The regular expression has more than one capturing group.\n\n**Return type**\n\n` INT64 `\n\n**Examples**\n\n\nWITH example AS ( SELECT 'ab@cd-ef' AS source_value, '@[^-]*' AS regexp UNION ALL SELECT 'ab@d-ef', '@[^-]*' UNION ALL SELECT 'abc@cd-ef', '@[^-]*' UNION ALL SELECT 'abc-ef', '@[^-]*') SELECT source_value, regexp, REGEXP_INSTR(source_value, regexp) AS instr FROM example;\n\n/*--------------+--------+-------*\n| source_value | regexp | instr |\n+--------------+--------+-------+\n| ab@cd-ef     | @[^-]* | 3     |\n| ab@d-ef      | @[^-]* | 3     |\n| abc@cd-ef    | @[^-]* | 4     |\n| abc-ef       | @[^-]* | 0     |\n*--------------+--------+-------*/\n\n\nWITH example AS ( SELECT 'a@cd-ef b@cd-ef' AS source_value, '@[^-]*' AS regexp, 1 AS position UNION ALL SELECT 'a@cd-ef b@cd-ef', '@[^-]*', 2 UNION ALL SELECT 'a@cd-ef b@cd-ef', '@[^-]*', 3 UNION ALL SELECT 'a@cd-ef b@cd-ef', '@[^-]*', 4) SELECT source_value, regexp, position,\nREGEXP_INSTR(source_value, regexp, position) AS instr FROM example;\n\n/*-----------------+--------+----------+-------*\n| source_value    | regexp | position | instr |\n+-----------------+--------+----------+-------+\n| a@cd-ef b@cd-ef | @[^-]* | 1        | 2     |\n| a@cd-ef b@cd-ef | @[^-]* | 2        | 2     |\n| a@cd-ef b@cd-ef | @[^-]* | 3        | 10    |\n| a@cd-ef b@cd-ef | @[^-]* | 4        | 10    |\n*-----------------+--------+----------+-------*/\n\n\nWITH example AS ( SELECT 'a@cd-ef b@cd-ef c@cd-ef' AS source_value,\n'@[^-]*' AS regexp, 1 AS position, 1 AS occurrence UNION ALL SELECT 'a@cd-ef b@cd-ef c@cd-ef', '@[^-]*', 1, 2 UNION ALL SELECT 'a@cd-ef b@cd-ef c@cd-ef', '@[^-]*', 1, 3) SELECT source_value, regexp, position, occurrence,\nREGEXP_INSTR(source_value, regexp, position, occurrence) AS instr FROM example;\n\n/*-------------------------+--------+----------+------------+-------*\n| source_value            | regexp | position | occurrence | instr |\n+-------------------------+--------+----------+------------+-------+\n| a@cd-ef b@cd-ef c@cd-ef | @[^-]* | 1        | 1          | 2     |\n| a@cd-ef b@cd-ef c@cd-ef | @[^-]* | 1        | 2          | 10    |\n| a@cd-ef b@cd-ef c@cd-ef | @[^-]* | 1        | 3          | 18    |\n*-------------------------+--------+----------+------------+-------*/\n\n\nWITH example AS ( SELECT 'a@cd-ef' AS source_value, '@[^-]*' AS regexp,\n1 AS position, 1 AS occurrence, 0 AS o_position UNION ALL SELECT 'a@cd-ef', '@[^-]*', 1, 1, 1) SELECT source_value, regexp, position, occurrence, o_position,\nREGEXP_INSTR(source_value, regexp, position, occurrence, o_position) AS instr FROM example;\n\n/*--------------+--------+----------+------------+------------+-------*\n| source_value | regexp | position | occurrence | o_position | instr |\n+--------------+--------+----------+------------+------------+-------+\n| a@cd-ef      | @[^-]* | 1        | 1          | 0          | 2     |\n| a@cd-ef      | @[^-]* | 1        | 1          | 1          | 5     |\n*--------------+--------+----------+------------+------------+-------*/"
            },
            "REGEXP_REPLACE": {
                "name": "REGEXP_REPLACE",
                "summary": "Produces a ` STRING ` value where all substrings that match a regular expression are replaced with a specified value.",
                "description": "REGEXP_REPLACE(value, regexp, replacement)\n\n**Description**\n\nReturns a ` STRING ` where all substrings of ` value ` that match regular expression ` regexp ` are replaced with ` replacement ` .\n\nYou can use backslashed-escaped digits (\\1 to \\9) within the ` replacement `\nargument to insert text matching the corresponding parenthesized group in the\n` regexp ` pattern. Use \\0 to refer to the entire matching text.\n\nTo add a backslash in your regular expression, you must first escape it. For example, ` SELECT REGEXP_REPLACE('abc', 'b(.)', 'X\\\\1'); ` returns ` aXc ` .\nYou can also use [ raw strings ](/bigquery/docs/reference/standard-\nsql/lexical#string_and_bytes_literals) to remove one layer of escaping, for example ` SELECT REGEXP_REPLACE('abc', 'b(.)', r'X\\1'); ` .\n\nThe ` REGEXP_REPLACE ` function only replaces non-overlapping matches. For example, replacing ` ana ` within ` banana ` results in only one replacement,\nnot two.\n\nIf the ` regexp ` argument is not a valid regular expression, this function returns an error.\n\n**Note:** GoogleSQL provides regular expression support using the [ re2\n](https://github.com/google/re2/wiki/Syntax) library; see that documentation for its regular expression syntax.\n\n**Return type**\n\n` STRING ` or ` BYTES `\n\n**Examples**\n\n\nWITH markdown AS (SELECT '# Heading' as heading UNION ALL SELECT '# Another heading' as heading)\n\nSELECT REGEXP_REPLACE(heading, r'^# ([a-zA-Z0-9\\s]+$)', '<h1>\\\\1</h1>') AS html FROM markdown;\n\n/*--------------------------*\n| html                     |\n+--------------------------+\n| <h1>Heading</h1>         |\n| <h1>Another heading</h1> |\n*--------------------------*/"
            },
            "REGEXP_SUBSTR": {
                "name": "REGEXP_SUBSTR",
                "summary": "Synonym for ` REGEXP_EXTRACT ` .",
                "description": "REGEXP_SUBSTR(value, regexp[, position[, occurrence]])\n\n**Description**\n\nSynonym for  REGEXP_EXTRACT  .\n\n**Return type**\n\n` STRING ` or ` BYTES `\n\n**Examples**\n\n\nWITH example AS (SELECT 'Hello World Helloo' AS value, 'H?ello+' AS regex, 1 AS position, 1 AS occurrence ) SELECT value, regex, position, occurrence, REGEXP_SUBSTR(value, regex,\nposition, occurrence) AS regexp_value FROM example;\n\n/*--------------------+---------+----------+------------+--------------*\n| value              | regex   | position | occurrence | regexp_value |\n+--------------------+---------+----------+------------+--------------+\n| Hello World Helloo | H?ello+ | 1        | 1          | Hello        |\n*--------------------+---------+----------+------------+--------------*/"
            },
            "REPEAT": {
                "name": "REPEAT",
                "summary": "Produces a ` STRING ` or ` BYTES ` value that consists of an original value, repeated.",
                "description": "REPEAT(original_value, repetitions)\n\n**Description**\n\nReturns a ` STRING ` or ` BYTES ` value that consists of ` original_value ` ,\nrepeated. The ` repetitions ` parameter specifies the number of times to repeat ` original_value ` . Returns ` NULL ` if either ` original_value ` or `\nrepetitions ` are ` NULL ` .\n\nThis function returns an error if the ` repetitions ` value is negative.\n\n**Return type**\n\n` STRING ` or ` BYTES `\n\n**Examples**\n\n\nSELECT t, n, REPEAT(t, n) AS REPEAT FROM UNNEST([\nSTRUCT('abc' AS t, 3 AS n),\n('\u4f8b\u5b50', 2),\n('abc', null),\n(null, 3)\n]);\n\n/*------+------+-----------*\n| t    | n    | REPEAT    |\n|------|------|-----------|\n| abc  | 3    | abcabcabc |\n| \u4f8b\u5b50 | 2    | \u4f8b\u5b50\u4f8b\u5b50  |\n| abc  | NULL | NULL      |\n| NULL | 3    | NULL      |\n*------+------+-----------*/"
            },
            "REPLACE": {
                "name": "REPLACE",
                "summary": "Replaces all occurrences of a pattern with another pattern in a\n` STRING ` or ` BYTES ` value.",
                "description": "REPLACE(original_value, from_pattern, to_pattern)\n\n**Description**\n\nReplaces all occurrences of ` from_pattern ` with ` to_pattern ` in `\noriginal_value ` . If ` from_pattern ` is empty, no replacement is made.\n\nThis function supports specifying [ collation\n](/bigquery/docs/reference/standard-sql/collation-concepts#collate_about) .\n\n**Return type**\n\n` STRING ` or ` BYTES `\n\n**Examples**\n\n\nWITH desserts AS (SELECT 'apple pie' as dessert UNION ALL SELECT 'blackberry pie' as dessert UNION ALL SELECT 'cherry pie' as dessert)\n\nSELECT REPLACE (dessert, 'pie', 'cobbler') as example FROM desserts;\n\n/*--------------------*\n| example            |\n+--------------------+\n| apple cobbler      |\n| blackberry cobbler |\n| cherry cobbler     |\n*--------------------*/"
            },
            "REVERSE": {
                "name": "REVERSE",
                "summary": "Reverses a ` STRING ` or ` BYTES ` value.",
                "description": "REVERSE(value)\n\n**Description**\n\nReturns the reverse of the input ` STRING ` or ` BYTES ` .\n\n**Return type**\n\n` STRING ` or ` BYTES `\n\n**Examples**\n\n\nWITH example AS ( SELECT 'foo' AS sample_string, b'bar' AS sample_bytes UNION ALL SELECT '\u0430\u0431\u0432\u0433\u0434' AS sample_string, b'123' AS sample_bytes ) SELECT sample_string,\nREVERSE(sample_string) AS reverse_string,\nsample_bytes,\nREVERSE(sample_bytes) AS reverse_bytes FROM example;\n\n/*---------------+----------------+--------------+---------------*\n| sample_string | reverse_string | sample_bytes | reverse_bytes |\n+---------------+----------------+--------------+---------------+\n| foo           | oof            | bar          | rab           |\n| \u0430\u0431\u0432\u0433\u0434         | \u0434\u0433\u0432\u0431\u0430          | 123          | 321           |\n*---------------+----------------+--------------+---------------*/"
            },
            "RIGHT": {
                "name": "RIGHT",
                "summary": "Gets the specified rightmost portion from a ` STRING ` or ` BYTES\n` value.",
                "description": "RIGHT(value, length)\n\n**Description**\n\nReturns a ` STRING ` or ` BYTES ` value that consists of the specified number of rightmost characters or bytes from ` value ` . The ` length ` is an ` INT64\n` that specifies the length of the returned value. If ` value ` is ` BYTES ` ,\n` length ` is the number of rightmost bytes to return. If ` value ` is `\nSTRING ` , ` length ` is the number of rightmost characters to return.\n\nIf ` length ` is 0, an empty ` STRING ` or ` BYTES ` value will be returned.\nIf ` length ` is negative, an error will be returned. If ` length ` exceeds the number of characters or bytes from ` value ` , the original ` value ` will be returned.\n\n**Return type**\n\n` STRING ` or ` BYTES `\n\n**Examples**\n\n\nWITH examples AS (SELECT 'apple' as example UNION ALL SELECT 'banana' as example UNION ALL SELECT '\u0430\u0431\u0432\u0433\u0434' as example ) SELECT example, RIGHT(example, 3) AS right_example FROM examples;\n\n/*---------+---------------*\n| example | right_example |\n+---------+---------------+\n| apple   | ple           |\n| banana  | ana           |\n| \u0430\u0431\u0432\u0433\u0434   | \u0432\u0433\u0434           |\n*---------+---------------*/\n\n\nWITH examples AS (SELECT b'apple' as example UNION ALL SELECT b'banana' as example UNION ALL SELECT b'\\xab\\xcd\\xef\\xaa\\xbb' as example ) SELECT example, RIGHT(example, 3) AS right_example FROM examples;\n\n-- Note that the result of RIGHT is of type BYTES, displayed as a base64-encoded string.\n/*----------+---------------*\n| example  | right_example |\n+----------+---------------+\n| YXBwbGU= | cGxl          |\n| YmFuYW5h | YW5h          |\n| q83vqrs= | 76q7          |\n*----------+---------------*/"
            },
            "RPAD": {
                "name": "RPAD",
                "summary": "Appends a ` STRING ` or ` BYTES ` value with a pattern.",
                "description": "RPAD(original_value, return_length[, pattern])\n\n**Description**\n\nReturns a ` STRING ` or ` BYTES ` value that consists of ` original_value `\nappended with ` pattern ` . The ` return_length ` parameter is an ` INT64 `\nthat specifies the length of the returned value. If ` original_value ` is `\nBYTES ` , ` return_length ` is the number of bytes. If ` original_value ` is `\nSTRING ` , ` return_length ` is the number of characters.\n\nThe default value of ` pattern ` is a blank space.\n\nBoth ` original_value ` and ` pattern ` must be the same data type.\n\nIf ` return_length ` is less than or equal to the ` original_value ` length,\nthis function returns the ` original_value ` value, truncated to the value of\n` return_length ` . For example, ` RPAD('hello world', 7); ` returns ` 'hello w' ` .\n\nIf ` original_value ` , ` return_length ` , or ` pattern ` is ` NULL ` , this function returns ` NULL ` .\n\nThis function returns an error if:\n\n* ` return_length ` is negative\n* ` pattern ` is empty\n\n**Return type**\n\n` STRING ` or ` BYTES `\n\n**Examples**\n\n\nSELECT t, len, FORMAT('%T', RPAD(t, len)) AS RPAD FROM UNNEST([\nSTRUCT('abc' AS t, 5 AS len),\n('abc', 2),\n('\u4f8b\u5b50', 4)\n]);\n\n/*------+-----+----------*\n| t    | len | RPAD     |\n+------+-----+----------+\n| abc  | 5   | \"abc  \"  |\n| abc  | 2   | \"ab\"     |\n| \u4f8b\u5b50  | 4   | \"\u4f8b\u5b50  \" |\n*------+-----+----------*/\n\n\nSELECT t, len, pattern, FORMAT('%T', RPAD(t, len, pattern)) AS RPAD FROM UNNEST([\nSTRUCT('abc' AS t, 8 AS len, 'def' AS pattern),\n('abc', 5, '-'),\n('\u4f8b\u5b50', 5, '\u4e2d\u6587')\n]);\n\n/*------+-----+---------+--------------*\n| t    | len | pattern | RPAD         |\n+------+-----+---------+--------------+\n| abc  | 8   | def     | \"abcdefde\"   |\n| abc  | 5   | -       | \"abc--\"      |\n| \u4f8b\u5b50  | 5   | \u4e2d\u6587     | \"\u4f8b\u5b50\u4e2d\u6587\u4e2d\"  |\n*------+-----+---------+--------------*/\n\n\nSELECT FORMAT('%T', t) AS t, len, FORMAT('%T', RPAD(t, len)) AS RPAD FROM UNNEST([\nSTRUCT(b'abc' AS t, 5 AS len),\n(b'abc', 2),\n(b'\\xab\\xcd\\xef', 4)\n]);\n\n/*-----------------+-----+------------------*\n| t               | len | RPAD             |\n+-----------------+-----+------------------+\n| b\"abc\"          | 5   | b\"abc  \"         |\n| b\"abc\"          | 2   | b\"ab\"            |\n| b\"\\xab\\xcd\\xef\" | 4   | b\"\\xab\\xcd\\xef \" |\n*-----------------+-----+------------------*/\n\n\nSELECT FORMAT('%T', t) AS t,\nlen,\nFORMAT('%T', pattern) AS pattern,\nFORMAT('%T', RPAD(t, len, pattern)) AS RPAD FROM UNNEST([\nSTRUCT(b'abc' AS t, 8 AS len, b'def' AS pattern),\n(b'abc', 5, b'-'),\n(b'\\xab\\xcd\\xef', 5, b'\\x00')\n]);\n\n/*-----------------+-----+---------+-------------------------*\n| t               | len | pattern | RPAD                    |\n+-----------------+-----+---------+-------------------------+\n| b\"abc\"          | 8   | b\"def\"  | b\"abcdefde\"             |\n| b\"abc\"          | 5   | b\"-\"    | b\"abc--\"                |\n| b\"\\xab\\xcd\\xef\" | 5   | b\"\\x00\" | b\"\\xab\\xcd\\xef\\x00\\x00\" |\n*-----------------+-----+---------+-------------------------*/"
            },
            "RTRIM": {
                "name": "RTRIM",
                "summary": "Identical to the ` TRIM ` function, but only removes trailing characters.",
                "description": "RTRIM(value1[, value2])\n\n**Description**\n\nIdentical to  TRIM  , but only removes trailing characters.\n\n**Return type**\n\n` STRING ` or ` BYTES `\n\n**Examples**\n\n\nWITH items AS (SELECT '***apple***' as item UNION ALL SELECT '***banana***' as item UNION ALL SELECT '***orange***' as item)\n\nSELECT RTRIM(item, '*') as example FROM items;\n\n/*-----------*\n| example   |\n+-----------+\n| ***apple  |\n| ***banana |\n| ***orange |\n*-----------*/\n\n\nWITH items AS (SELECT 'applexxx' as item UNION ALL SELECT 'bananayyy' as item UNION ALL SELECT 'orangezzz' as item UNION ALL SELECT 'pearxyz' as item)\n\nSELECT RTRIM(item, 'xyz') as example FROM items;\n\n/*---------*\n| example |\n+---------+\n| apple   |\n| banana  |\n| orange  |\n| pear    |\n*---------*/"
            },
            "SAFE_CONVERT_BYTES_TO_STRING": {
                "name": "SAFE_CONVERT_BYTES_TO_STRING",
                "summary": "Converts a ` BYTES ` value to a ` STRING `\nvalue and replace any invalid UTF-8 characters with the Unicode replacement character, ` U+FFFD ` .",
                "description": "SAFE_CONVERT_BYTES_TO_STRING(value)\n\n**Description**\n\nConverts a sequence of ` BYTES ` to a ` STRING ` . Any invalid UTF-8 characters are replaced with the Unicode replacement character, ` U+FFFD ` .\n\n**Return type**\n\n` STRING `\n\n**Examples**\n\nThe following statement returns the Unicode replacement character, \ufffd.\n\n\nSELECT SAFE_CONVERT_BYTES_TO_STRING(b'\\xc2') as safe_convert;"
            },
            "SOUNDEX": {
                "name": "SOUNDEX",
                "summary": "Gets the Soundex codes for words in a ` STRING ` value.",
                "description": "SOUNDEX(value)\n\n**Description**\n\nReturns a ` STRING ` that represents the [ Soundex\n](https://en.wikipedia.org/wiki/Soundex) code for ` value ` .\n\nSOUNDEX produces a phonetic representation of a string. It indexes words by sound, as pronounced in English. It is typically used to help determine whether two strings, such as the family names _Levine_ and _Lavine_ , or the words _to_ and _too_ , have similar English-language pronunciation.\n\nThe result of the SOUNDEX consists of a letter followed by 3 digits. Non-latin characters are ignored. If the remaining string is empty after removing non-\nLatin characters, an empty ` STRING ` is returned.\n\n**Return type**\n\n` STRING `\n\n**Examples**\n\n\nWITH example AS ( SELECT 'Ashcraft' AS value UNION ALL SELECT 'Raven' AS value UNION ALL SELECT 'Ribbon' AS value UNION ALL SELECT 'apple' AS value UNION ALL SELECT 'Hello world!' AS value UNION ALL SELECT '  H3##!@llo w00orld!' AS value UNION ALL SELECT '#1' AS value UNION ALL SELECT NULL AS value ) SELECT value, SOUNDEX(value) AS soundex FROM example;\n\n/*----------------------+---------*\n| value                | soundex |\n+----------------------+---------+\n| Ashcraft             | A261    |\n| Raven                | R150    |\n| Ribbon               | R150    |\n| apple                | a140    |\n| Hello world!         | H464    |\n|   H3##!@llo w00orld! | H464    |\n| #1                   |         |\n| NULL                 | NULL    |\n*----------------------+---------*/"
            },
            "SPLIT": {
                "name": "SPLIT",
                "summary": "Splits a ` STRING ` or ` BYTES ` value, using a delimiter.",
                "description": "SPLIT(value[, delimiter])\n\n**Description**\n\nSplits ` value ` using the ` delimiter ` argument.\n\nFor ` STRING ` , the default delimiter is the comma ` , ` .\n\nFor ` BYTES ` , you must specify a delimiter.\n\nSplitting on an empty delimiter produces an array of UTF-8 characters for `\nSTRING ` values, and an array of ` BYTES ` for ` BYTES ` values.\n\nSplitting an empty ` STRING ` returns an ` ARRAY ` with a single empty `\nSTRING ` .\n\nThis function supports specifying [ collation\n](/bigquery/docs/reference/standard-sql/collation-concepts#collate_about) .\n\n**Return type**\n\n` ARRAY<STRING> ` or ` ARRAY<BYTES> `\n\n**Examples**\n\n\nWITH letters AS (SELECT '' as letter_group UNION ALL SELECT 'a' as letter_group UNION ALL SELECT 'b c d' as letter_group)\n\nSELECT SPLIT(letter_group, ' ') as example FROM letters;\n\n/*----------------------*\n| example              |\n+----------------------+\n| []                   |\n| [a]                  |\n| [b, c, d]            |\n*----------------------*/"
            },
            "STARTS_WITH": {
                "name": "STARTS_WITH",
                "summary": "Checks if a ` STRING ` or ` BYTES ` value is a prefix of another value.",
                "description": "STARTS_WITH(value, prefix)\n\n**Description**\n\nTakes two ` STRING ` or ` BYTES ` values. Returns ` TRUE ` if ` prefix ` is a prefix of ` value ` .\n\nThis function supports specifying [ collation\n](/bigquery/docs/reference/standard-sql/collation-concepts#collate_about) .\n\n**Return type**\n\n` BOOL `\n\n**Examples**\n\n\nWITH items AS (SELECT 'foo' as item UNION ALL SELECT 'bar' as item UNION ALL SELECT 'baz' as item)\n\nSELECT STARTS_WITH(item, 'b') as example FROM items;\n\n/*---------*\n| example |\n+---------+\n|   False |\n|    True |\n|    True |\n*---------*/"
            },
            "STRPOS": {
                "name": "STRPOS",
                "summary": "Finds the position of the first occurrence of a subvalue inside another value.",
                "description": "STRPOS(value, subvalue)\n\n**Description**\n\nTakes two ` STRING ` or ` BYTES ` values. Returns the 1-based position of the first occurrence of ` subvalue ` inside ` value ` . Returns ` 0 ` if `\nsubvalue ` is not found.\n\nThis function supports specifying [ collation\n](/bigquery/docs/reference/standard-sql/collation-concepts#collate_about) .\n\n**Return type**\n\n` INT64 `\n\n**Examples**\n\n\nWITH email_addresses AS (SELECT\n'foo@example.com' AS email_address UNION ALL SELECT\n'foobar@example.com' AS email_address UNION ALL SELECT\n'foobarbaz@example.com' AS email_address UNION ALL SELECT\n'quxexample.com' AS email_address)\n\nSELECT STRPOS(email_address, '@') AS example FROM email_addresses;\n\n/*---------*\n| example |\n+---------+\n|       4 |\n|       7 |\n|      10 |\n|       0 |\n*---------*/"
            },
            "SUBSTR": {
                "name": "SUBSTR",
                "summary": "Gets a portion of a ` STRING ` or ` BYTES ` value.",
                "description": "SUBSTR(value, position[, length])\n\n**Description**\n\nGets a portion (substring) of the supplied ` STRING ` or ` BYTES ` value.\n\nThe ` position ` argument is an integer specifying the starting position of the substring.\n\n* If ` position ` is ` 1 ` , the substring starts from the first character or byte.\n* If ` position ` is ` 0 ` or less than ` -LENGTH(value) ` , ` position ` is set to ` 1 ` , and the substring starts from the first character or byte.\n* If ` position ` is greater than the length of ` value ` , the function produces an empty substring.\n* If ` position ` is negative, the function counts from the end of ` value ` , with ` -1 ` indicating the last character or byte.\n\nThe ` length ` argument specifies the maximum number of characters or bytes to return.\n\n* If ` length ` is not specified, the function produces a substring that starts at the specified position and ends at the last character or byte of ` value ` .\n* If ` length ` is ` 0 ` , the function produces an empty substring.\n* If ` length ` is negative, the function produces an error.\n* The returned substring may be shorter than ` length ` , for example, when ` length ` exceeds the length of ` value ` , or when the starting position of the substring plus ` length ` is greater than the length of ` value ` .\n\n**Return type**\n\n` STRING ` or ` BYTES `\n\n**Examples**\n\n\nWITH items AS (SELECT 'apple' as item UNION ALL SELECT 'banana' as item UNION ALL SELECT 'orange' as item)\n\nSELECT SUBSTR(item, 2) as example FROM items;\n\n/*---------*\n| example |\n+---------+\n| pple    |\n| anana   |\n| range   |\n*---------*/\n\n\nWITH items AS (SELECT 'apple' as item UNION ALL SELECT 'banana' as item UNION ALL SELECT 'orange' as item)\n\nSELECT SUBSTR(item, 2, 2) as example FROM items;\n\n/*---------*\n| example |\n+---------+\n| pp      |\n| an      |\n| ra      |\n*---------*/\n\n\nWITH items AS (SELECT 'apple' as item UNION ALL SELECT 'banana' as item UNION ALL SELECT 'orange' as item)\n\nSELECT SUBSTR(item, -2) as example FROM items;\n\n/*---------*\n| example |\n+---------+\n| le      |\n| na      |\n| ge      |\n*---------*/\n\n\nWITH items AS (SELECT 'apple' as item UNION ALL SELECT 'banana' as item UNION ALL SELECT 'orange' as item)\n\nSELECT SUBSTR(item, 1, 123) as example FROM items;\n\n/*---------*\n| example |\n+---------+\n| apple   |\n| banana  |\n| orange  |\n*---------*/\n\n\nWITH items AS (SELECT 'apple' as item UNION ALL SELECT 'banana' as item UNION ALL SELECT 'orange' as item)\n\nSELECT SUBSTR(item, 123) as example FROM items;\n\n/*---------*\n| example |\n+---------+\n|         |\n|         |\n|         |\n*---------*/\n\n\nWITH items AS (SELECT 'apple' as item UNION ALL SELECT 'banana' as item UNION ALL SELECT 'orange' as item)\n\nSELECT SUBSTR(item, 123, 5) as example FROM items;\n\n/*---------*\n| example |\n+---------+\n|         |\n|         |\n|         |\n*---------*/"
            },
            "SUBSTRING": {
                "name": "SUBSTRING",
                "summary": "Alias for ` SUBSTR `",
                "description": "SUBSTRING(value, position[, length])\n\nAlias for  ` SUBSTR ` ."
            },
            "TO_BASE32": {
                "name": "TO_BASE32",
                "summary": "Converts a ` BYTES ` value to a base32-encoded ` STRING `\nvalue.",
                "description": "TO_BASE32(bytes_expr)\n\n**Description**\n\nConverts a sequence of ` BYTES ` into a base32-encoded ` STRING ` . To convert a base32-encoded ` STRING ` into ` BYTES ` , use  FROM_BASE32  .\n\n**Return type**\n\n` STRING `\n\n**Example**\n\n\nSELECT TO_BASE32(b'abcde\\xFF') AS base32_string;\n\n/*------------------*\n| base32_string    |\n+------------------+\n| MFRGGZDF74====== |\n*------------------*/"
            },
            "TO_BASE64": {
                "name": "TO_BASE64",
                "summary": "Converts a ` BYTES ` value to a base64-encoded ` STRING `\nvalue.",
                "description": "TO_BASE64(bytes_expr)\n\n**Description**\n\nConverts a sequence of ` BYTES ` into a base64-encoded ` STRING ` . To convert a base64-encoded ` STRING ` into ` BYTES ` , use  FROM_BASE64  .\n\nThere are several base64 encodings in common use that vary in exactly which alphabet of 65 ASCII characters are used to encode the 64 digits and padding.\nSee [ RFC 4648 ](https://tools.ietf.org/html/rfc4648#section-4) for details.\nThis function adds padding and uses the alphabet ` [A-Za-z0-9+/=] ` .\n\n**Return type**\n\n` STRING `\n\n**Example**\n\n\nSELECT TO_BASE64(b'\\377\\340') AS base64_string;\n\n/*---------------*\n| base64_string |\n+---------------+\n| /+A=          |\n*---------------*/\n\nTo work with an encoding using a different base64 alphabet, you might need to compose ` TO_BASE64 ` with the ` REPLACE ` function. For instance, the `\nbase64url ` url-safe and filename-safe encoding commonly used in web programming uses ` -_= ` as the last characters rather than ` +/= ` . To encode a ` base64url ` -encoded string, replace ` + ` and ` / ` with ` - ` and\n` _ ` respectively.\n\n\nSELECT REPLACE(REPLACE(TO_BASE64(b'\\377\\340'), '+', '-'), '/', '_') as websafe_base64;\n\n/*----------------*\n| websafe_base64 |\n+----------------+\n| _-A=           |\n*----------------*/"
            },
            "TO_CODE_POINTS": {
                "name": "TO_CODE_POINTS",
                "summary": "Converts a ` STRING ` or ` BYTES ` value into an array of extended ASCII code points.",
                "description": "TO_CODE_POINTS(value)\n\n**Description**\n\nTakes a ` STRING ` or ` BYTES ` value and returns an array of ` INT64 ` values that represent code points or extended ASCII character values.\n\n* If ` value ` is a ` STRING ` , each element in the returned array represents a [ code point ](https://en.wikipedia.org/wiki/Code_point) . Each code point falls within the range of [0, 0xD7FF] and [0xE000, 0x10FFFF].\n* If ` value ` is ` BYTES ` , each element in the array is an extended ASCII character value in the range of [0, 255].\n\nTo convert from an array of code points to a ` STRING ` or ` BYTES ` , see CODE_POINTS_TO_STRING  or  CODE_POINTS_TO_BYTES  .\n\n**Return type**\n\n` ARRAY<INT64> `\n\n**Examples**\n\nThe following example gets the code points for each element in an array of words.\n\n\nSELECT word, TO_CODE_POINTS(word) AS code_points FROM UNNEST(['foo', 'bar', 'baz', 'giraffe', 'llama']) AS word;\n\n/*---------+------------------------------------*\n| word    | code_points                        |\n+---------+------------------------------------+\n| foo     | [102, 111, 111]                    |\n| bar     | [98, 97, 114]                      |\n| baz     | [98, 97, 122]                      |\n| giraffe | [103, 105, 114, 97, 102, 102, 101] |\n| llama   | [108, 108, 97, 109, 97]            |\n*---------+------------------------------------*/\n\nThe following example converts integer representations of ` BYTES ` to their corresponding ASCII character values.\n\n\nSELECT word, TO_CODE_POINTS(word) AS bytes_value_as_integer FROM UNNEST([b'\\x00\\x01\\x10\\xff', b'\\x66\\x6f\\x6f']) AS word;\n\n/*------------------+------------------------*\n| word             | bytes_value_as_integer |\n+------------------+------------------------+\n| \\x00\\x01\\x10\\xff | [0, 1, 16, 255]        |\n| foo              | [102, 111, 111]        |\n*------------------+------------------------*/\n\nThe following example demonstrates the difference between a ` BYTES ` result and a ` STRING ` result.\n\n\nSELECT TO_CODE_POINTS(b'\u0100') AS b_result, TO_CODE_POINTS('\u0100') AS s_result;\n\n/*------------+----------*\n| b_result   | s_result |\n+------------+----------+\n| [196, 128] | [256]    |\n*------------+----------*/\n\nNotice that the character, \u0100, is represented as a two-byte Unicode sequence.\nAs a result, the ` BYTES ` version of ` TO_CODE_POINTS ` returns an array with two elements, while the ` STRING ` version returns an array with a single element."
            },
            "TO_HEX": {
                "name": "TO_HEX",
                "summary": "Converts a ` BYTES ` value to a hexadecimal ` STRING ` value.",
                "description": "TO_HEX(bytes)\n\n**Description**\n\nConverts a sequence of ` BYTES ` into a hexadecimal ` STRING ` . Converts each byte in the ` STRING ` as two hexadecimal characters in the range ` (0..9,\na..f) ` . To convert a hexadecimal-encoded ` STRING ` to ` BYTES ` , use FROM_HEX  .\n\n**Return type**\n\n` STRING `\n\n**Example**\n\n\nWITH Input AS ( SELECT b'\\x00\\x01\\x02\\x03\\xAA\\xEE\\xEF\\xFF' AS byte_str UNION ALL SELECT b'foobar'\n) SELECT byte_str, TO_HEX(byte_str) AS hex_str FROM Input;\n\n/*----------------------------------+------------------*\n| byte_string                      | hex_string       |\n+----------------------------------+------------------+\n| \\x00\\x01\\x02\\x03\\xaa\\xee\\xef\\xff | 00010203aaeeefff |\n| foobar                           | 666f6f626172     |\n*----------------------------------+------------------*/"
            },
            "TRANSLATE": {
                "name": "TRANSLATE",
                "summary": "Within a value, replaces each source character with the corresponding target character.",
                "description": "TRANSLATE(expression, source_characters, target_characters)\n\n**Description**\n\nIn ` expression ` , replaces each character in ` source_characters ` with the corresponding character in ` target_characters ` . All inputs must be the same type, either ` STRING ` or ` BYTES ` .\n\n* Each character in ` expression ` is translated at most once.\n* A character in ` expression ` that is not present in ` source_characters ` is left unchanged in ` expression ` .\n* A character in ` source_characters ` without a corresponding character in ` target_characters ` is omitted from the result.\n* A duplicate character in ` source_characters ` results in an error.\n\n**Return type**\n\n` STRING ` or ` BYTES `\n\n**Examples**\n\n\nWITH example AS ( SELECT 'This is a cookie' AS expression, 'sco' AS source_characters, 'zku' AS target_characters UNION ALL SELECT 'A coaster' AS expression, 'co' AS source_characters, 'k' as target_characters ) SELECT expression, source_characters, target_characters, TRANSLATE(expression,\nsource_characters, target_characters) AS translate FROM example;\n\n/*------------------+-------------------+-------------------+------------------*\n| expression       | source_characters | target_characters | translate        |\n+------------------+-------------------+-------------------+------------------+\n| This is a cookie | sco               | zku               | Thiz iz a kuukie |\n| A coaster        | co                | k                 | A kaster         |\n*------------------+-------------------+-------------------+------------------*/"
            },
            "TRIM": {
                "name": "TRIM",
                "summary": "Removes the specified leading and trailing Unicode code points or bytes from a ` STRING ` or ` BYTES ` value.",
                "description": "TRIM(value_to_trim[, set_of_characters_to_remove])\n\n**Description**\n\nTakes a ` STRING ` or ` BYTES ` value to trim.\n\nIf the value to trim is a ` STRING ` , removes from this value all leading and trailing Unicode code points in ` set_of_characters_to_remove ` . The set of code points is optional. If it is not specified, all whitespace characters are removed from the beginning and end of the value to trim.\n\nIf the value to trim is ` BYTES ` , removes from this value all leading and trailing bytes in ` set_of_characters_to_remove ` . The set of bytes is required.\n\n**Return type**\n\n* ` STRING ` if ` value_to_trim ` is a ` STRING ` value.\n* ` BYTES ` if ` value_to_trim ` is a ` BYTES ` value.\n\n**Examples**\n\nIn the following example, all leading and trailing whitespace characters are removed from ` item ` because ` set_of_characters_to_remove ` is not specified.\n\n\nWITH items AS (SELECT '   apple   ' as item UNION ALL SELECT '   banana   ' as item UNION ALL SELECT '   orange   ' as item)\n\nSELECT CONCAT('#', TRIM(item), '#') as example FROM items;\n\n/*----------*\n| example  |\n+----------+\n| #apple#  |\n| #banana# |\n| #orange# |\n*----------*/\n\nIn the following example, all leading and trailing ` * ` characters are removed from ` item ` .\n\n\nWITH items AS (SELECT '***apple***' as item UNION ALL SELECT '***banana***' as item UNION ALL SELECT '***orange***' as item)\n\nSELECT TRIM(item, '*') as example FROM items;\n\n/*---------*\n| example |\n+---------+\n| apple   |\n| banana  |\n| orange  |\n*---------*/\n\nIn the following example, all leading and trailing ` x ` , ` y ` , and ` z `\ncharacters are removed from ` item ` .\n\n\nWITH items AS (SELECT 'xxxapplexxx' as item UNION ALL SELECT 'yyybananayyy' as item UNION ALL SELECT 'zzzorangezzz' as item UNION ALL SELECT 'xyzpearxyz' as item)\n\nSELECT TRIM(item, 'xyz') as example FROM items;\n\n/*---------*\n| example |\n+---------+\n| apple   |\n| banana  |\n| orange  |\n| pear    |\n*---------*/\n\nIn the following example, examine how ` TRIM ` interprets characters as Unicode code-points. If your trailing character set contains a combining diacritic mark over a particular letter, ` TRIM ` might strip the same diacritic mark from a different letter.\n\n\nSELECT TRIM('abaW\u030a', 'Y\u030a') AS a,\nTRIM('W\u030aaba', 'Y\u030a') AS b,\nTRIM('aba\u016a\u030a', 'Y\u030a') AS c,\nTRIM('\u016a\u030aaba', 'Y\u030a') AS d;\n\n/*------+------+------+------*\n| a    | b    | c    | d    |\n+------+------+------+------+\n| abaW | W\u030aaba | aba\u016a | \u016aaba |\n*------+------+------+------*/\n\nIn the following example, all leading and trailing ` b'n' ` , ` b'a' ` , `\nb'\\xab' ` bytes are removed from ` item ` .\n\n\nWITH items AS ( SELECT b'apple' as item UNION ALL SELECT b'banana' as item UNION ALL SELECT b'\\xab\\xcd\\xef\\xaa\\xbb' as item ) SELECT item, TRIM(item, b'na\\xab') AS examples FROM items;\n\n-- Note that the result of TRIM is of type BYTES, displayed as a base64-encoded string.\n/*----------------------+------------------*\n| item                 | example          |\n+----------------------+------------------+\n| YXBwbGU=             | cHBsZQ==         |\n| YmFuYW5h             | Yg==             |\n| q83vqrs=             | ze+quw==         |\n*----------------------+------------------*/"
            },
            "UNICODE": {
                "name": "UNICODE",
                "summary": "Gets the Unicode code point for the first character in a value.",
                "description": "UNICODE(value)\n\n**Description**\n\nReturns the Unicode [ code point ](https://en.wikipedia.org/wiki/Code_point) for the first character in ` value ` . Returns ` 0 ` if ` value ` is empty, or if the resulting Unicode code point is ` 0 ` .\n\n**Return type**\n\n` INT64 `\n\n**Examples**\n\n\nSELECT UNICODE('\u00e2bcd') as A, UNICODE('\u00e2') as B, UNICODE('') as C, UNICODE(NULL) as D;\n\n/*-------+-------+-------+-------*\n| A     | B     | C     | D     |\n+-------+-------+-------+-------+\n| 226   | 226   | 0     | NULL  |\n*-------+-------+-------+-------*/"
            },
            "UPPER": {
                "name": "UPPER",
                "summary": "Formats alphabetic characters in a ` STRING ` value as uppercase.\n\nFormats ASCII characters in a ` BYTES ` value as uppercase.",
                "description": "UPPER(value)\n\n**Description**\n\nFor ` STRING ` arguments, returns the original string with all alphabetic characters in uppercase. Mapping between uppercase and lowercase is done according to the [ Unicode Character Database ](http://unicode.org/ucd/) without taking into account language-specific mappings.\n\nFor ` BYTES ` arguments, the argument is treated as ASCII text, with all bytes greater than 127 left intact.\n\n**Return type**\n\n` STRING ` or ` BYTES `\n\n**Examples**\n\n\nWITH items AS (SELECT\n'foo' as item UNION ALL SELECT\n'bar' as item UNION ALL SELECT\n'baz' as item)\n\nSELECT UPPER(item) AS example FROM items;\n\n/*---------*\n| example |\n+---------+\n| FOO     |\n| BAR     |\n| BAZ     |\n*---------*/"
            }
        }
    },
    {
        "category": "table-functions-built-in",
        "description": "GoogleSQL for BigQuery supports built-in table functions.\n\nThis topic includes functions that produce columns of a table. You can only use these functions in the ` FROM ` clause.",
        "source": "table-functions-built-in.txt",
        "functions": {
            "APPENDS": {
                "name": "APPENDS",
                "summary": "Gets all rows that are appended to a table for a given time range.",
                "description": "Gets all rows that are appended to a table for a given time range. For more information, see [ ` APPENDS TVF ` ](/bigquery/docs/change-history#appends-\ntvf) ."
            },
            "EXTERNAL_OBJECT_TRANSFORM": {
                "name": "EXTERNAL_OBJECT_TRANSFORM",
                "summary": "Produces an object table with the original columns plus one or more additional columns.",
                "description": "EXTERNAL_OBJECT_TRANSFORM(TABLE object_table_name, transform_types_array)\n\n**Description**\n\nThis function returns a transformed object table with the original columns plus one or more additional columns, depending on the ` transform_types `\nvalues specified.\n\nThis function only supports [ object tables\n](https://cloud.google.com/bigquery/docs/object-table-introduction) as inputs.\nSubqueries or any other types of tables are not supported.\n\n` object_table_name ` is the name of the object table to be transformed, in the format ` dataset_name.object_table_name ` .\n\n` transform_types_array ` is an array of ` STRING ` literals. Currently, the only supported ` transform_types_array ` value is ` SIGNED_URL ` . Specifying\n` SIGNED_URL ` creates read-only signed URLs for the objects in the identified object table, which are returned in a ` signed_url ` column. Generated signed URLs are valid for 6 hours.\n\n**Return Type**\n\nTABLE\n\n**Example**\n\nRun the following query to return URIs and signed URLs for the objects in the\n` mydataset.myobjecttable ` object table.\n\n\nSELECT uri, signed_url FROM EXTERNAL_OBJECT_TRANSFORM(TABLE mydataset.myobjecttable, ['SIGNED_URL']);\n\n--The preceding statement returns results similar to the following:\n/*-----------------------------------------------------------------------------------------------------------------------------*\n|  uri                                 | signed_url                                                                           |\n+-----------------------------------------------------------------------------------------------------------------------------+\n| gs://myobjecttable/1234_Main_St.jpeg | https://storage.googleapis.com/mybucket/1234_Main_St.jpeg?X-Goog-Algorithm=1234abcd\u2026 |\n+-----------------------------------------------------------------------------------------------------------------------------+\n| gs://myobjecttable/345_River_Rd.jpeg | https://storage.googleapis.com/mybucket/345_River_Rd.jpeg?X-Goog-Algorithm=2345bcde\u2026 |\n*-----------------------------------------------------------------------------------------------------------------------------*/"
            },
            "GAP_FILL": {
                "name": "GAP_FILL",
                "summary": "Finds and fills gaps in a time series.",
                "description": "Finds and fills gaps in a time series. For more information, see [ ` GAP_FILL\n` ](/bigquery/docs/reference/standard-sql/time-series-functions#gap_fill) in Time series functions."
            },
            "RANGE_SESSIONIZE": {
                "name": "RANGE_SESSIONIZE",
                "summary": "Produces a table of session ranges.",
                "description": "For more information, see [ ` RANGE_SESSIONIZE `\n](/bigquery/docs/reference/standard-sql/range-functions#range_sessionize) in Range functions."
            }
        }
    },
    {
        "category": "text-analysis-functions",
        "description": "GoogleSQL for BigQuery supports the following text analysis functions.",
        "source": "text-analysis-functions.txt",
        "functions": {
            "BAG_OF_WORDS": {
                "name": "BAG_OF_WORDS",
                "summary": "Gets the frequency of each term (token) in a tokenized document.",
                "description": "BAG_OF_WORDS(tokenized_document)\n\n**Definition**\n\nGets the frequency of each term (token) in a tokenized document.\n\n**Definitions**\n\n* ` tokenized_document ` : ` ARRAY<STRING> ` value that represents a document that has been tokenized. A tokenized document is a collection of terms (tokens), which are used for text analysis.\n\n**Return type**\n\n` ARRAY<STRUCT<term STRING, count INT64>> `\n\nDefinitions:\n\n* ` term ` : A unique term in the tokenized document.\n* ` count ` : The number of times the term was found in the tokenized document.\n\n**Examples**\n\nThe following query produces terms and their frequencies in two tokenized documents:\n\n\nWITH ExampleTable AS ( SELECT 1 AS id, ['I', 'like', 'pie', 'pie', 'pie', NULL] AS f UNION ALL SELECT 2 AS id, ['yum', 'yum', 'pie', NULL] AS f ) SELECT id, BAG_OF_WORDS(f) AS results FROM ExampleTable ORDER BY id;\n\n/*----+------------------------------------------------*\n| id | results                                        |\n+----+------------------------------------------------+\n| 1  | [(null, 1), ('I', 1), ('like', 1), ('pie', 3)] |\n| 2  | [(null, 1), ('pie', 1), ('yum', 2)]            |\n*----+------------------------------------------------*/"
            },
            "TEXT_ANALYZE": {
                "name": "TEXT_ANALYZE",
                "summary": "Extracts terms (tokens) from text and converts them into a tokenized document.",
                "description": "TEXT_ANALYZE( text\n[, analyzer=>{ 'LOG_ANALYZER' | 'NO_OP_ANALYZER' | 'PATTERN_ANALYZER' }]\n[, analyzer_options=>analyzer_options_values]\n)\n\n**Description**\n\nExtracts terms (tokens) from text and converts them into a tokenized document.\n\n**Definitions**\n\n* ` text ` : ` STRING ` value that represents the input text to tokenize.\n* ` analyzer ` : Optional mandatory-named argument that determines which analyzer to use to convert ` text ` into an array of terms (tokens). This can be:\n\n* ` 'LOG_ANALYZER' ` (default): Breaks the input into terms when delimiters are encountered and then normalizes the terms. If ` analyzer ` isn't specified, this is used by default. For more information, see [ ` LOG_ANALYZER ` text analyzer ](/bigquery/docs/reference/standard-sql/text-analysis#log_analyzer) .\n\n* ` 'NO_OP_ANALYZER' ` : Extracts the text as a single term (token), but doesn't apply normalization. For more information, see [ ` NO_OP_ANALYZER ` text analyzer ](/bigquery/docs/reference/standard-sql/text-analysis#no_op_analyzer) .\n\n* ` 'PATTERN_ANALYZER' ` : Breaks the input into terms that match a regular expression. For more information, see [ ` PATTERN_ANALYZER ` text analyzer ](/bigquery/docs/reference/standard-sql/text-analysis#pattern_analyzer) .\n\n* ` analyzer_options ` : Optional mandatory named argument that takes a list of text analysis rules as a JSON-formatted ` STRING ` . For more information, see [ Text analyzer options ](/bigquery/docs/reference/standard-sql/text-analysis#text_analyzer_options) .\n\n**Details**\n\nThere is no guarantee on the order of the tokens produced by this function.\n\nIf no analyzer is specified, the ` LOG_ANALYZER ` analyzer is used by default.\n\n**Return type**\n\n` ARRAY<STRING> `\n\n**Examples**\n\nThe following query uses the default text analyzer, [ ` LOG_ANALYZER `\n](/bigquery/docs/reference/standard-sql/text-analysis#log_analyzer) , with the input text:\n\n\nSELECT TEXT_ANALYZE('I like pie, you like-pie, they like 2 PIEs.') AS results\n\n/*--------------------------------------------------------------------------*\n| results                                                                  |\n+--------------------------------------------------------------------------+\n| ['i', 'like', 'pie', 'you', 'like', 'pie', 'they', 'like', '2', 'pies' ] |\n*--------------------------------------------------------------------------*/\n\nThe following query uses the [ ` NO_OP_ANALYZER `\n](/bigquery/docs/reference/standard-sql/text-analysis#no_op_analyzer) text analyzer with the input text:\n\n\nSELECT TEXT_ANALYZE(\n'I like pie, you like-pie, they like 2 PIEs.',\nanalyzer=>'NO_OP_ANALYZER'\n) AS results\n\n/*-----------------------------------------------*\n| results                                       |\n+-----------------------------------------------+\n| 'I like pie, you like-pie, they like 2 PIEs.' |\n*-----------------------------------------------*/\n\nThe following query uses the [ ` PATTERN_ANALYZER `\n](/bigquery/docs/reference/standard-sql/text-analysis#pattern_analyzer) text analyzer with the input text:\n\n\nSELECT TEXT_ANALYZE(\n'I like pie, you like-pie, they like 2 PIEs.',\nanalyzer=>'PATTERN_ANALYZER'\n) AS results\n\n/*----------------------------------------------------------------*\n| results                                                        |\n+----------------------------------------------------------------+\n| ['like', 'pie', 'you', 'like', 'pie', 'they', 'like', 'pies' ] |\n*----------------------------------------------------------------*/\n\nFor additional examples that include analyzer options, see [ Text analysis\n](/bigquery/docs/reference/standard-sql/text-analysis) .\n\nFor helpful analyzer recipes that you can use to enhance analyzer-supported queries, see [ Search with text analyzers ](/bigquery/docs/text-analysis-\nsearch) ."
            },
            "TF_IDF": {
                "name": "TF_IDF",
                "summary": "Evaluates how relevant a term (token) is to a tokenized document in a set of tokenized documents.",
                "description": "TF_IDF(tokenized_document) OVER()\n\n\nTF_IDF(tokenized_document, max_distinct_tokens) OVER()\n\n\nTF_IDF(tokenized_document, max_distinct_tokens, frequency_threshold) OVER()\n\n**Description**\n\nEvaluates how relevant a term is to a tokenized document in a set of tokenized documents, using the TF-IDF (term frequency-inverse document frequency) algorithm.\n\n**Definitions**\n\n* ` tokenized_document ` : ` ARRAY<STRING> ` value that represents a document that has been tokenized. A tokenized document is a collection of terms (tokens), which are used for text analysis.\n* ` max_distinct_tokens ` : Optional argument. Takes a non-negative ` INT64 ` value, which represents the size of the dictionary, excluding the unknown term.\n\nTerms are added to the dictionary until this threshold is met. So, if this value is ` 20 ` , the first 20 unique terms are added and then no additional terms are added.\n\nIf this argument is not provided, the default value is ` 32000 ` . If this argument is specified, the maximum value is ` 1048576 ` .\n\n* ` frequency_threshold ` : Optional argument. Takes a non-negative ` INT64 ` value that represents the minimum number of times a term must appear in a tokenized document to be included in the dictionary. So, if this value is ` 3 ` , a term must appear at least three times in the tokenized document to be added to the dictionary.\n\nIf this argument is not provided, the default value is ` 5 ` .\n\n**Details**\n\nThis function uses a TF-IDF (term frequency-inverse document frequency) algorithm to compute the relevance of terms in a set of tokenized documents.\nTF-IDF multiplies two metrics: how many times a term appears in a document (term frequency), and the inverse document frequency of the term across a collection of documents (inverse document frequency).\n\n* TDIF:\n\nterm frequency * inverse document frequency\n\n* term frequency:\n\n(count of term in document) / (document size)\n\n* inverse document frequency:\n\nlog(1 + document set size / (1 + count of documents containing term))\n\nTerms are added to a dictionary of terms if they satisfy the criteria for `\nmax_distinct_tokens ` and ` frequency_threshold ` , otherwise they are considered the _unknown term_ . The unknown term is always the first term in the dictionary and represented as ` NULL ` . The rest of the dictionary is ordered by term frequency rather than alphabetically.\n\n**Return type**\n\n` ARRAY<STRUCT<term STRING, tf_idf DOUBLE>> `\n\nDefinitions:\n\n* ` term ` : The unique term that was added to the dictionary.\n* ` tf_idf ` : The TF-IDF computation for the term.\n\n**Examples**\n\nThe following query computes the relevance of up to 10 terms that appear at least twice in a set of tokenized documents. In this example, the named arguments are passed in positionally. ` 10 ` represents ` max_distinct_tokens\n` and ` 2 ` represents ` frequency_threshold ` :\n\n\nWITH ExampleTable AS ( SELECT 1 AS id, ['I', 'like', 'pie', 'pie', 'pie', NULL] AS f UNION ALL SELECT 2 AS id, ['yum', 'yum', 'pie', NULL] AS f UNION ALL SELECT 3 AS id, ['I', 'yum', 'pie', NULL] AS f UNION ALL SELECT 4 AS id, ['you', 'like', 'pie', 'too', NULL] AS f ) SELECT id, TF_IDF(f, 10, 2) OVER() AS results FROM ExampleTable ORDER BY id;\n\n/*----+-------------------------------------------------*\n| id | results                                         |\n+----+-------------------------------------------------+\n| 1  | [{\"index\":null,\"value\":\"0.1304033435859887\"},   |\n|    |  {\"index\":\"I\",\"value\":\"0.1412163100645339\"},    |\n|    |  {\"index\":\"like\",\"value\":\"0.1412163100645339\"}, |\n|    |  {\"index\":\"pie\",\"value\":\"0.29389333245105953\"}] |\n+----+-------------------------------------------------+\n| 2  | [{\"index\":null,\"value\":\"0.1956050153789831\"},   |\n|    |  {\"index\":\"pie\",\"value\":\"0.14694666622552977\"}, |\n|    |  {\"index\":\"yum\",\"value\":\"0.4236489301936017\"}]  |\n+----+-------------------------------------------------+\n| 3  | [{\"index\":null,\"value\":\"0.1956050153789831\"},   |\n|    |  {\"index\":\"I\",\"value\":\"0.21182446509680086\"},   |\n|    |  {\"index\":\"pie\",\"value\":\"0.14694666622552977\"}, |\n|    |  {\"index\":\"yum\",\"value\":\"0.21182446509680086\"}] |\n+----+-------------------------------------------------+\n| 4  | [{\"index\":null,\"value\":\"0.4694520369095594\"},   |\n|    |  {\"index\":\"like\",\"value\":\"0.1694595720774407\"}, |\n|    |  {\"index\":\"pie\",\"value\":\"0.11755733298042381\"}] |\n*----+-------------------------------------------------*/\n\nThe following query computes the relevance of up to three terms that appear at least once in a set of tokenized documents:\n\n\nWITH ExampleTable AS ( SELECT 1 AS id, ['I', 'like', 'pie', 'pie', 'pie', NULL] AS f UNION ALL SELECT 2 AS id, ['yum', 'yum', 'pie', NULL] AS f UNION ALL SELECT 3 AS id, ['I', 'yum', 'pie', NULL] AS f UNION ALL SELECT 4 AS id, ['you', 'like', 'pie', 'too', NULL] AS f ) SELECT id, TF_IDF(f, 3, 2) OVER() AS results FROM ExampleTable ORDER BY id;\n\n/*----+-------------------------------------------------*\n| id | results                                         |\n+----+-------------------------------------------------+\n| 1  | [{\"index\":null,\"value\":\"0.12679902142647365\"},  |\n|    |  {\"index\":\"I\",\"value\":\"0.1412163100645339\"},    |\n|    |  {\"index\":\"like\",\"value\":\"0.1412163100645339\"}, |\n|    |  {\"index\":\"pie\",\"value\":\"0.29389333245105953\"}] |\n+----+-------------------------------------------------+\n| 2  | [{\"index\":null,\"value\":\"0.5705955964191315\"},   |\n|    |  {\"index\":\"pie\",\"value\":\"0.14694666622552977\"}] |\n+----+-------------------------------------------------+\n| 3  | [{\"index\":null,\"value\":\"0.380397064279421\"},    |\n|    |  {\"index\":\"I\",\"value\":\"0.21182446509680086\"},   |\n|    |  {\"index\":\"pie\",\"value\":\"0.14694666622552977\"}] |\n+----+-------------------------------------------------+\n| 4  | [{\"index\":null,\"value\":\"0.45647647713530515\"},  |\n|    |  {\"index\":\"like\",\"value\":\"0.1694595720774407\"}, |\n|    |  {\"index\":\"pie\",\"value\":\"0.11755733298042381\"}] |\n*----+-------------------------------------------------*/"
            }
        }
    },
    {
        "category": "time-functions",
        "description": "GoogleSQL for BigQuery supports the following time functions.",
        "source": "time_functions.txt",
        "functions": {
            "CURRENT_TIME": {
                "name": "CURRENT_TIME",
                "summary": "Returns the current time as a ` TIME ` value.",
                "description": "CURRENT_TIME([time_zone])\n\n\nCURRENT_TIME\n\n**Description**\n\nReturns the current time as a ` TIME ` object. Parentheses are optional when called with no arguments.\n\nThis function supports an optional ` time_zone ` parameter. See [ Time zone definitions ](/bigquery/docs/reference/standard-\nsql/timestamp_functions#timezone_definitions) for information on how to specify a time zone.\n\nThe current time is recorded at the start of the query statement which contains this function, not when this specific function is evaluated.\n\n**Return Data Type**\n\n` TIME `\n\n**Example**\n\n\nSELECT CURRENT_TIME() as now;\n\n/*----------------------------*\n| now                        |\n+----------------------------+\n| 15:31:38.776361            |\n*----------------------------*/\n\nWhen a column named ` current_time ` is present, the column name and the function call without parentheses are ambiguous. To ensure the function call,\nadd parentheses; to ensure the column name, qualify it with its [ range variable ](/bigquery/docs/reference/standard-sql/query-syntax#range_variables) . For example, the following query will select the function in the ` now `\ncolumn and the table column in the ` current_time ` column.\n\n\nWITH t AS (SELECT 'column value' AS `current_time`) SELECT current_time() as now, t.current_time FROM t;\n\n/*-----------------+--------------*\n| now             | current_time |\n+-----------------+--------------+\n| 15:31:38.776361 | column value |\n*-----------------+--------------*/"
            },
            "EXTRACT": {
                "name": "EXTRACT",
                "summary": "Extracts part of a ` TIME ` value.",
                "description": "EXTRACT(part FROM time_expression)\n\n**Description**\n\nReturns a value that corresponds to the specified ` part ` from a supplied `\ntime_expression ` .\n\nAllowed ` part ` values are:\n\n* ` MICROSECOND `\n* ` MILLISECOND `\n* ` SECOND `\n* ` MINUTE `\n* ` HOUR `\n\nReturned values truncate lower order time periods. For example, when extracting seconds, ` EXTRACT ` truncates the millisecond and microsecond values.\n\n**Return Data Type**\n\n` INT64 `\n\n**Example**\n\nIn the following example, ` EXTRACT ` returns a value corresponding to the `\nHOUR ` time part.\n\n\nSELECT EXTRACT(HOUR FROM TIME \"15:30:00\") as hour;\n\n/*------------------*\n| hour             |\n+------------------+\n| 15               |\n*------------------*/"
            },
            "FORMAT_TIME": {
                "name": "FORMAT_TIME",
                "summary": "Formats a ` TIME ` value according to the specified format string.",
                "description": "FORMAT_TIME(format_string, time_object)\n\n**Description** Formats a ` TIME ` object according to the specified `\nformat_string ` . See [ Supported Format Elements For TIME\n](/bigquery/docs/reference/standard-sql/format-\nelements#format_elements_date_time) for a list of format elements that this function supports.\n\n**Return Data Type**\n\n` STRING `\n\n**Example**\n\n\nSELECT FORMAT_TIME(\"%R\", TIME \"15:30:00\") as formatted_time;\n\n/*----------------*\n| formatted_time |\n+----------------+\n| 15:30          |\n*----------------*/"
            },
            "PARSE_TIME": {
                "name": "PARSE_TIME",
                "summary": "Converts a ` STRING ` value to a ` TIME ` value.",
                "description": "PARSE_TIME(format_string, time_string)\n\n**Description**\n\nConverts a  string representation of time  to a ` TIME ` object.\n\n` format_string ` contains the [ format elements\n](/bigquery/docs/reference/standard-sql/format-\nelements#format_elements_date_time) that define how ` time_string ` is formatted. Each element in ` time_string ` must have a corresponding element in ` format_string ` . The location of each element in ` format_string ` must match the location of each element in ` time_string ` .\n\n\n-- This works because elements on both sides match.\nSELECT PARSE_TIME(\"%I:%M:%S\", \"07:30:00\");\n\n-- This produces an error because the seconds element is in different locations.\nSELECT PARSE_TIME(\"%S:%I:%M\", \"07:30:00\");\n\n-- This produces an error because one of the seconds elements is missing.\nSELECT PARSE_TIME(\"%I:%M\", \"07:30:00\");\n\n-- This works because %T can find all matching elements in time_string.\nSELECT PARSE_TIME(\"%T\", \"07:30:00\");\n\nThe format string fully supports most format elements except for ` %P ` .\n\nWhen using ` PARSE_TIME ` , keep the following in mind:\n\n* **Unspecified fields.** Any unspecified field is initialized from ` 00:00:00.0 ` . For instance, if ` seconds ` is unspecified then it defaults to ` 00 ` , and so on.\n* **Whitespace.** One or more consecutive white spaces in the format string matches zero or more consecutive white spaces in the ` TIME ` string. In addition, leading and trailing white spaces in the ` TIME ` string are always allowed, even if they are not in the format string.\n* **Format precedence.** When two (or more) format elements have overlapping information, the last one generally overrides any earlier ones.\n* **Format divergence.** ` %p ` can be used with ` am ` , ` AM ` , ` pm ` , and ` PM ` .\n\n**Return Data Type**\n\n` TIME `\n\n**Example**\n\n\nSELECT PARSE_TIME(\"%H\", \"15\") as parsed_time;\n\n/*-------------*\n| parsed_time |\n+-------------+\n| 15:00:00    |\n*-------------*/\n\n\nSELECT PARSE_TIME('%I:%M:%S %p', '2:23:38 pm') AS parsed_time;\n\n/*-------------*\n| parsed_time |\n+-------------+\n| 14:23:38    |\n*-------------*/"
            },
            "TIME": {
                "name": "TIME",
                "summary": "Constructs a ` TIME ` value.",
                "description": "1. TIME(hour, minute, second) 2. TIME(timestamp, [time_zone]) 3. TIME(datetime)\n\n**Description**\n\n1. Constructs a ` TIME ` object using ` INT64 ` values representing the hour, minute, and second.\n2. Constructs a ` TIME ` object using a ` TIMESTAMP ` object. It supports an optional parameter to [ specify a time zone ](/bigquery/docs/reference/standard-sql/timestamp_functions#timezone_definitions) . If no time zone is specified, the default time zone, UTC, is used.\n3. Constructs a ` TIME ` object using a ` DATETIME ` object.\n\n**Return Data Type**\n\n` TIME `\n\n**Example**\n\n\nSELECT TIME(15, 30, 00) as time_hms,\nTIME(TIMESTAMP \"2008-12-25 15:30:00+08\", \"America/Los_Angeles\") as time_tstz;\n\n/*----------+-----------*\n| time_hms | time_tstz |\n+----------+-----------+\n| 15:30:00 | 23:30:00  |\n*----------+-----------*/\n\n\nSELECT TIME(DATETIME \"2008-12-25 15:30:00.000000\") AS time_dt;\n\n/*----------*\n| time_dt  |\n+----------+\n| 15:30:00 |\n*----------*/"
            },
            "TIME_ADD": {
                "name": "TIME_ADD",
                "summary": "Adds a specified time interval to a ` TIME ` value.",
                "description": "TIME_ADD(time_expression, INTERVAL int64_expression part)\n\n**Description**\n\nAdds ` int64_expression ` units of ` part ` to the ` TIME ` object.\n\n` TIME_ADD ` supports the following values for ` part ` :\n\n* ` MICROSECOND `\n* ` MILLISECOND `\n* ` SECOND `\n* ` MINUTE `\n* ` HOUR `\n\nThis function automatically adjusts when values fall outside of the 00:00:00 to 24:00:00 boundary. For example, if you add an hour to ` 23:30:00 ` , the returned value is ` 00:30:00 ` .\n\n**Return Data Types**\n\n` TIME `\n\n**Example**\n\n\nSELECT TIME \"15:30:00\" as original_time,\nTIME_ADD(TIME \"15:30:00\", INTERVAL 10 MINUTE) as later;\n\n/*-----------------------------+------------------------*\n| original_time               | later                  |\n+-----------------------------+------------------------+\n| 15:30:00                    | 15:40:00               |\n*-----------------------------+------------------------*/"
            },
            "TIME_DIFF": {
                "name": "TIME_DIFF",
                "summary": "Gets the number of unit boundaries between two ` TIME `\nvalues at a particular time granularity.",
                "description": "TIME_DIFF(start_time, end_time, granularity)\n\n**Description**\n\nGets the number of unit boundaries between two ` TIME ` values ( ` start_time\n` \\- ` end_time ` ) at a particular time granularity.\n\n**Definitions**\n\n* ` start_time ` : The starting ` TIME ` value.\n* ` end_time ` : The ending ` TIME ` value.\n* ` granularity ` : The time part that represents the granularity. This can be:\n\n* ` MICROSECOND `\n* ` MILLISECOND `\n* ` SECOND `\n* ` MINUTE `\n* ` HOUR `\n\n**Details**\n\nIf ` end_time ` is earlier than ` start_time ` , the output is negative.\nProduces an error if the computation overflows, such as if the difference in microseconds between the two ` TIME ` values overflows.\n\n**Note:** The behavior of the this function follows the type of arguments passed in. For example, ` TIME_DIFF(TIMESTAMP, TIMESTAMP, PART) ` behaves like\n` TIMESTAMP_DIFF(TIMESTAMP, TIMESTAMP, PART) ` .\n\n**Return Data Type**\n\n` INT64 `\n\n**Example**\n\n\nSELECT TIME \"15:30:00\" as first_time,\nTIME \"14:35:00\" as second_time,\nTIME_DIFF(TIME \"15:30:00\", TIME \"14:35:00\", MINUTE) as difference;\n\n/*----------------------------+------------------------+------------------------*\n| first_time                 | second_time            | difference             |\n+----------------------------+------------------------+------------------------+\n| 15:30:00                   | 14:35:00               | 55                     |\n*----------------------------+------------------------+------------------------*/"
            },
            "TIME_SUB": {
                "name": "TIME_SUB",
                "summary": "Subtracts a specified time interval from a ` TIME ` value.",
                "description": "TIME_SUB(time_expression, INTERVAL int64_expression part)\n\n**Description**\n\nSubtracts ` int64_expression ` units of ` part ` from the ` TIME ` object.\n\n` TIME_SUB ` supports the following values for ` part ` :\n\n* ` MICROSECOND `\n* ` MILLISECOND `\n* ` SECOND `\n* ` MINUTE `\n* ` HOUR `\n\nThis function automatically adjusts when values fall outside of the 00:00:00 to 24:00:00 boundary. For example, if you subtract an hour from ` 00:30:00 ` ,\nthe returned value is ` 23:30:00 ` .\n\n**Return Data Type**\n\n` TIME `\n\n**Example**\n\n\nSELECT TIME \"15:30:00\" as original_date,\nTIME_SUB(TIME \"15:30:00\", INTERVAL 10 MINUTE) as earlier;\n\n/*-----------------------------+------------------------*\n| original_date               | earlier                |\n+-----------------------------+------------------------+\n| 15:30:00                    | 15:20:00               |\n*-----------------------------+------------------------*/"
            },
            "TIME_TRUNC": {
                "name": "TIME_TRUNC",
                "summary": "Truncates a ` TIME ` value.",
                "description": "TIME_TRUNC(time_expression, time_part)\n\n**Description**\n\nTruncates a ` TIME ` value to the granularity of ` time_part ` . The ` TIME `\nvalue is always rounded to the beginning of ` time_part ` , which can be one of the following:\n\n* ` MICROSECOND ` : If used, nothing is truncated from the value.\n* ` MILLISECOND ` : The nearest lessor or equal millisecond.\n* ` SECOND ` : The nearest lessor or equal second.\n* ` MINUTE ` : The nearest lessor or equal minute.\n* ` HOUR ` : The nearest lessor or equal hour.\n\n**Return Data Type**\n\n` TIME `\n\n**Example**\n\n\nSELECT TIME \"15:30:00\" as original,\nTIME_TRUNC(TIME \"15:30:00\", HOUR) as truncated;\n\n/*----------------------------+------------------------*\n| original                   | truncated              |\n+----------------------------+------------------------+\n| 15:30:00                   | 15:00:00               |\n*----------------------------+------------------------*/"
            }
        }
    },
    {
        "category": "time-series-functions",
        "description": "GoogleSQL for BigQuery supports the following time series functions.",
        "source": "time-series-functions.txt",
        "functions": {
            "DATE_BUCKET": {
                "name": "DATE_BUCKET",
                "summary": "Gets the lower bound of the date bucket that contains a date.",
                "description": "**Preview**\n\nThis product or feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of the [ Service Specific Terms\n](/terms/service-terms) . Pre-GA products and features are available \"as is\"\nand might have limited support. For more information, see the [ launch stage descriptions ](/products#product-launch-stages) .\n\n**Note:** To provide feedback or request support for this feature, send an email to [ bigquery-time-series-preview-support@google.com ](mailto:bigquery-\ntime-series-preview-support@google.com) .\n\n\nDATE_BUCKET(date_in_bucket, bucket_width)\n\n\nDATE_BUCKET(date_in_bucket, bucket_width, bucket_origin_date)\n\n**Description**\n\nGets the lower bound of the date bucket that contains a date.\n\n**Definitions**\n\n* ` date_in_bucket ` : A ` DATE ` value that you can use to look up a date bucket.\n* ` bucket_width ` : An ` INTERVAL ` value that represents the width of a date bucket. A [ single interval ](/bigquery/docs/reference/standard-sql/data-types#single_datetime_part_interval) with [ date parts ](/bigquery/docs/reference/standard-sql/data-types#interval_datetime_parts) is supported.\n* ` bucket_origin_date ` : A ` DATE ` value that represents a point in time. All buckets expand left and right from this point. If this argument is not set, ` 1950-01-01 ` is used by default.\n\n**Return type**\n\n` DATE `\n\n**Examples**\n\nIn the following example, the origin is omitted and the default origin, `\n1950-01-01 ` is used. All buckets expand in both directions from the origin,\nand the size of each bucket is two days. The lower bound of the bucket in which ` my_date ` belongs is returned.\n\n\nWITH some_dates AS ( SELECT DATE '1949-12-29' AS my_date UNION ALL SELECT DATE '1949-12-30' UNION ALL SELECT DATE '1949-12-31' UNION ALL SELECT DATE '1950-01-01' UNION ALL SELECT DATE '1950-01-02' UNION ALL SELECT DATE '1950-01-03'\n) SELECT DATE_BUCKET(my_date, INTERVAL 2 DAY) AS bucket_lower_bound FROM some_dates;\n\n/*--------------------+\n| bucket_lower_bound |\n+--------------------+\n| 1949-12-28         |\n| 1949-12-30         |\n| 1949-12-30         |\n| 1950-12-01         |\n| 1950-12-01         |\n| 1950-12-03         |\n+--------------------*/\n\n-- Some date buckets that originate from 1950-01-01:\n-- + Bucket: ...\n-- + Bucket: [1949-12-28, 1949-12-30)\n-- + Bucket: [1949-12-30, 1950-01-01)\n-- + Origin: [1950-01-01]\n-- + Bucket: [1950-01-01, 1950-01-03)\n-- + Bucket: [1950-01-03, 1950-01-05)\n-- + Bucket: ...\n\nIn the following example, the origin has been changed to ` 2000-12-24 ` , and all buckets expand in both directions from this point. The size of each bucket is seven days. The lower bound of the bucket in which ` my_date ` belongs is returned:\n\n\nWITH some_dates AS ( SELECT DATE '2000-12-20' AS my_date UNION ALL SELECT DATE '2000-12-21' UNION ALL SELECT DATE '2000-12-22' UNION ALL SELECT DATE '2000-12-23' UNION ALL SELECT DATE '2000-12-24' UNION ALL SELECT DATE '2000-12-25'\n) SELECT DATE_BUCKET( my_date,\nINTERVAL 7 DAY,\nDATE '2000-12-24') AS bucket_lower_bound FROM some_dates;\n\n/*--------------------+\n| bucket_lower_bound |\n+--------------------+\n| 2000-12-17         |\n| 2000-12-17         |\n| 2000-12-17         |\n| 2000-12-17         |\n| 2000-12-24         |\n| 2000-12-24         |\n+--------------------*/\n\n-- Some date buckets that originate from 2000-12-24:\n-- + Bucket: ...\n-- + Bucket: [2000-12-10, 2000-12-17)\n-- + Bucket: [2000-12-17, 2000-12-24)\n-- + Origin: [2000-12-24]\n-- + Bucket: [2000-12-24, 2000-12-31)\n-- + Bucket: [2000-12-31, 2000-01-07)\n-- + Bucket: ..."
            },
            "DATETIME_BUCKET": {
                "name": "DATETIME_BUCKET",
                "summary": "Gets the lower bound of the datetime bucket that contains a datetime.",
                "description": "**Preview**\n\nThis product or feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of the [ Service Specific Terms\n](/terms/service-terms) . Pre-GA products and features are available \"as is\"\nand might have limited support. For more information, see the [ launch stage descriptions ](/products#product-launch-stages) .\n\n**Note:** To provide feedback or request support for this feature, send an email to [ bigquery-time-series-preview-support@google.com ](mailto:bigquery-\ntime-series-preview-support@google.com) .\n\n\nDATETIME_BUCKET(datetime_in_bucket, bucket_width)\n\n\nDATETIME_BUCKET(datetime_in_bucket, bucket_width, bucket_origin_datetime)\n\n**Description**\n\nGets the lower bound of the datetime bucket that contains a datetime.\n\n**Definitions**\n\n* ` datetime_in_bucket ` : A ` DATETIME ` value that you can use to look up a datetime bucket.\n* ` bucket_width ` : An ` INTERVAL ` value that represents the width of a datetime bucket. A [ single interval ](/bigquery/docs/reference/standard-sql/data-types#single_datetime_part_interval) with [ date and time parts ](/bigquery/docs/reference/standard-sql/data-types#interval_datetime_parts) is supported.\n* ` bucket_origin_datetime ` : A ` DATETIME ` value that represents a point in time. All buckets expand left and right from this point. If this argument is not set, ` 1950-01-01 00:00:00 ` is used by default.\n\n**Return type**\n\n` DATETIME `\n\n**Examples**\n\nIn the following example, the origin is omitted and the default origin, `\n1950-01-01 00:00:00 ` is used. All buckets expand in both directions from the origin, and the size of each bucket is 12 hours. The lower bound of the bucket in which ` my_datetime ` belongs is returned:\n\n\nWITH some_datetimes AS ( SELECT DATETIME '1949-12-30 13:00:00' AS my_datetime UNION ALL SELECT DATETIME '1949-12-31 00:00:00' UNION ALL SELECT DATETIME '1949-12-31 13:00:00' UNION ALL SELECT DATETIME '1950-01-01 00:00:00' UNION ALL SELECT DATETIME '1950-01-01 13:00:00' UNION ALL SELECT DATETIME '1950-01-02 00:00:00'\n) SELECT DATETIME_BUCKET(my_datetime, INTERVAL 12 HOUR) AS bucket_lower_bound FROM some_datetimes;\n\n/*---------------------+\n| bucket_lower_bound  |\n+---------------------+\n| 1949-12-30T12:00:00 |\n| 1949-12-31T00:00:00 |\n| 1949-12-31T12:00:00 |\n| 1950-01-01T00:00:00 |\n| 1950-01-01T12:00:00 |\n| 1950-01-02T00:00:00 |\n+---------------------*/\n\n-- Some datetime buckets that originate from 1950-01-01 00:00:00:\n-- + Bucket: ...\n-- + Bucket: [1949-12-30 00:00:00, 1949-12-30 12:00:00)\n-- + Bucket: [1949-12-30 12:00:00, 1950-01-01 00:00:00)\n-- + Origin: [1950-01-01 00:00:00]\n-- + Bucket: [1950-01-01 00:00:00, 1950-01-01 12:00:00)\n-- + Bucket: [1950-01-01 12:00:00, 1950-02-00 00:00:00)\n-- + Bucket: ...\n\nIn the following example, the origin has been changed to ` 2000-12-24 12:00:00\n` , and all buckets expand in both directions from this point. The size of each bucket is seven days. The lower bound of the bucket in which `\nmy_datetime ` belongs is returned:\n\n\nWITH some_datetimes AS ( SELECT DATETIME '2000-12-20 00:00:00' AS my_datetime UNION ALL SELECT DATETIME '2000-12-21 00:00:00' UNION ALL SELECT DATETIME '2000-12-22 00:00:00' UNION ALL SELECT DATETIME '2000-12-23 00:00:00' UNION ALL SELECT DATETIME '2000-12-24 00:00:00' UNION ALL SELECT DATETIME '2000-12-25 00:00:00'\n) SELECT DATETIME_BUCKET( my_datetime,\nINTERVAL 7 DAY,\nDATETIME '2000-12-22 12:00:00') AS bucket_lower_bound FROM some_datetimes;\n\n/*--------------------+\n| bucket_lower_bound |\n+--------------------+\n| 2000-12-15T12:00:00 |\n| 2000-12-15T12:00:00 |\n| 2000-12-15T12:00:00 |\n| 2000-12-22T12:00:00 |\n| 2000-12-22T12:00:00 |\n| 2000-12-22T12:00:00 |\n+--------------------*/\n\n-- Some datetime buckets that originate from 2000-12-22 12:00:00:\n-- + Bucket: ...\n-- + Bucket: [2000-12-08 12:00:00, 2000-12-15 12:00:00)\n-- + Bucket: [2000-12-15 12:00:00, 2000-12-22 12:00:00)\n-- + Origin: [2000-12-22 12:00:00]\n-- + Bucket: [2000-12-22 12:00:00, 2000-12-29 12:00:00)\n-- + Bucket: [2000-12-29 12:00:00, 2000-01-05 12:00:00)\n-- + Bucket: ..."
            },
            "GAP_FILL": {
                "name": "GAP_FILL",
                "summary": "Finds and fills gaps in a time series.",
                "description": "**Preview**\n\nThis product or feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of the [ Service Specific Terms\n](/terms/service-terms) . Pre-GA products and features are available \"as is\"\nand might have limited support. For more information, see the [ launch stage descriptions ](/products#product-launch-stages) .\n\n**Note:** To provide feedback or request support for this feature, send an email to [ bigquery-time-series-preview-support@google.com ](mailto:bigquery-\ntime-series-preview-support@google.com) .\n\n\nGAP_FILL ( TABLE time_series_table,\ntime_series_column,\nbucket_width,\n[, partitioning_columns=>value]\n[, value_columns=>value ]\n[, origin=>value]\n[, ignore_null_values=>value]\n)\n\n\nGAP_FILL ( (time_series_subquery),\ntime_series_column,\nbucket_width,\n[, partitioning_columns=>values]\n[, value_columns=>value ]\n[, origin=>value]\n[, ignore_null_values=>value]\n)\n\n**Description**\n\nFinds and fills gaps in a time series.\n\n**Definitions**\n\n* ` time_series_table ` : The name of the table that contains the time series data.\n* ` time_series_subquery ` : The subquery that contains the time series data.\n* ` time_series_column ` : The name of the column in ` time_series_table ` or ` time_series_subquery ` that contains the time points of the time series data. This column must represent a ` DATE ` , ` DATETIME ` , or ` TIMESTAMP ` type.\n* ` bucket_width ` : The ` INTERVAL ` value that represents the selected width of the time buckets. The interval can represent a ` DATE ` , ` DATETIME ` , or ` TIMESTAMP ` type.\n* ` partitioning_columns ` : An ` ARRAY<STRING> ` optional named argument. Represents an array of zero or more column names used to partition data into individual time series (time series identity). This has the same column type requirements as the ` PARTITION BY ` clause.\n* ` value_columns ` : An ` ARRAY<STRUCT<STRING, STRING>> ` optional named argument. Represents an array of column name and gap-filling method pairs in this format:\n\n[(column_name, gap_filling_method), ...]\n\n* ` column_name ` : A ` STRING ` value that represents a valid column from ` time_series_table ` . A column name can only be used once in ` value_columns ` .\n\n* ` gap_filling_method ` : A ` STRING ` value that can be one of the following gap-filling methods:\n\n* ` null ` (default): Fill in missing values with ` NULL ` values.\n\n* ` linear ` : Fill in missing values using linear interpolation. So, when a new value is added, it's based on a linear slope for a specific time bucket. When this method is used, ` column_name ` must be a numeric data type.\n\n* ` locf ` : Fill in missing values by carrying the last observed value forward. So, when a new value is added, it's based on the previous value.\n\n* ` origin ` : A ` DATE ` , ` DATETIME ` or ` TIMESTAMP ` optional named argument. Represents a point in time from which all time buckets expand in each direction.\n\nIf ` origin ` is not provided, the data type for ` time_series_column ` is assumed, and the corresponding default value is used:\n\n* ` DATE '1950-01-01' `\n* ` DATETIME '1950-01-01 00:00:00' `\n* ` TIMESTAMP '1950-01-01 00:00:00' `\n* ` ignore_null_values ` : A ` BOOL ` optional named argument. Indicates whether the function ignores ` NULL ` values in the input data when performing gap filling. By default, this value is ` TRUE ` .\n\n* If ` TRUE ` (default), ` NULL ` values are skipped during gap filling.\n\n* ` null ` is the gap-filling method for a column: If a value in a column is ` NULL ` , the output is ` NULL ` for that column.\n\n* ` locf ` or ` linear ` is the gap-filling method for a column: The previous or next non- ` NULL ` value is used. The side effect of this is that output value columns are never ` NULL ` , except for the edges.\n\n* If ` FALSE ` , ` NULL ` values are included during gap filling.\n\n* ` null ` is the gap-filling method for a column: If a value in a column is ` NULL ` , the output is ` NULL ` for that column.\n\n* ` locf ` is the gap-filling method for a column: If the previous value in that column is ` NULL ` , the output is ` NULL ` for that column.\n\n* ` linear ` is the gap-filling method for a column: If either of the endpoints in that column is ` NULL ` , the output is ` NULL ` for that column.\n\n**Details**\n\nSometimes the fixed time intervals produced by time bucket functions have gaps, either due to irregular sampling intervals or an event that caused data loss for some time period. This can cause irregularities in reporting. For example, a plot with irregular intervals might have visible discontinuity. You can use the ` GAP_FILL ` function to employ various gap-filling methods to fill in those missing data points.\n\n` time_series_column ` and ` origin ` must be of the same data type.\n\n**Return type**\n\n` TABLE `\n\n**Examples**\n\nIn the following query, the ` locf ` gap-filling method is applied to gaps:\n\n\nCREATE TEMP TABLE device_data AS SELECT * FROM UNNEST( ARRAY<STRUCT<device_id INT64, time DATETIME, signal INT64, state STRING>>[\nSTRUCT(1, DATETIME '2023-11-01 09:34:01', 74, 'INACTIVE'),\nSTRUCT(2, DATETIME '2023-11-01 09:36:00', 77, 'ACTIVE'),\nSTRUCT(3, DATETIME '2023-11-01 09:37:00', 78, 'ACTIVE'),\nSTRUCT(4, DATETIME '2023-11-01 09:38:01', 80, 'ACTIVE')\n]);\n\nSELECT *\nFROM GAP_FILL( TABLE device_data,\nts_column => 'time',\nbucket_width => INTERVAL 1 MINUTE,\nvalue_columns => [\n('signal', 'locf')\n]\n) ORDER BY time;\n\n/*---------------------+--------+\n| time                | signal |\n+---------------------+--------+\n| 2023-11-01T09:35:00 | 74     |\n| 2023-11-01T09:36:00 | 77     |\n| 2023-11-01T09:37:00 | 78     |\n| 2023-11-01T09:38:00 | 78     |\n+---------------------+--------*/\n\nIn the following query, the ` linear ` gap-filling method is applied to gaps:\n\n\nCREATE TEMP TABLE device_data AS SELECT * FROM UNNEST( ARRAY<STRUCT<device_id INT64, time DATETIME, signal INT64, state STRING>>[\nSTRUCT(1, DATETIME '2023-11-01 09:34:01', 74, 'INACTIVE'),\nSTRUCT(2, DATETIME '2023-11-01 09:36:00', 77, 'ACTIVE'),\nSTRUCT(3, DATETIME '2023-11-01 09:37:00', 78, 'ACTIVE'),\nSTRUCT(4, DATETIME '2023-11-01 09:38:01', 80, 'ACTIVE')\n]);\n\nSELECT *\nFROM GAP_FILL( TABLE device_data,\nts_column => 'time',\nbucket_width => INTERVAL 1 MINUTE,\nvalue_columns => [\n('signal', 'linear')\n]\n) ORDER BY time;\n\n/*---------------------+--------+\n| time                | signal |\n+---------------------+--------+\n| 2023-11-01T09:35:00 | 75     |\n| 2023-11-01T09:36:00 | 77     |\n| 2023-11-01T09:37:00 | 78     |\n| 2023-11-01T09:38:00 | 80     |\n+---------------------+--------*/\n\nIn the following query, the ` null ` gap-filling method is applied to gaps:\n\n\nCREATE TEMP TABLE device_data AS SELECT * FROM UNNEST( ARRAY<STRUCT<device_id INT64, time DATETIME, signal INT64, state STRING>>[\nSTRUCT(1, DATETIME '2023-11-01 09:34:01', 74, 'INACTIVE'),\nSTRUCT(2, DATETIME '2023-11-01 09:36:00', 77, 'ACTIVE'),\nSTRUCT(3, DATETIME '2023-11-01 09:37:00', 78, 'ACTIVE'),\nSTRUCT(4, DATETIME '2023-11-01 09:38:01', 80, 'ACTIVE')\n]);\n\nSELECT *\nFROM GAP_FILL( TABLE device_data,\nts_column => 'time',\nbucket_width => INTERVAL 1 MINUTE,\nvalue_columns => [\n('signal', 'null')\n]\n) ORDER BY time;\n\n/*---------------------+--------+\n| time                | signal |\n+---------------------+--------+\n| 2023-11-01T09:35:00 | NULL   |\n| 2023-11-01T09:36:00 | 77     |\n| 2023-11-01T09:37:00 | 78     |\n| 2023-11-01T09:38:00 | NULL   |\n+---------------------+--------*/\n\nIn the following query, ` NULL ` values in the input data are ignored by default:\n\n\nCREATE TEMP TABLE device_data AS SELECT * FROM UNNEST( ARRAY<STRUCT<device_id INT64, time DATETIME, signal INT64, state STRING>>[\nSTRUCT(1, DATETIME '2023-11-01 09:34:01', 74, 'INACTIVE'),\nSTRUCT(2, DATETIME '2023-11-01 09:36:00', 77, 'ACTIVE'),\nSTRUCT(3, DATETIME '2023-11-01 09:37:00', NULL, 'ACTIVE'),\nSTRUCT(4, DATETIME '2023-11-01 09:38:01', 80, 'ACTIVE')\n]);\n\nSELECT *\nFROM GAP_FILL( TABLE device_data,\nts_column => 'time',\nbucket_width => INTERVAL 1 MINUTE,\nvalue_columns => [\n('signal', 'linear')\n]\n) ORDER BY time;\n\n/*---------------------+--------+\n| time                | signal |\n+---------------------+--------+\n| 2023-11-01T09:35:00 | 75     |\n| 2023-11-01T09:36:00 | 77     |\n| 2023-11-01T09:37:00 | 78     |\n| 2023-11-01T09:38:00 | 80     |\n+---------------------+--------*/\n\nIn the following query, ` NULL ` values in the input data are not ignored,\nusing the ` ignore_null_values ` argument:\n\n\nCREATE TEMP TABLE device_data AS SELECT * FROM UNNEST( ARRAY<STRUCT<device_id INT64, time DATETIME, signal INT64, state STRING>>[\nSTRUCT(1, DATETIME '2023-11-01 09:34:01', 74, 'INACTIVE'),\nSTRUCT(2, DATETIME '2023-11-01 09:36:00', 77, 'ACTIVE'),\nSTRUCT(3, DATETIME '2023-11-01 09:37:00', NULL, 'ACTIVE'),\nSTRUCT(4, DATETIME '2023-11-01 09:38:01', 80, 'ACTIVE')\n]);\n\nSELECT *\nFROM GAP_FILL( TABLE device_data,\nts_column => 'time',\nbucket_width => INTERVAL 1 MINUTE,\nvalue_columns => [\n('signal', 'linear')\n],\nignore_null_values => FALSE ) ORDER BY time;\n\n/*---------------------+--------+\n| time                | signal |\n+---------------------+--------+\n| 2023-11-01T09:35:00 | 75     |\n| 2023-11-01T09:36:00 | 77     |\n| 2023-11-01T09:37:00 | NULL   |\n| 2023-11-01T09:38:00 | NULL   |\n+---------------------+--------*/\n\nIn the following query, when the ` value_columns ` argument is not passed in,\nthe ` null ` gap-filling method is used on all columns:\n\n\nCREATE TEMP TABLE device_data AS SELECT * FROM UNNEST( ARRAY<STRUCT<device_id INT64, time DATETIME, signal INT64, state STRING>>[\nSTRUCT(1, DATETIME '2023-11-01 09:34:01', 74, 'INACTIVE'),\nSTRUCT(2, DATETIME '2023-11-01 09:36:00', 77, 'ACTIVE'),\nSTRUCT(3, DATETIME '2023-11-01 09:37:00', 79, 'ACTIVE'),\nSTRUCT(4, DATETIME '2023-11-01 09:38:01', 80, 'ACTIVE')\n]);\n\nSELECT *\nFROM GAP_FILL( TABLE device_data,\nts_column => 'time',\nbucket_width => INTERVAL 1 MINUTE ) ORDER BY time;\n\n/*---------------------+-----------+--------+----------+\n| time                | device_id | signal | state    |\n+---------------------+-----------+--------+----------+\n| 2023-11-01T09:35:00 | NULL      | NULL   | NULL     |\n| 2023-11-01T09:36:00 | 2         | 77     | ACTIVE   |\n| 2023-11-01T09:37:00 | 3         | 79     | ACTIVE   |\n| 2023-11-01T09:38:00 | NULL      | NULL   | NULL     |\n+---------------------+-----------+--------+----------*/\n\nIn the following query, rows (buckets) are added for gaps that are found:\n\n\nCREATE TEMP TABLE device_data AS SELECT * FROM UNNEST( ARRAY<STRUCT<device_id INT64, time DATETIME, signal INT64, state STRING>>[\nSTRUCT(1, DATETIME '2023-11-01 09:35:39', 74, 'INACTIVE'),\nSTRUCT(2, DATETIME '2023-11-01 09:37:39', 77, 'ACTIVE'),\nSTRUCT(3, DATETIME '2023-11-01 09:38:00', 77, 'ACTIVE'),\nSTRUCT(4, DATETIME '2023-11-01 09:40:00', 80, 'ACTIVE')\n]);\n\nSELECT *\nFROM GAP_FILL( TABLE device_data,\nts_column => 'time',\nbucket_width => INTERVAL 1 MINUTE,\nvalue_columns => [\n('signal', 'locf')\n]\n) ORDER BY time;\n\n/*---------------------+--------+\n| time                | signal |\n+---------------------+--------+\n| 2023-11-01T09:36:00 | 74     |\n| 2023-11-01T09:37:00 | 74     |\n| 2023-11-01T09:38:00 | 74     |\n| 2023-11-01T09:39:00 | 77     |\n| 2023-11-01T09:40:00 | 77     |\n+---------------------+--------*/\n\nIn the following query, data is condensed when it fits in the same bucket and has the same values:\n\n\nCREATE TEMP TABLE device_data AS SELECT * FROM UNNEST( ARRAY<STRUCT<device_id INT64, time DATETIME, signal INT64, state STRING>>[\nSTRUCT(1, DATETIME '2023-11-01 09:35:39', 74, 'INACTIVE'),\nSTRUCT(2, DATETIME '2023-11-01 09:36:60', 77, 'ACTIVE'),\nSTRUCT(3, DATETIME '2023-11-01 09:37:00', 77, 'ACTIVE'),\nSTRUCT(4, DATETIME '2023-11-01 09:37:20', 80, 'ACTIVE')\n]);\n\nSELECT *\nFROM GAP_FILL( TABLE device_data,\nts_column => 'time',\nbucket_width => INTERVAL 1 MINUTE,\nvalue_columns => [\n('signal', 'locf')\n]\n) ORDER BY time;\n\n/*---------------------+--------+\n| time                | signal |\n+---------------------+--------+\n| 2023-11-01T09:36:00 | 74     |\n| 2023-11-01T09:37:00 | 77     |\n+---------------------+--------*/\n\nIn the following query, gap filling is applied to partitions:\n\n\nCREATE TEMP TABLE device_data AS SELECT * FROM UNNEST( ARRAY<STRUCT<device_id INT64, time DATETIME, signal INT64, state STRING>>[\nSTRUCT(2, DATETIME '2023-11-01 09:35:07', 87, 'ACTIVE'),\nSTRUCT(1, DATETIME '2023-11-01 09:35:26', 82, 'ACTIVE'),\nSTRUCT(3, DATETIME '2023-11-01 09:35:39', 74, 'INACTIVE'),\nSTRUCT(2, DATETIME '2023-11-01 09:36:07', 88, 'ACTIVE'),\nSTRUCT(1, DATETIME '2023-11-01 09:36:26', 82, 'ACTIVE'),\nSTRUCT(2, DATETIME '2023-11-01 09:37:07', 88, 'ACTIVE'),\nSTRUCT(1, DATETIME '2023-11-01 09:37:28', 80, 'ACTIVE'),\nSTRUCT(3, DATETIME '2023-11-01 09:37:39', 77, 'ACTIVE'),\nSTRUCT(2, DATETIME '2023-11-01 09:38:07', 86, 'ACTIVE'),\nSTRUCT(1, DATETIME '2023-11-01 09:38:26', 81, 'ACTIVE'),\nSTRUCT(3, DATETIME '2023-11-01 09:38:39', 77, 'ACTIVE')\n]);\n\nSELECT *\nFROM GAP_FILL( TABLE device_data,\nts_column => 'time',\nbucket_width => INTERVAL 1 MINUTE,\npartitioning_columns => ['device_id'],\nvalue_columns => [\n('signal', 'locf')\n]\n) ORDER BY device_id;\n\n/*---------------------+-----------+--------+\n| time                | device_id | signal |\n+---------------------+-----------+--------+\n| 2023-11-01T09:36:00 | 1         | 82     |\n| 2023-11-01T09:37:00 | 1         | 82     |\n| 2023-11-01T09:38:00 | 1         | 80     |\n| 2023-11-01T09:36:00 | 2         | 87     |\n| 2023-11-01T09:37:00 | 2         | 88     |\n| 2023-11-01T09:38:00 | 2         | 88     |\n| 2023-11-01T09:36:00 | 3         | 74     |\n| 2023-11-01T09:37:00 | 3         | 74     |\n| 2023-11-01T09:38:00 | 3         | 77     |\n+---------------------+-----------+--------*/\n\nIn the following query, gap filling is applied to multiple columns, and each column uses a different gap-filling method:\n\n\nCREATE TEMP TABLE device_data AS SELECT * FROM UNNEST( ARRAY<STRUCT<device_id INT64, time DATETIME, signal INT64, state STRING>>[\nSTRUCT(1, DATETIME '2023-11-01 09:34:01', 74, 'ACTIVE'),\nSTRUCT(2, DATETIME '2023-11-01 09:36:00', 77, 'INACTIVE'),\nSTRUCT(3, DATETIME '2023-11-01 09:38:00', 78, 'ACTIVE'),\nSTRUCT(4, DATETIME '2023-11-01 09:39:01', 80, 'ACTIVE')\n]);\n\nSELECT *\nFROM GAP_FILL( TABLE device_data,\nts_column => 'time',\nbucket_width => INTERVAL 1 MINUTE,\nvalue_columns => [\n('signal', 'linear'),\n('state', 'locf')\n]\n) ORDER BY time;\n\n/*---------------------+--------+----------+\n| time                | signal | state    |\n+---------------------+--------+----------+\n| 2023-11-01T09:35:00 | 75     | ACTIVE   |\n| 2023-11-01T09:36:00 | 77     | INACTIVE |\n| 2023-11-01T09:37:00 | 78     | INACTIVE |\n| 2023-11-01T09:38:00 | 78     | ACTIVE   |\n| 2023-11-01T09:39:00 | 80     | ACTIVE   |\n+---------------------+--------+----------*/\n\nIn the following query, the point of origin is changed in the gap-filling results to a custom origin, using the ` origin ` argument:\n\n\nCREATE TEMP TABLE device_data AS SELECT * FROM UNNEST( ARRAY<STRUCT<device_id INT64, time DATETIME, signal INT64, state STRING>>[\nSTRUCT(1, DATETIME '2023-11-01 09:34:01', 74, 'ACTIVE'),\nSTRUCT(2, DATETIME '2023-11-01 09:36:00', 77, 'INACTIVE'),\nSTRUCT(3, DATETIME '2023-11-01 09:38:00', 78, 'ACTIVE'),\nSTRUCT(4, DATETIME '2023-11-01 09:39:01', 80, 'ACTIVE')\n]);\n\nSELECT *\nFROM GAP_FILL( TABLE device_data,\nts_column => 'time',\nbucket_width => INTERVAL 1 MINUTE,\nvalue_columns => [\n('signal', 'null')\n],\norigin => DATETIME '2023-11-01 09:30:01'\n) ORDER BY time;\n\n/*---------------------+--------+\n| time                | signal |\n+---------------------+--------+\n| 2023-11-01T09:34:01 | 74     |\n| 2023-11-01T09:35:01 | NULL   |\n| 2023-11-01T09:36:01 | NULL   |\n| 2023-11-01T09:37:01 | NULL   |\n| 2023-11-01T09:38:01 | NULL   |\n| 2023-11-01T09:39:01 | 80     |\n+---------------------+--------*/\n\nIn the following query, a subquery is passed into the function instead of a table:\n\n\nSELECT *\nFROM GAP_FILL( ( SELECT * FROM UNNEST( ARRAY<STRUCT<device_id INT64, time DATETIME, signal INT64, state STRING>>[\nSTRUCT(1, DATETIME '2023-11-01 09:34:01', 74, 'INACTIVE'),\nSTRUCT(2, DATETIME '2023-11-01 09:36:00', 77, 'ACTIVE'),\nSTRUCT(3, DATETIME '2023-11-01 09:37:00', 78, 'ACTIVE'),\nSTRUCT(4, DATETIME '2023-11-01 09:38:01', 80, 'ACTIVE')\n]) ),\nts_column => 'time',\nbucket_width => INTERVAL 1 MINUTE,\nvalue_columns => [\n('signal', 'linear')\n]\n) ORDER BY time;\n\n/*---------------------+--------+\n| time                | signal |\n+---------------------+--------+\n| 2023-11-01T09:35:00 | 75     |\n| 2023-11-01T09:36:00 | 77     |\n| 2023-11-01T09:37:00 | 78     |\n| 2023-11-01T09:38:00 | 80     |\n+---------------------+--------*/"
            },
            "TIMESTAMP_BUCKET": {
                "name": "TIMESTAMP_BUCKET",
                "summary": "Gets the lower bound of the timestamp bucket that contains a timestamp.",
                "description": "**Preview**\n\nThis product or feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section of the [ Service Specific Terms\n](/terms/service-terms) . Pre-GA products and features are available \"as is\"\nand might have limited support. For more information, see the [ launch stage descriptions ](/products#product-launch-stages) .\n\n**Note:** To provide feedback or request support for this feature, send an email to [ bigquery-time-series-preview-support@google.com ](mailto:bigquery-\ntime-series-preview-support@google.com) .\n\n\nTIMESTAMP_BUCKET(timestamp_in_bucket, bucket_width)\n\n\nTIMESTAMP_BUCKET(timestamp_in_bucket, bucket_width, bucket_origin_timestamp)\n\n**Description**\n\nGets the lower bound of the timestamp bucket that contains a timestamp.\n\n**Definitions**\n\n* ` timestamp_in_bucket ` : A ` TIMESTAMP ` value that you can use to look up a timestamp bucket.\n* ` bucket_width ` : An ` INTERVAL ` value that represents the width of a timestamp bucket. A [ single interval ](/bigquery/docs/reference/standard-sql/data-types#single_datetime_part_interval) with [ date and time parts ](/bigquery/docs/reference/standard-sql/data-types#interval_datetime_parts) is supported.\n* ` bucket_origin_timestamp ` : A ` TIMESTAMP ` value that represents a point in time. All buckets expand left and right from this point. If this argument is not set, ` 1950-01-01 00:00:00 ` is used by default.\n\n**Return type**\n\n` TIMESTAMP `\n\n**Examples**\n\nIn the following example, the origin is omitted and the default origin, `\n1950-01-01 00:00:00 ` is used. All buckets expand in both directions from the origin, and the size of each bucket is 12 hours. The lower bound of the bucket in which ` my_timestamp ` belongs is returned:\n\n\nWITH some_timestamps AS ( SELECT TIMESTAMP '1949-12-30 13:00:00.00' AS my_timestamp UNION ALL SELECT TIMESTAMP '1949-12-31 00:00:00.00' UNION ALL SELECT TIMESTAMP '1949-12-31 13:00:00.00' UNION ALL SELECT TIMESTAMP '1950-01-01 00:00:00.00' UNION ALL SELECT TIMESTAMP '1950-01-01 13:00:00.00' UNION ALL SELECT TIMESTAMP '1950-01-02 00:00:00.00'\n) SELECT TIMESTAMP_BUCKET(my_timestamp, INTERVAL 12 HOUR) AS bucket_lower_bound FROM some_timestamps;\n\n-- Display of results may differ, depending upon the environment and\n-- time zone where this query was executed.\n/*------------------------+\n| bucket_lower_bound      |\n+-------------------------+\n| 2000-12-30 12:00:00 UTC |\n| 2000-12-31 00:00:00 UTC |\n| 2000-12-31 12:00:00 UTC |\n| 2000-01-01 00:00:00 UTC |\n| 2000-01-01 12:00:00 UTC |\n| 2000-01-01 00:00:00 UTC |\n+-------------------------*/\n\n-- Some timestamp buckets that originate from 1950-01-01 00:00:00:\n-- + Bucket: ...\n-- + Bucket: [1949-12-30 00:00:00.00 UTC, 1949-12-30 12:00:00.00 UTC)\n-- + Bucket: [1949-12-30 12:00:00.00 UTC, 1950-01-01 00:00:00.00 UTC)\n-- + Origin: [1950-01-01 00:00:00.00 UTC]\n-- + Bucket: [1950-01-01 00:00:00.00 UTC, 1950-01-01 12:00:00.00 UTC)\n-- + Bucket: [1950-01-01 12:00:00.00 UTC, 1950-02-00 00:00:00.00 UTC)\n-- + Bucket: ...\n\nIn the following example, the origin has been changed to ` 2000-12-24 12:00:00\n` , and all buckets expand in both directions from this point. The size of each bucket is seven days. The lower bound of the bucket in which `\nmy_timestamp ` belongs is returned:\n\n\nWITH some_timestamps AS ( SELECT TIMESTAMP '2000-12-20 00:00:00.00' AS my_timestamp UNION ALL SELECT TIMESTAMP '2000-12-21 00:00:00.00' UNION ALL SELECT TIMESTAMP '2000-12-22 00:00:00.00' UNION ALL SELECT TIMESTAMP '2000-12-23 00:00:00.00' UNION ALL SELECT TIMESTAMP '2000-12-24 00:00:00.00' UNION ALL SELECT TIMESTAMP '2000-12-25 00:00:00.00'\n) SELECT TIMESTAMP_BUCKET( my_timestamp,\nINTERVAL 7 DAY,\nTIMESTAMP '2000-12-22 12:00:00.00') AS bucket_lower_bound FROM some_timestamps;\n\n-- Display of results may differ, depending upon the environment and\n-- time zone where this query was executed.\n/*------------------------+\n| bucket_lower_bound      |\n+-------------------------+\n| 2000-12-15 12:00:00 UTC |\n| 2000-12-15 12:00:00 UTC |\n| 2000-12-15 12:00:00 UTC |\n| 2000-12-22 12:00:00 UTC |\n| 2000-12-22 12:00:00 UTC |\n| 2000-12-22 12:00:00 UTC |\n+-------------------------*/\n\n-- Some timestamp buckets that originate from 2000-12-22 12:00:00:\n-- + Bucket: ...\n-- + Bucket: [2000-12-08 12:00:00.00 UTC, 2000-12-15 12:00:00.00 UTC)\n-- + Bucket: [2000-12-15 12:00:00.00 UTC, 2000-12-22 12:00:00.00 UTC)\n-- + Origin: [2000-12-22 12:00:00.00 UTC]\n-- + Bucket: [2000-12-22 12:00:00.00 UTC, 2000-12-29 12:00:00.00 UTC)\n-- + Bucket: [2000-12-29 12:00:00.00 UTC, 2000-01-05 12:00:00.00 UTC)\n-- + Bucket: ..."
            }
        }
    },
    {
        "category": "timestamp-functions",
        "description": "GoogleSQL for BigQuery supports the following timestamp functions.\n\nIMPORTANT: Before working with these functions, you need to understand the difference between the formats in which timestamps are stored and displayed,\nand how time zones are used for the conversion between these formats. To learn more, see  How time zones work with timestamp functions  .\n\nNOTE: These functions return a runtime error if overflow occurs; result values are bounded by the defined [ ` DATE ` range\n](/bigquery/docs/reference/standard-sql/data-types#date_type) and [ `\nTIMESTAMP ` range ](/bigquery/docs/reference/standard-sql/data-\ntypes#timestamp_type) .",
        "source": "timestamp_functions.txt",
        "functions": {
            "CURRENT_TIMESTAMP": {
                "name": "CURRENT_TIMESTAMP",
                "summary": "Returns the current date and time as a ` TIMESTAMP `\nobject.",
                "description": "CURRENT_TIMESTAMP()\n\n\nCURRENT_TIMESTAMP\n\n**Description**\n\nReturns the current date and time as a timestamp object. The timestamp is continuous, non-ambiguous, has exactly 60 seconds per minute and does not repeat values over the leap second. Parentheses are optional.\n\nThis function handles leap seconds by smearing them across a window of 20 hours around the inserted leap second.\n\nThe current date and time is recorded at the start of the query statement which contains this function, not when this specific function is evaluated.\n\n**Supported Input Types**\n\nNot applicable\n\n**Result Data Type**\n\n` TIMESTAMP `\n\n**Examples**\n\n\nSELECT CURRENT_TIMESTAMP() AS now;\n\n/*--------------------------------*\n| now                            |\n+--------------------------------+\n| 2020-06-02 23:57:12.120174 UTC |\n*--------------------------------*/\n\nWhen a column named ` current_timestamp ` is present, the column name and the function call without parentheses are ambiguous. To ensure the function call,\nadd parentheses; to ensure the column name, qualify it with its [ range variable ](/bigquery/docs/reference/standard-sql/query-syntax#range_variables) . For example, the following query selects the function in the ` now ` column and the table column in the ` current_timestamp ` column.\n\n\nWITH t AS (SELECT 'column value' AS `current_timestamp`) SELECT current_timestamp() AS now, t.current_timestamp FROM t;\n\n/*--------------------------------+-------------------*\n| now                            | current_timestamp |\n+--------------------------------+-------------------+\n| 2020-06-02 23:57:12.120174 UTC | column value      |\n*--------------------------------+-------------------*/"
            },
            "EXTRACT": {
                "name": "EXTRACT",
                "summary": "Extracts part of a ` TIMESTAMP ` value.",
                "description": "EXTRACT(part FROM timestamp_expression [AT TIME ZONE time_zone])\n\n**Description**\n\nReturns a value that corresponds to the specified ` part ` from a supplied `\ntimestamp_expression ` . This function supports an optional ` time_zone `\nparameter. See  Time zone definitions  for information on how to specify a time zone.\n\nAllowed ` part ` values are:\n\n* ` MICROSECOND `\n* ` MILLISECOND `\n* ` SECOND `\n* ` MINUTE `\n* ` HOUR `\n* ` DAYOFWEEK ` : Returns values in the range [1,7] with Sunday as the first day of of the week.\n* ` DAY `\n* ` DAYOFYEAR `\n* ` WEEK ` : Returns the week number of the date in the range [0, 53]. Weeks begin with Sunday, and dates prior to the first Sunday of the year are in week 0.\n* ` WEEK(<WEEKDAY>) ` : Returns the week number of ` timestamp_expression ` in the range [0, 53]. Weeks begin on ` WEEKDAY ` . ` datetime ` s prior to the first ` WEEKDAY ` of the year are in week 0. Valid values for ` WEEKDAY ` are ` SUNDAY ` , ` MONDAY ` , ` TUESDAY ` , ` WEDNESDAY ` , ` THURSDAY ` , ` FRIDAY ` , and ` SATURDAY ` .\n* ` ISOWEEK ` : Returns the [ ISO 8601 week ](https://en.wikipedia.org/wiki/ISO_week_date) number of the ` datetime_expression ` . ` ISOWEEK ` s begin on Monday. Return values are in the range [1, 53]. The first ` ISOWEEK ` of each ISO year begins on the Monday before the first Thursday of the Gregorian calendar year.\n* ` MONTH `\n* ` QUARTER `\n* ` YEAR `\n* ` ISOYEAR ` : Returns the [ ISO 8601 ](https://en.wikipedia.org/wiki/ISO_8601) week-numbering year, which is the Gregorian calendar year containing the Thursday of the week to which ` date_expression ` belongs.\n* ` DATE `\n* ` DATETIME `\n* ` TIME `\n\nReturned values truncate lower order time periods. For example, when extracting seconds, ` EXTRACT ` truncates the millisecond and microsecond values.\n\n**Return Data Type**\n\n` INT64 ` , except in the following cases:\n\n* If ` part ` is ` DATE ` , the function returns a ` DATE ` object.\n\n**Examples**\n\nIn the following example, ` EXTRACT ` returns a value corresponding to the `\nDAY ` time part.\n\n\nWITH Input AS (SELECT TIMESTAMP(\"2008-12-25 05:30:00+00\") AS timestamp_value) SELECT EXTRACT(DAY FROM timestamp_value AT TIME ZONE \"UTC\") AS the_day_utc,\nEXTRACT(DAY FROM timestamp_value AT TIME ZONE \"America/Los_Angeles\") AS the_day_california FROM Input\n\n/*-------------+--------------------*\n| the_day_utc | the_day_california |\n+-------------+--------------------+\n| 25          | 24                 |\n*-------------+--------------------*/\n\nIn the following example, ` EXTRACT ` returns values corresponding to different time parts from a column of type ` TIMESTAMP ` .\n\n\nWITH Timestamps AS ( SELECT TIMESTAMP(\"2005-01-03 12:34:56+00\") AS timestamp_value UNION ALL SELECT TIMESTAMP(\"2007-12-31 12:00:00+00\") UNION ALL SELECT TIMESTAMP(\"2009-01-01 12:00:00+00\") UNION ALL SELECT TIMESTAMP(\"2009-12-31 12:00:00+00\") UNION ALL SELECT TIMESTAMP(\"2017-01-02 12:00:00+00\") UNION ALL SELECT TIMESTAMP(\"2017-05-26 12:00:00+00\") ) SELECT timestamp_value,\nEXTRACT(ISOYEAR FROM timestamp_value) AS isoyear,\nEXTRACT(ISOWEEK FROM timestamp_value) AS isoweek,\nEXTRACT(YEAR FROM timestamp_value) AS year,\nEXTRACT(WEEK FROM timestamp_value) AS week FROM Timestamps ORDER BY timestamp_value;\n\n-- Display of results may differ, depending upon the environment and time zone where this query was executed.\n/*-------------------------+---------+---------+------+------*\n| timestamp_value         | isoyear | isoweek | year | week |\n+-------------------------+---------+---------+------+------+\n| 2005-01-03 12:34:56 UTC | 2005    | 1       | 2005 | 1    |\n| 2007-12-31 12:00:00 UTC | 2008    | 1       | 2007 | 52   |\n| 2009-01-01 12:00:00 UTC | 2009    | 1       | 2009 | 0    |\n| 2009-12-31 12:00:00 UTC | 2009    | 53      | 2009 | 52   |\n| 2017-01-02 12:00:00 UTC | 2017    | 1       | 2017 | 1    |\n| 2017-05-26 12:00:00 UTC | 2017    | 21      | 2017 | 21   |\n*-------------------------+---------+---------+------+------*/\n\nIn the following example, ` timestamp_expression ` falls on a Monday. `\nEXTRACT ` calculates the first column using weeks that begin on Sunday, and it calculates the second column using weeks that begin on Monday.\n\n\nWITH table AS (SELECT TIMESTAMP(\"2017-11-05 00:00:00+00\") AS timestamp_value) SELECT timestamp_value,\nEXTRACT(WEEK(SUNDAY) FROM timestamp_value) AS week_sunday,\nEXTRACT(WEEK(MONDAY) FROM timestamp_value) AS week_monday FROM table;\n\n-- Display of results may differ, depending upon the environment and time zone where this query was executed.\n/*-------------------------+-------------+---------------*\n| timestamp_value         | week_sunday | week_monday   |\n+-------------------------+-------------+---------------+\n| 2017-11-05 00:00:00 UTC | 45          | 44            |\n*-------------------------+-------------+---------------*/"
            },
            "FORMAT_TIMESTAMP": {
                "name": "FORMAT_TIMESTAMP",
                "summary": "Formats a ` TIMESTAMP ` value according to the specified format string.",
                "description": "FORMAT_TIMESTAMP(format_string, timestamp[, time_zone])\n\n**Description**\n\nFormats a timestamp according to the specified ` format_string ` .\n\nSee [ Format elements for date and time parts\n](/bigquery/docs/reference/standard-sql/format-\nelements#format_elements_date_time) for a list of format elements that this function supports.\n\n**Return Data Type**\n\n` STRING `\n\n**Example**\n\n\nSELECT FORMAT_TIMESTAMP(\"%c\", TIMESTAMP \"2050-12-25 15:30:55+00\", \"UTC\") AS formatted;\n\n/*--------------------------*\n| formatted                |\n+--------------------------+\n| Sun Dec 25 15:30:55 2050 |\n*--------------------------*/\n\n\nSELECT FORMAT_TIMESTAMP(\"%b-%d-%Y\", TIMESTAMP \"2050-12-25 15:30:55+00\") AS formatted;\n\n/*-------------*\n| formatted   |\n+-------------+\n| Dec-25-2050 |\n*-------------*/\n\n\nSELECT FORMAT_TIMESTAMP(\"%b %Y\", TIMESTAMP \"2050-12-25 15:30:55+00\") AS formatted;\n\n/*-------------*\n| formatted   |\n+-------------+\n| Dec 2050    |\n*-------------*/\n\n\nSELECT FORMAT_TIMESTAMP(\"%Y-%m-%dT%H:%M:%SZ\", TIMESTAMP \"2050-12-25 15:30:55\", \"UTC\") AS formatted;\n\n/*+---------------------*\n|      formatted       |\n+----------------------+\n| 2050-12-25T15:30:55Z |\n*----------------------*/"
            },
            "PARSE_TIMESTAMP": {
                "name": "PARSE_TIMESTAMP",
                "summary": "Converts a ` STRING ` value to a ` TIMESTAMP ` value.",
                "description": "PARSE_TIMESTAMP(format_string, timestamp_string[, time_zone])\n\n**Description**\n\nConverts a  string representation of a timestamp  to a ` TIMESTAMP ` object.\n\n` format_string ` contains the [ format elements\n](/bigquery/docs/reference/standard-sql/format-\nelements#format_elements_date_time) that define how ` timestamp_string ` is formatted. Each element in ` timestamp_string ` must have a corresponding element in ` format_string ` . The location of each element in ` format_string\n` must match the location of each element in ` timestamp_string ` .\n\n\n-- This works because elements on both sides match.\nSELECT PARSE_TIMESTAMP(\"%a %b %e %I:%M:%S %Y\", \"Thu Dec 25 07:30:00 2008\");\n\n-- This produces an error because the year element is in different locations.\nSELECT PARSE_TIMESTAMP(\"%a %b %e %Y %I:%M:%S\", \"Thu Dec 25 07:30:00 2008\");\n\n-- This produces an error because one of the year elements is missing.\nSELECT PARSE_TIMESTAMP(\"%a %b %e %I:%M:%S\", \"Thu Dec 25 07:30:00 2008\");\n\n-- This works because %c can find all matching elements in timestamp_string.\nSELECT PARSE_TIMESTAMP(\"%c\", \"Thu Dec 25 07:30:00 2008\");\n\nThe format string fully supports most format elements, except for ` %P ` .\n\nWhen using ` PARSE_TIMESTAMP ` , keep the following in mind:\n\n* **Unspecified fields.** Any unspecified field is initialized from ` 1970-01-01 00:00:00.0 ` . This initialization value uses the time zone specified by the function's time zone argument, if present. If not, the initialization value uses the default time zone, UTC. For instance, if the year is unspecified then it defaults to ` 1970 ` , and so on.\n* **Case insensitivity.** Names, such as ` Monday ` , ` February ` , and so on, are case insensitive.\n* **Whitespace.** One or more consecutive white spaces in the format string matches zero or more consecutive white spaces in the timestamp string. In addition, leading and trailing white spaces in the timestamp string are always allowed, even if they are not in the format string.\n* **Format precedence.** When two (or more) format elements have overlapping information (for example both ` %F ` and ` %Y ` affect the year), the last one generally overrides any earlier ones, with some exceptions (see the descriptions of ` %s ` , ` %C ` , and ` %y ` ).\n* **Format divergence.** ` %p ` can be used with ` am ` , ` AM ` , ` pm ` , and ` PM ` .\n\n**Return Data Type**\n\n` TIMESTAMP `\n\n**Example**\n\n\nSELECT PARSE_TIMESTAMP(\"%c\", \"Thu Dec 25 07:30:00 2008\") AS parsed;\n\n-- Display of results may differ, depending upon the environment and time zone where this query was executed.\n/*-------------------------*\n| parsed                  |\n+-------------------------+\n| 2008-12-25 07:30:00 UTC |\n*-------------------------*/"
            },
            "STRING": {
                "name": "STRING",
                "summary": "Converts a ` TIMESTAMP ` value to a ` STRING ` value.",
                "description": "STRING(timestamp_expression[, time_zone])\n\n**Description**\n\nConverts a timestamp to a string. Supports an optional parameter to specify a time zone. See  Time zone definitions  for information on how to specify a time zone.\n\n**Return Data Type**\n\n` STRING `\n\n**Example**\n\n\nSELECT STRING(TIMESTAMP \"2008-12-25 15:30:00+00\", \"UTC\") AS string;\n\n/*-------------------------------*\n| string                        |\n+-------------------------------+\n| 2008-12-25 15:30:00+00        |\n*-------------------------------*/"
            },
            "TIMESTAMP": {
                "name": "TIMESTAMP",
                "summary": "Constructs a ` TIMESTAMP ` value.",
                "description": "TIMESTAMP(string_expression[, time_zone]) TIMESTAMP(date_expression[, time_zone]) TIMESTAMP(datetime_expression[, time_zone])\n\n**Description**\n\n* ` string_expression[, time_zone] ` : Converts a string to a timestamp. ` string_expression ` must include a timestamp literal. If ` string_expression ` includes a time zone in the timestamp literal, do not include an explicit ` time_zone ` argument.\n* ` date_expression[, time_zone] ` : Converts a date to a timestamp. The value returned is the earliest timestamp that falls within the given date.\n* ` datetime_expression[, time_zone] ` : Converts a datetime to a timestamp.\n\nThis function supports an optional parameter to  specify a time zone  . If no time zone is specified, the default time zone, UTC, is used.\n\n**Return Data Type**\n\n` TIMESTAMP `\n\n**Examples**\n\n\nSELECT TIMESTAMP(\"2008-12-25 15:30:00+00\") AS timestamp_str;\n\n-- Display of results may differ, depending upon the environment and time zone where this query was executed.\n/*-------------------------*\n| timestamp_str           |\n+-------------------------+\n| 2008-12-25 15:30:00 UTC |\n*-------------------------*/\n\n\nSELECT TIMESTAMP(\"2008-12-25 15:30:00\", \"America/Los_Angeles\") AS timestamp_str;\n\n-- Display of results may differ, depending upon the environment and time zone where this query was executed.\n/*-------------------------*\n| timestamp_str           |\n+-------------------------+\n| 2008-12-25 23:30:00 UTC |\n*-------------------------*/\n\n\nSELECT TIMESTAMP(\"2008-12-25 15:30:00 UTC\") AS timestamp_str;\n\n-- Display of results may differ, depending upon the environment and time zone where this query was executed.\n/*-------------------------*\n| timestamp_str           |\n+-------------------------+\n| 2008-12-25 15:30:00 UTC |\n*-------------------------*/\n\n\nSELECT TIMESTAMP(DATETIME \"2008-12-25 15:30:00\") AS timestamp_datetime;\n\n-- Display of results may differ, depending upon the environment and time zone where this query was executed.\n/*-------------------------*\n| timestamp_datetime      |\n+-------------------------+\n| 2008-12-25 15:30:00 UTC |\n*-------------------------*/\n\n\nSELECT TIMESTAMP(DATE \"2008-12-25\") AS timestamp_date;\n\n-- Display of results may differ, depending upon the environment and time zone where this query was executed.\n/*-------------------------*\n| timestamp_date          |\n+-------------------------+\n| 2008-12-25 00:00:00 UTC |\n*-------------------------*/"
            },
            "TIMESTAMP_ADD": {
                "name": "TIMESTAMP_ADD",
                "summary": "Adds a specified time interval to a ` TIMESTAMP ` value.",
                "description": "TIMESTAMP_ADD(timestamp_expression, INTERVAL int64_expression date_part)\n\n**Description**\n\nAdds ` int64_expression ` units of ` date_part ` to the timestamp, independent of any time zone.\n\n` TIMESTAMP_ADD ` supports the following values for ` date_part ` :\n\n* ` MICROSECOND `\n* ` MILLISECOND `\n* ` SECOND `\n* ` MINUTE `\n* ` HOUR ` . Equivalent to 60 ` MINUTE ` parts.\n* ` DAY ` . Equivalent to 24 ` HOUR ` parts.\n\n**Return Data Types**\n\n` TIMESTAMP `\n\n**Example**\n\n\nSELECT TIMESTAMP(\"2008-12-25 15:30:00+00\") AS original,\nTIMESTAMP_ADD(TIMESTAMP \"2008-12-25 15:30:00+00\", INTERVAL 10 MINUTE) AS later;\n\n-- Display of results may differ, depending upon the environment and time zone where this query was executed.\n/*-------------------------+-------------------------*\n| original                | later                   |\n+-------------------------+-------------------------+\n| 2008-12-25 15:30:00 UTC | 2008-12-25 15:40:00 UTC |\n*-------------------------+-------------------------*/"
            },
            "TIMESTAMP_DIFF": {
                "name": "TIMESTAMP_DIFF",
                "summary": "Gets the number of unit boundaries between two `\nTIMESTAMP ` values at a particular time granularity.",
                "description": "TIMESTAMP_DIFF(start_timestamp, end_timestamp, granularity)\n\n**Description**\n\nGets the number of unit boundaries between two ` TIMESTAMP ` values ( `\nstart_timestamp ` \\- ` end_timestamp ` ) at a particular time granularity.\n\n**Definitions**\n\n* ` start_timestamp ` : The starting ` TIMESTAMP ` value.\n* ` end_timestamp ` : The ending ` TIMESTAMP ` value.\n* ` granularity ` : The timestamp part that represents the granularity. This can be:\n\n* ` MICROSECOND `\n* ` MILLISECOND `\n* ` SECOND `\n* ` MINUTE `\n* ` HOUR ` . Equivalent to 60 ` MINUTE ` s.\n* ` DAY ` . Equivalent to 24 ` HOUR ` s.\n\n**Details**\n\nIf ` end_timestamp ` is earlier than ` start_timestamp ` , the output is negative. Produces an error if the computation overflows, such as if the difference in microseconds between the two ` TIMESTAMP ` values overflows.\n\n**Note:** The behavior of the this function follows the type of arguments passed in. For example, ` TIMESTAMP_DIFF(DATE, DATE, PART) ` behaves like `\nDATE_DIFF(DATE, DATE, PART) ` .\n\n**Return Data Type**\n\n` INT64 `\n\n**Example**\n\n\nSELECT TIMESTAMP(\"2010-07-07 10:20:00+00\") AS later_timestamp,\nTIMESTAMP(\"2008-12-25 15:30:00+00\") AS earlier_timestamp,\nTIMESTAMP_DIFF(TIMESTAMP \"2010-07-07 10:20:00+00\", TIMESTAMP \"2008-12-25 15:30:00+00\", HOUR) AS hours;\n\n-- Display of results may differ, depending upon the environment and time zone where this query was executed.\n/*-------------------------+-------------------------+-------*\n| later_timestamp         | earlier_timestamp       | hours |\n+-------------------------+-------------------------+-------+\n| 2010-07-07 10:20:00 UTC | 2008-12-25 15:30:00 UTC | 13410 |\n*-------------------------+-------------------------+-------*/\n\nIn the following example, the first timestamp occurs before the second timestamp, resulting in a negative output.\n\n\nSELECT TIMESTAMP_DIFF(TIMESTAMP \"2018-08-14\", TIMESTAMP \"2018-10-14\", DAY) AS negative_diff;\n\n/*---------------*\n| negative_diff |\n+---------------+\n| -61           |\n*---------------*/\n\nIn this example, the result is 0 because only the number of whole specified `\nHOUR ` intervals are included.\n\n\nSELECT TIMESTAMP_DIFF(\"2001-02-01 01:00:00\", \"2001-02-01 00:00:01\", HOUR) AS diff;\n\n/*---------------*\n| diff          |\n+---------------+\n| 0             |\n*---------------*/"
            },
            "TIMESTAMP_MICROS": {
                "name": "TIMESTAMP_MICROS",
                "summary": "Converts the number of microseconds since 1970-01-01 00:00:00 UTC to a ` TIMESTAMP . `",
                "description": "TIMESTAMP_MICROS(int64_expression)\n\n**Description**\n\nInterprets ` int64_expression ` as the number of microseconds since 1970-01-01 00:00:00 UTC and returns a timestamp.\n\n**Return Data Type**\n\n` TIMESTAMP `\n\n**Example**\n\n\nSELECT TIMESTAMP_MICROS(1230219000000000) AS timestamp_value;\n\n-- Display of results may differ, depending upon the environment and time zone where this query was executed.\n/*-------------------------*\n| timestamp_value         |\n+-------------------------+\n| 2008-12-25 15:30:00 UTC |\n*-------------------------*/"
            },
            "TIMESTAMP_MILLIS": {
                "name": "TIMESTAMP_MILLIS",
                "summary": "Converts the number of milliseconds since 1970-01-01 00:00:00 UTC to a ` TIMESTAMP . `",
                "description": "TIMESTAMP_MILLIS(int64_expression)\n\n**Description**\n\nInterprets ` int64_expression ` as the number of milliseconds since 1970-01-01 00:00:00 UTC and returns a timestamp.\n\n**Return Data Type**\n\n` TIMESTAMP `\n\n**Example**\n\n\nSELECT TIMESTAMP_MILLIS(1230219000000) AS timestamp_value;\n\n-- Display of results may differ, depending upon the environment and time zone where this query was executed.\n/*-------------------------*\n| timestamp_value         |\n+-------------------------+\n| 2008-12-25 15:30:00 UTC |\n*-------------------------*/"
            },
            "TIMESTAMP_SECONDS": {
                "name": "TIMESTAMP_SECONDS",
                "summary": "Converts the number of seconds since 1970-01-01 00:00:00 UTC to a ` TIMESTAMP . `",
                "description": "TIMESTAMP_SECONDS(int64_expression)\n\n**Description**\n\nInterprets ` int64_expression ` as the number of seconds since 1970-01-01 00:00:00 UTC and returns a timestamp.\n\n**Return Data Type**\n\n` TIMESTAMP `\n\n**Example**\n\n\nSELECT TIMESTAMP_SECONDS(1230219000) AS timestamp_value;\n\n-- Display of results may differ, depending upon the environment and time zone where this query was executed.\n/*-------------------------*\n| timestamp_value         |\n+-------------------------+\n| 2008-12-25 15:30:00 UTC |\n*-------------------------*/"
            },
            "TIMESTAMP_SUB": {
                "name": "TIMESTAMP_SUB",
                "summary": "Subtracts a specified time interval from a ` TIMESTAMP `\nvalue.",
                "description": "TIMESTAMP_SUB(timestamp_expression, INTERVAL int64_expression date_part)\n\n**Description**\n\nSubtracts ` int64_expression ` units of ` date_part ` from the timestamp,\nindependent of any time zone.\n\n` TIMESTAMP_SUB ` supports the following values for ` date_part ` :\n\n* ` MICROSECOND `\n* ` MILLISECOND `\n* ` SECOND `\n* ` MINUTE `\n* ` HOUR ` . Equivalent to 60 ` MINUTE ` parts.\n* ` DAY ` . Equivalent to 24 ` HOUR ` parts.\n\n**Return Data Type**\n\n` TIMESTAMP `\n\n**Example**\n\n\nSELECT TIMESTAMP(\"2008-12-25 15:30:00+00\") AS original,\nTIMESTAMP_SUB(TIMESTAMP \"2008-12-25 15:30:00+00\", INTERVAL 10 MINUTE) AS earlier;\n\n-- Display of results may differ, depending upon the environment and time zone where this query was executed.\n/*-------------------------+-------------------------*\n| original                | earlier                 |\n+-------------------------+-------------------------+\n| 2008-12-25 15:30:00 UTC | 2008-12-25 15:20:00 UTC |\n*-------------------------+-------------------------*/"
            },
            "TIMESTAMP_TRUNC": {
                "name": "TIMESTAMP_TRUNC",
                "summary": "Truncates a ` TIMESTAMP ` value.",
                "description": "TIMESTAMP_TRUNC(timestamp_expression, date_time_part[, time_zone])\n\n**Description**\n\nTruncates a timestamp to the granularity of ` date_time_part ` . The timestamp is always rounded to the beginning of ` date_time_part ` , which can be one of the following:\n\n* ` MICROSECOND ` : If used, nothing is truncated from the value.\n* ` MILLISECOND ` : The nearest lessor or equal millisecond.\n* ` SECOND ` : The nearest lessor or equal second.\n* ` MINUTE ` : The nearest lessor or equal minute.\n* ` HOUR ` : The nearest lessor or equal hour.\n* ` DAY ` : The day in the Gregorian calendar year that contains the ` TIMESTAMP ` value.\n* ` WEEK ` : The first day of the week in the week that contains the ` TIMESTAMP ` value. Weeks begin on Sundays. ` WEEK ` is equivalent to ` WEEK(SUNDAY) ` .\n* ` WEEK(WEEKDAY) ` : The first day of the week in the week that contains the ` TIMESTAMP ` value. Weeks begin on ` WEEKDAY ` . ` WEEKDAY ` must be one of the following: ` SUNDAY ` , ` MONDAY ` , ` TUESDAY ` , ` WEDNESDAY ` , ` THURSDAY ` , ` FRIDAY ` , or ` SATURDAY ` .\n* ` ISOWEEK ` : The first day of the [ ISO 8601 week ](https://en.wikipedia.org/wiki/ISO_week_date) in the ISO week that contains the ` TIMESTAMP ` value. The ISO week begins on Monday. The first ISO week of each ISO year contains the first Thursday of the corresponding Gregorian calendar year.\n* ` MONTH ` : The first day of the month in the month that contains the ` TIMESTAMP ` value.\n* ` QUARTER ` : The first day of the quarter in the quarter that contains the ` TIMESTAMP ` value.\n* ` YEAR ` : The first day of the year in the year that contains the ` TIMESTAMP ` value.\n* ` ISOYEAR ` : The first day of the [ ISO 8601 ](https://en.wikipedia.org/wiki/ISO_8601) week-numbering year in the ISO year that contains the ` TIMESTAMP ` value. The ISO year is the Monday of the first week whose Thursday belongs to the corresponding Gregorian calendar year.\n\n` TIMESTAMP_TRUNC ` function supports an optional ` time_zone ` parameter.\nThis parameter applies to the following ` date_time_part ` :\n\n* ` MINUTE `\n* ` HOUR `\n* ` DAY `\n* ` WEEK `\n* ` WEEK(<WEEKDAY>) `\n* ` ISOWEEK `\n* ` MONTH `\n* ` QUARTER `\n* ` YEAR `\n* ` ISOYEAR `\n\nUse this parameter if you want to use a time zone other than the default time zone, UTC, as part of the truncate operation.\n\nWhen truncating a timestamp to ` MINUTE ` or ` HOUR ` parts, ` TIMESTAMP_TRUNC\n` determines the civil time of the timestamp in the specified (or default) time zone and subtracts the minutes and seconds (when truncating to ` HOUR ` ) or the seconds (when truncating to ` MINUTE ` ) from that timestamp. While this provides intuitive results in most cases, the result is non-intuitive near daylight savings transitions that are not hour-aligned.\n\n**Return Data Type**\n\n` TIMESTAMP `\n\n**Examples**\n\n\nSELECT TIMESTAMP_TRUNC(TIMESTAMP \"2008-12-25 15:30:00+00\", DAY, \"UTC\") AS utc,\nTIMESTAMP_TRUNC(TIMESTAMP \"2008-12-25 15:30:00+00\", DAY, \"America/Los_Angeles\") AS la;\n\n-- Display of results may differ, depending upon the environment and time zone where this query was executed.\n/*-------------------------+-------------------------*\n| utc                     | la                      |\n+-------------------------+-------------------------+\n| 2008-12-25 00:00:00 UTC | 2008-12-25 08:00:00 UTC |\n*-------------------------+-------------------------*/\n\nIn the following example, ` timestamp_expression ` has a time zone offset of\n+12. The first column shows the ` timestamp_expression ` in UTC time. The second column shows the output of ` TIMESTAMP_TRUNC ` using weeks that start on Monday. Because the ` timestamp_expression ` falls on a Sunday in UTC, `\nTIMESTAMP_TRUNC ` truncates it to the preceding Monday. The third column shows the same function with the optional  Time zone definition  argument\n'Pacific/Auckland'. Here, the function truncates the ` timestamp_expression `\nusing New Zealand Daylight Time, where it falls on a Monday.\n\n\nSELECT timestamp_value AS timestamp_value,\nTIMESTAMP_TRUNC(timestamp_value, WEEK(MONDAY), \"UTC\") AS utc_truncated,\nTIMESTAMP_TRUNC(timestamp_value, WEEK(MONDAY), \"Pacific/Auckland\") AS nzdt_truncated FROM (SELECT TIMESTAMP(\"2017-11-06 00:00:00+12\") AS timestamp_value);\n\n-- Display of results may differ, depending upon the environment and time zone where this query was executed.\n/*-------------------------+-------------------------+-------------------------*\n| timestamp_value         | utc_truncated           | nzdt_truncated          |\n+-------------------------+-------------------------+-------------------------+\n| 2017-11-05 12:00:00 UTC | 2017-10-30 00:00:00 UTC | 2017-11-05 11:00:00 UTC |\n*-------------------------+-------------------------+-------------------------*/\n\nIn the following example, the original ` timestamp_expression ` is in the Gregorian calendar year 2015. However, ` TIMESTAMP_TRUNC ` with the ` ISOYEAR\n` date part truncates the ` timestamp_expression ` to the beginning of the ISO year, not the Gregorian calendar year. The first Thursday of the 2015 calendar year was 2015-01-01, so the ISO year 2015 begins on the preceding Monday,\n2014-12-29. Therefore the ISO year boundary preceding the `\ntimestamp_expression ` 2015-06-15 00:00:00+00 is 2014-12-29.\n\n\nSELECT TIMESTAMP_TRUNC(\"2015-06-15 00:00:00+00\", ISOYEAR) AS isoyear_boundary,\nEXTRACT(ISOYEAR FROM TIMESTAMP \"2015-06-15 00:00:00+00\") AS isoyear_number;\n\n-- Display of results may differ, depending upon the environment and time zone where this query was executed.\n/*-------------------------+----------------*\n| isoyear_boundary        | isoyear_number |\n+-------------------------+----------------+\n| 2014-12-29 00:00:00 UTC | 2015           |\n*-------------------------+----------------*/"
            },
            "UNIX_MICROS": {
                "name": "UNIX_MICROS",
                "summary": "Converts a ` TIMESTAMP ` value to the number of microseconds since 1970-01-01 00:00:00 UTC.",
                "description": "UNIX_MICROS(timestamp_expression)\n\n**Description**\n\nReturns the number of microseconds since ` 1970-01-01 00:00:00 UTC ` .\n\n**Return Data Type**\n\n` INT64 `\n\n**Examples**\n\n\nSELECT UNIX_MICROS(TIMESTAMP \"2008-12-25 15:30:00+00\") AS micros;\n\n/*------------------*\n| micros           |\n+------------------+\n| 1230219000000000 |\n*------------------*/"
            },
            "UNIX_MILLIS": {
                "name": "UNIX_MILLIS",
                "summary": "Converts a ` TIMESTAMP ` value to the number of milliseconds since 1970-01-01 00:00:00 UTC.",
                "description": "UNIX_MILLIS(timestamp_expression)\n\n**Description**\n\nReturns the number of milliseconds since ` 1970-01-01 00:00:00 UTC ` .\nTruncates higher levels of precision by rounding down to the beginning of the millisecond.\n\n**Return Data Type**\n\n` INT64 `\n\n**Examples**\n\n\nSELECT UNIX_MILLIS(TIMESTAMP \"2008-12-25 15:30:00+00\") AS millis;\n\n/*---------------*\n| millis        |\n+---------------+\n| 1230219000000 |\n*---------------*/\n\n\nSELECT UNIX_MILLIS(TIMESTAMP \"1970-01-01 00:00:00.0018+00\") AS millis;\n\n/*---------------*\n| millis        |\n+---------------+\n| 1             |\n*---------------*/"
            },
            "UNIX_SECONDS": {
                "name": "UNIX_SECONDS",
                "summary": "Converts a ` TIMESTAMP ` value to the number of seconds since 1970-01-01 00:00:00 UTC.",
                "description": "UNIX_SECONDS(timestamp_expression)\n\n**Description**\n\nReturns the number of seconds since ` 1970-01-01 00:00:00 UTC ` . Truncates higher levels of precision by rounding down to the beginning of the second.\n\n**Return Data Type**\n\n` INT64 `\n\n**Examples**\n\n\nSELECT UNIX_SECONDS(TIMESTAMP \"2008-12-25 15:30:00+00\") AS seconds;\n\n/*------------*\n| seconds    |\n+------------+\n| 1230219000 |\n*------------*/\n\n\nSELECT UNIX_SECONDS(TIMESTAMP \"1970-01-01 00:00:01.8+00\") AS seconds;\n\n/*------------*\n| seconds    |\n+------------+\n| 1          |\n*------------*/"
            }
        }
    },
    {
        "category": "utility-functions",
        "description": "GoogleSQL for BigQuery supports the following utility functions.",
        "source": "utility-functions.txt",
        "functions": {
            "GENERATE_UUID": {
                "name": "GENERATE_UUID",
                "summary": "Produces a random universally unique identifier (UUID) as a ` STRING ` value.",
                "description": "GENERATE_UUID()\n\n**Description**\n\nReturns a random universally unique identifier (UUID) as a ` STRING ` . The returned ` STRING ` consists of 32 hexadecimal digits in five groups separated by hyphens in the form 8-4-4-4-12. The hexadecimal digits represent 122 random bits and 6 fixed bits, in compliance with [ RFC 4122 section 4.4\n](https://tools.ietf.org/html/rfc4122#section-4.4) . The returned ` STRING `\nis lowercase.\n\n**Return Data Type**\n\nSTRING\n\n**Example**\n\nThe following query generates a random UUID.\n\n\nSELECT GENERATE_UUID() AS uuid;\n\n/*--------------------------------------*\n| uuid                                 |\n+--------------------------------------+\n| 4192bff0-e1e0-43ce-a4db-912808c32493 |\n*--------------------------------------*/"
            }
        }
    },
    {
        "category": "conditional-functions",
        "description": "GoogleSQL for BigQuery supports conditional expressions. Conditional expressions impose constraints on the evaluation order of their inputs. In essence, they are evaluated left to right, with short-circuiting, and only evaluate the output value that was chosen. In contrast, all inputs to regular functions are evaluated before calling the function. Short-circuiting in conditional expressions can be exploited for error handling or performance tuning.",
        "source": "conditional_expressions.txt",
        "functions": {
            "CASE_EXPR": {
                "name": "CASE_EXPR",
                "summary": "Compares the given expression to each successive WHEN clause and produces the first result where the values are equal.",
                "description": "CASE expr WHEN expr_to_match THEN result\n[ ... ]\n[ ELSE else_result ]\nEND\n\n**Description**\n\nCompares ` expr ` to ` expr_to_match ` of each successive ` WHEN ` clause and returns the first result where this comparison evaluates to ` TRUE ` . The remaining ` WHEN ` clauses and ` else_result ` aren't evaluated.\n\nIf the ` expr = expr_to_match ` comparison evaluates to ` FALSE ` or ` NULL `\nfor all ` WHEN ` clauses, returns the evaluation of ` else_result ` if present; if ` else_result ` isn't present, then returns ` NULL ` .\n\nConsistent with [ equality comparisons ](/bigquery/docs/reference/standard-\nsql/operators#logical_operators) elsewhere, if both ` expr ` and `\nexpr_to_match ` are ` NULL ` , then ` expr = expr_to_match ` evaluates to `\nNULL ` , which returns ` else_result ` . If a CASE statement needs to distinguish a ` NULL ` value, then the alternate  CASE  syntax should be used.\n\n` expr ` and ` expr_to_match ` can be any type. They must be implicitly coercible to a common [ supertype ](/bigquery/docs/reference/standard-\nsql/conversion_rules#supertypes) ; equality comparisons are done on coerced values. There may be multiple ` result ` types. ` result ` and ` else_result `\nexpressions must be coercible to a common supertype.\n\nThis expression supports specifying [ collation\n](/bigquery/docs/reference/standard-sql/collation-concepts#collate_about) .\n\n**Return Data Type**\n\n[ Supertype ](/bigquery/docs/reference/standard-\nsql/conversion_rules#supertypes) of ` result ` [, ...] and ` else_result ` .\n\n**Example**\n\n\nWITH Numbers AS ( SELECT 90 as A, 2 as B UNION ALL SELECT 50, 8 UNION ALL SELECT 60, 6 UNION ALL SELECT 50, 10 ) SELECT A,\nB,\nCASE A WHEN 90 THEN 'red'\nWHEN 50 THEN 'blue'\nELSE 'green'\nEND AS result FROM Numbers\n\n/*------------------*\n| A  | B  | result |\n+------------------+\n| 90 | 2  | red    |\n| 50 | 8  | blue   |\n| 60 | 6  | green  |\n| 50 | 10 | blue   |\n*------------------*/"
            },
            "CASE": {
                "name": "CASE",
                "summary": "Evaluates the condition of each successive WHEN clause and produces the first result where the condition evaluates to TRUE.",
                "description": "CASE WHEN condition THEN result\n[ ... ]\n[ ELSE else_result ]\nEND\n\n**Description**\n\nEvaluates the condition of each successive ` WHEN ` clause and returns the first result where the condition evaluates to ` TRUE ` ; any remaining ` WHEN\n` clauses and ` else_result ` aren't evaluated.\n\nIf all conditions evaluate to ` FALSE ` or ` NULL ` , returns evaluation of `\nelse_result ` if present; if ` else_result ` isn't present, then returns `\nNULL ` .\n\nFor additional rules on how values are evaluated, see the three-valued logic table in [ Logical operators ](/bigquery/docs/reference/standard-\nsql/operators#logical_operators) .\n\n` condition ` must be a boolean expression. There may be multiple ` result `\ntypes. ` result ` and ` else_result ` expressions must be implicitly coercible to a common [ supertype ](/bigquery/docs/reference/standard-\nsql/conversion_rules#supertypes) .\n\nThis expression supports specifying [ collation\n](/bigquery/docs/reference/standard-sql/collation-concepts#collate_about) .\n\n**Return Data Type**\n\n[ Supertype ](/bigquery/docs/reference/standard-\nsql/conversion_rules#supertypes) of ` result ` [, ...] and ` else_result ` .\n\n**Example**\n\n\nWITH Numbers AS ( SELECT 90 as A, 2 as B UNION ALL SELECT 50, 6 UNION ALL SELECT 20, 10 ) SELECT A,\nB,\nCASE WHEN A > 60 THEN 'red'\nWHEN B = 6 THEN 'blue'\nELSE 'green'\nEND AS result FROM Numbers\n\n/*------------------*\n| A  | B  | result |\n+------------------+\n| 90 | 2  | red    |\n| 50 | 6  | blue   |\n| 20 | 10 | green  |\n*------------------*/"
            },
            "COALESCE": {
                "name": "COALESCE",
                "summary": "Produces the value of the first non-NULL expression, if any, otherwise NULL.",
                "description": "COALESCE(expr[, ...])\n\n**Description**\n\nReturns the value of the first non- ` NULL ` expression, if any, otherwise `\nNULL ` . The remaining expressions aren't evaluated. An input expression can be any type. There may be multiple input expression types. All input expressions must be implicitly coercible to a common [ supertype\n](/bigquery/docs/reference/standard-sql/conversion_rules#supertypes) .\n\n**Return Data Type**\n\n[ Supertype ](/bigquery/docs/reference/standard-\nsql/conversion_rules#supertypes) of ` expr ` [, ...].\n\n**Examples**\n\n\nSELECT COALESCE('A', 'B', 'C') as result\n\n/*--------*\n| result |\n+--------+\n| A      |\n*--------*/\n\n\nSELECT COALESCE(NULL, 'B', 'C') as result\n\n/*--------*\n| result |\n+--------+\n| B      |\n*--------*/"
            },
            "IF": {
                "name": "IF",
                "summary": "If an expression evaluates to TRUE, produces a specified result, otherwise produces the evaluation for an else result.",
                "description": "IF(expr, true_result, else_result)\n\n**Description**\n\nIf ` expr ` evaluates to ` TRUE ` , returns ` true_result ` , else returns the evaluation for ` else_result ` . ` else_result ` isn't evaluated if ` expr `\nevaluates to ` TRUE ` . ` true_result ` isn't evaluated if ` expr ` evaluates to ` FALSE ` or ` NULL ` .\n\n` expr ` must be a boolean expression. ` true_result ` and ` else_result `\nmust be coercible to a common [ supertype ](/bigquery/docs/reference/standard-\nsql/conversion_rules#supertypes) .\n\n**Return Data Type**\n\n[ Supertype ](/bigquery/docs/reference/standard-\nsql/conversion_rules#supertypes) of ` true_result ` and ` else_result ` .\n\n**Example**\n\n\nWITH Numbers AS ( SELECT 10 as A, 20 as B UNION ALL SELECT 50, 30 UNION ALL SELECT 60, 60 ) SELECT A,\nB,\nIF(A < B, 'true', 'false') AS result FROM Numbers\n\n/*------------------*\n| A  | B  | result |\n+------------------+\n| 10 | 20 | true   |\n| 50 | 30 | false  |\n| 60 | 60 | false  |\n*------------------*/"
            },
            "IFNULL": {
                "name": "IFNULL",
                "summary": "If an expression evaluates to NULL, produces a specified result, otherwise produces the expression.",
                "description": "IFNULL(expr, null_result)\n\n**Description**\n\nIf ` expr ` evaluates to ` NULL ` , returns ` null_result ` . Otherwise,\nreturns ` expr ` . If ` expr ` doesn't evaluate to ` NULL ` , ` null_result `\nisn't evaluated.\n\n` expr ` and ` null_result ` can be any type and must be implicitly coercible to a common [ supertype ](/bigquery/docs/reference/standard-\nsql/conversion_rules#supertypes) . Synonym for ` COALESCE(expr, null_result) `\n.\n\n**Return Data Type**\n\n[ Supertype ](/bigquery/docs/reference/standard-\nsql/conversion_rules#supertypes) of ` expr ` or ` null_result ` .\n\n**Examples**\n\n\nSELECT IFNULL(NULL, 0) as result\n\n/*--------*\n| result |\n+--------+\n| 0      |\n*--------*/\n\n\nSELECT IFNULL(10, 0) as result\n\n/*--------*\n| result |\n+--------+\n| 10     |\n*--------*/"
            },
            "NULLIF": {
                "name": "NULLIF",
                "summary": "Produces NULL if the first expression that matches another evaluates to TRUE, otherwise returns the first expression.",
                "description": "NULLIF(expr, expr_to_match)\n\n**Description**\n\nReturns ` NULL ` if ` expr = expr_to_match ` evaluates to ` TRUE ` , otherwise returns ` expr ` .\n\n` expr ` and ` expr_to_match ` must be implicitly coercible to a common [\nsupertype ](/bigquery/docs/reference/standard-sql/conversion_rules#supertypes)\n, and must be comparable.\n\nThis expression supports specifying [ collation\n](/bigquery/docs/reference/standard-sql/collation-concepts#collate_about) .\n\n**Return Data Type**\n\n[ Supertype ](/bigquery/docs/reference/standard-\nsql/conversion_rules#supertypes) of ` expr ` and ` expr_to_match ` .\n\n**Example**\n\n\nSELECT NULLIF(0, 0) as result\n\n/*--------*\n| result |\n+--------+\n| NULL   |\n*--------*/\n\n\nSELECT NULLIF(10, 0) as result\n\n/*--------*\n| result |\n+--------+\n| 10     |\n*--------*/"
            }
        }
    },
    {
        "category": "other-functions",
        "description": "This category introduces some common functions, operators, syntax, expressions or statements that are very popular in Google BigQuery.",
        "source": "other_functions.txt",
        "functions": {
            "CREATE_FUNCTION": {
                "name": "CREATE_FUNCTION",
                "summary": "Creates a new user-defined function (UDF). BigQuery supports UDFs written in either SQL or JavaScript.",
                "description": "Creates a new [ user-defined function ](/bigquery/docs/user-defined-functions) (UDF). BigQuery supports UDFs written in either SQL or JavaScript.\n\n####  Syntax\n\nTo create a SQL UDF, use the following syntax:\n\n\nCREATE [ OR REPLACE ] [ TEMPORARY | TEMP ] FUNCTION [ IF NOT EXISTS ]\n[[project_name.]dataset_name.]function_name ([named_parameter[, ...]]) ([named_parameter[, ...]])\n[RETURNS data_type]\nAS (sql_expression)\n[OPTIONS (function_option_list)]\n\nnamed_parameter:\nparam_name param_type\n\nTo create a JavaScript UDF, use the following syntax:\n\n\nCREATE [OR REPLACE] [TEMPORARY | TEMP] FUNCTION [IF NOT EXISTS]\n[[project_name.]dataset_name.]function_name ([named_parameter[, ...]]) RETURNS data_type\n[determinism_specifier]\nLANGUAGE js\n[OPTIONS (function_option_list)]\nAS javascript_code\n\nnamed_parameter:\nparam_name param_type\n\ndeterminism_specifier:\n{ DETERMINISTIC | NOT DETERMINISTIC }\n\nTo create a remote function, use the following syntax:\n\n\nCREATE [OR REPLACE] FUNCTION [IF NOT EXISTS]\n[[project_name.]dataset_name.]function_name ([named_parameter[, ...]]) RETURNS data_type REMOTE WITH CONNECTION connection_path\n[OPTIONS (function_option_list)]\n\nnamed_parameter:\nparam_name param_type\n\nRoutine names must contain only letters, numbers, and underscores, and be at most 256 characters long.\n\n####  Arguments\n\n* ` OR REPLACE ` : Replaces any function with the same name if it exists. Cannot appear with ` IF NOT EXISTS ` .\n\n* ` IF NOT EXISTS ` : If any dataset exists with the same name, the ` CREATE ` statement has no effect. Cannot appear with ` OR REPLACE ` .\n\n* ` TEMP ` or ` TEMPORARY ` : Creates a temporary function. If the clause is not present, the statement creates a persistent UDF. You can reuse persistent UDFs across multiple queries, whereas you can only use temporary UDFs in a single query, script, session, or procedure.\n\n* ` project_name ` : For persistent functions, the name of the project where you are creating the function. Defaults to the project that runs the DDL query. Do not include the project name for temporary functions.\n\n* ` dataset_name ` : For persistent functions, the name of the dataset where you are creating the function. Defaults to the ` defaultDataset ` in the request. Do not include the dataset name for temporary functions.\n\n* ` function_name ` : The name of the function.\n\n* ` named_parameter ` : A comma-separated ` param_name ` and ` param_type ` pair. The value of ` param_type ` is a BigQuery [ data type ](/bigquery/docs/reference/standard-sql/data-types) . For a SQL UDF, the value of ` param_type ` can also be ` ANY TYPE ` .\n\n* ` determinism_specifier ` : Applies only to JavaScript UDFs. Provides a hint to BigQuery as to whether the query result can be cached. Can be one of the following values:\n\n* ` DETERMINISTIC ` : The function always returns the same result when passed the same arguments. The query result is potentially cacheable. For example, if the function ` add_one(i) ` always returns ` i + 1 ` , the function is deterministic.\n\n* ` NOT DETERMINISTIC ` : The function does not always return the same result when passed the same arguments, and therefore is not cacheable. For example, if the functionj ` add_random(i) ` returns ` i + rand() ` , the function is not deterministic and BigQuery does not use cached results.\n\nIf all of the invoked functions are ` DETERMINISTIC ` , BigQuery tries to cache the result, unless the results can't be cached for other reasons. For more information, see [ Using cached query results ](/bigquery/docs/cached-\nresults) .\n\n* ` data_type ` : The data type that the function returns.\n\n* If the function is defined in SQL, then the ` RETURNS ` clause is optional. If the ` RETURNS ` clause is omitted, then BigQuery infers the result type of the function from the SQL function body when a query calls the function.\n* If the function is defined in JavaScript, then the ` RETURNS ` clause is required. For more information about allowed values for ` data_type ` , see [ Supported JavaScript UDF data types ](/bigquery/docs/reference/standard-sql/user-defined-functions#supported-javascript-udf-data-types) .\n* ` sql_expression ` : The SQL expression that defines the function.\n\n* ` function_option_list ` : A list of options for creating the function.\n\n* ` javascript_code ` : The definition of a JavaScript function. The value is a [ string literal ](/bigquery/docs/reference/standard-sql/lexical#string_and_bytes_literals) . If the code includes quotes and backslashes, it must be either escaped or represented as a raw string. For example, the code ` return \"\\n\"; ` can be represented as one of the following:\n\n* Quoted string ` \"return \\\"\\\\n\\\";\" ` . Both quotes and backslashes need to be escaped.\n* Triple quoted string: ` \"\"\"return \"\\\\n\";\"\"\" ` . Backslashes need to be escaped while quotes do not.\n* Raw string: ` r\"\"\"return \"\\n\";\"\"\" ` . No escaping is needed.\n* ` connection_name ` : Specifies a [ connection resource ](/bigquery/docs/connections-api-intro) that has credentials for accessing the remote endpoint. Specify the connection name in the form ` project_name.location.connection_id ` : If the project name or location contains a dash, enclose the connection name in backticks ( ` ` ` ).\n\n####  ` function_option_list `\n\nThe option list specifies options for creating a UDF. The following options are supported:\n\n` NAME ` |  ` VALUE ` |  Details\n---|---|---\n` description ` |\n\n` STRING `\n\n|  A description of the UDF.\n` library ` |\n\n` ARRAY<STRING> `\n\n|\n\nAn array of JavaScript libraries to include in the function definition.\nApplies only to JavaScript UDFs. For more information, see [ Including JavaScript libraries ](/bigquery/docs/user-defined-functions#including-\njavascript-libraries) .\n\nExample: ` [\"gs://my-bucket/lib1.js\", \"gs://my-bucket/lib2.js\"] `\n\n` endpoint ` |\n\n` STRING `\n\n|\n\nA HTTP endpoint of Cloud Functions. Applies only to remote functions.\n\nExample: ` \"https://us-east1-your-project.cloudfunctions.net/foo\" `\n\nFor more information, see [ Creating a Remote Function\n](/bigquery/docs/remote-functions#creating-remote-function) .\n\n` user_defined_context ` |\n\n` ARRAY<STRUCT<STRING,STRING>> `\n\n|\n\nA list of key-value pairs that will be sent with every HTTP request when the function is invoked. Applies only to remote functions.\n\nExample: ` [(\"key1\",\"value1\"),(\"key2\", \"value2\")] `\n\n` max_batching_rows ` |\n\n` INT64 `\n\n|\n\nThe maximum number of rows in each HTTP request. If not specified, BigQuery decides how many rows are included in a HTTP request. Applies only to remote functions.\n\n####  Required permissions\n\nThis statement requires the following [ IAM permissions\n](/bigquery/docs/access-control#bq-permissions) :\n\nPermission  |  Resource\n---|---\n` bigquery.routines.create ` |  The dataset where you create the function.\n\nIn addition, the ` OR REPLACE ` clause requires ` bigquery.routines.update `\npermission.\n\nTo create a remote function, additional [ IAM permissions\n](/bigquery/docs/access-control#bq-permissions) are needed:\n\nPermission  |  Resource\n---|---\n` bigquery.connections.delegate ` |  The connection which you use to create the remote function.\n\n####  Examples\n\n####  Create a SQL UDF\n\nThe following example creates a persistent SQL UDF named ` multiplyInputs ` in a dataset named ` mydataset ` .\n\n\nCREATE FUNCTION mydataset.multiplyInputs(x FLOAT64, y FLOAT64) RETURNS FLOAT64 AS (x * y);\n\n####  Create a JavaScript UDF\n\nThe following example creates a temporary JavaScript UDF named `\nmultiplyInputs ` and calls it from inside a ` SELECT ` statement.\n\n\nCREATE TEMP FUNCTION multiplyInputs(x FLOAT64, y FLOAT64) RETURNS FLOAT64 LANGUAGE js AS r\"\"\"\nreturn x*y;\n\"\"\";\n\nSELECT multiplyInputs(a, b) FROM (SELECT 3 as a, 2 as b);\n\n####  Create a remote function\n\nThe following example creates a persistent remote function named `\nremoteMultiplyInputs ` in a dataset named ` mydataset ` , assuming ` mydataset\n` is in ` US ` location and there is a connection ` myconnection ` in the same location and same project.\n\n\nCREATE FUNCTION mydataset.remoteMultiplyInputs(x FLOAT64, y FLOAT64) RETURNS FLOAT64 REMOTE WITH CONNECTION us.myconnection OPTIONS(endpoint=\"https://us-central1-myproject.cloudfunctions.net/multiply\");"
            },
            "DECLARE": {
                "name": "DECLARE",
                "summary": "Declares a variable of the specified type.",
                "description": "DECLARE variable_name[, ...] [variable_type] [DEFAULT expression];\n\n` variable_name ` must be a valid identifier, and ` variable_type ` is any GoogleSQL [ type ](/bigquery/docs/reference/standard-sql/data-types) .\n\n**Description**\n\nDeclares a variable of the specified type. If the ` DEFAULT ` clause is specified, the variable is initialized with the value of the expression; if no\n` DEFAULT ` clause is present, the variable is initialized with the value `\nNULL ` .\n\nIf ` [variable_type] ` is omitted then a ` DEFAULT ` clause must be specified.\nThe variable\u2019s type will be inferred by the type of the expression in the `\nDEFAULT ` clause.\n\nVariable declarations must appear before other procedural statements, or at the start of a ` BEGIN ` block. Variable names are case-insensitive.\n\nMultiple variable names can appear in a single ` DECLARE ` statement, but only one ` variable_type ` and ` expression ` .\n\nIt is an error to declare a variable with the same name as a variable declared earlier in the current block or in a containing block.\n\nIf the ` DEFAULT ` clause is present, the value of the expression must be coercible to the specified type. The expression may reference other variables declared previously within the same block or a containing block.\n\nGoogleSQL also supports [ system variables ](/bigquery/docs/reference/system-\nvariables) . You do not need to declare system variables, but you can set any of them that are not marked read-only. You can reference system variables in queries.\n\n**Examples**\n\nThe following example initializes the variable ` x ` as an ` INT64 ` with the value ` NULL ` .\n\n\nDECLARE x INT64;\n\nThe following example initializes the variable ` d ` as a ` DATE ` object with the value of the current date.\n\n\nDECLARE d DATE DEFAULT CURRENT_DATE();\n\nThe following example initializes the variables ` x ` , ` y ` , and ` z ` as `\nINT64 ` with the value 0.\n\n\nDECLARE x, y, z INT64 DEFAULT 0;\n\nThe following example declares a variable named ` item ` corresponding to an arbitrary item in the ` schema1.products ` table. The type of ` item ` is inferred from the table schema.\n\n\nDECLARE item DEFAULT (SELECT item FROM schema1.products LIMIT 1);"
            },
            "SET": {
                "name": "SET",
                "summary": "Sets a variable to have the value of the provided expression.",
                "description": "**Syntax**\n\n\nSET variable_name = expression;\n\n\nSET (variable_name[, ...]) = (expression[, ...]);\n\n**Description**\n\nSets a variable to have the value of the provided expression, or sets multiple variables at the same time based on the result of multiple expressions.\n\nThe ` SET ` statement may appear anywhere within a multi-statement query.\n\n**Examples**\n\nThe following example sets the variable ` x ` to have the value 5.\n\n\nSET x = 5;\n\nThe following example sets the variable ` a ` to have the value 4, ` b ` to have the value 'foo', and the variable ` c ` to have the value ` false ` .\n\n\nSET (a, b, c) = (1 + 3, 'foo', false);\n\nThe following example assigns the result of a query to multiple variables.\nFirst, it declares two variables, ` target_word ` and ` corpus_count ` ; next,\nit assigns the results of a [ ` SELECT AS STRUCT ` query\n](/bigquery/docs/reference/standard-sql/query-syntax#select_modifiers) to the two variables. The result of the query is a single row containing a ` STRUCT `\nwith two fields; the first element is assigned to the first variable, and the second element is assigned to the second variable.\n\n\nDECLARE target_word STRING DEFAULT 'methinks';\nDECLARE corpus_count, word_count INT64;\n\nSET (corpus_count, word_count) = ( SELECT AS STRUCT COUNT(DISTINCT corpus), SUM(word_count) FROM bigquery-public-data.samples.shakespeare WHERE LOWER(word) = target_word );\n\nSELECT FORMAT('Found %d occurrences of \"%s\" across %d Shakespeare works',\nword_count, target_word, corpus_count) AS result;\n\nThis statement list outputs the following string:\n\nFound 151 occurrences of \"methinks\" across 38 Shakespeare works"
            },
            "UNNEST": {
                "name": "UNNEST",
                "summary": "Takes an array and returns a table with one row for each element in the array.",
                "description": "unnest_operator:\n{\nUNNEST( array ) [ as_alias ]\n| array_path [ as_alias ]\n}\n[ WITH OFFSET [ as_alias ] ]\n\narray:\n{ array_expression | array_path }\n\nas_alias:\n[AS] alias\n\nThe ` UNNEST ` operator takes an array and returns a table with one row for each element in the array. The output of ` UNNEST ` is one  value table column. For these ` ARRAY ` element types, ` SELECT * ` against the value table column returns multiple columns:\n\n* ` STRUCT `\n\nInput values:\n\n* ` array_expression ` : An expression that produces an array.\n* ` array_path ` : The path to an ` ARRAY ` type.\n\n* In an implicit ` UNNEST ` operation, the path must start with a  range variable  name.\n* In an explicit ` UNNEST ` operation, the path can optionally start with a  range variable  name.\n\nThe ` UNNEST ` operation with any  correlated  ` array_path ` must be on the right side of a ` CROSS JOIN ` , ` LEFT JOIN ` , or ` INNER JOIN ` operation.\n\n* ` as_alias ` : If specified, defines the explicit name of the value table column containing the array element values. It can be used to refer to the column elsewhere in the query.\n\n* ` WITH OFFSET ` : ` UNNEST ` destroys the order of elements in the input array. Use this optional clause to return an additional column with the array element indexes, or _offsets_ . Offset counting starts at zero for each row produced by the ` UNNEST ` operation. This column has an optional alias; If the optional alias is not used, the default column name is ` offset ` .\n\nExample:\n\nSELECT * FROM UNNEST ([10,20,30]) as numbers WITH OFFSET;\n\n/*---------+--------*\n| numbers | offset |\n+---------+--------+\n| 10      | 0      |\n| 20      | 1      |\n| 30      | 2      |\n*---------+--------*/\n\nYou can also use ` UNNEST ` outside of the ` FROM ` clause with the [ ` IN `\noperator ](/bigquery/docs/reference/standard-sql/operators#in_operators) .\n\nFor several ways to use ` UNNEST ` , including construction, flattening, and filtering, see [ Work with arrays ](/bigquery/docs/arrays#working_with_arrays) .\n\nTo learn more about the ways you can use ` UNNEST ` explicitly and implicitly,\nsee  Explicit and implicit ` UNNEST ` .\n\n####  ` UNNEST ` and structs\n\nFor an input array of structs, ` UNNEST ` returns a row for each struct, with a separate column for each field in the struct. The alias for each column is the name of the corresponding struct field.\n\nExample:\n\n\nSELECT *\nFROM UNNEST( ARRAY<\nSTRUCT<\nx INT64,\ny STRING,\nz STRUCT<a INT64, b INT64>>>[\n(1, 'foo', (10, 11)),\n(3, 'bar', (20, 21))]);\n\n/*---+-----+----------*\n| x | y   | z        |\n+---+-----+----------+\n| 1 | foo | {10, 11} |\n| 3 | bar | {20, 21} |\n*---+-----+----------*/\n\nBecause the ` UNNEST ` operator returns a  value table  , you can alias `\nUNNEST ` to define a range variable that you can reference elsewhere in the query. If you reference the range variable in the ` SELECT ` list, the query returns a struct containing all of the fields of the original struct in the input table.\n\nExample:\n\n\nSELECT *, struct_value FROM UNNEST( ARRAY<\nSTRUCT<\nx INT64,\ny STRING>>[\n(1, 'foo'),\n(3, 'bar')]) AS struct_value;\n\n/*---+-----+--------------*\n| x | y   | struct_value |\n+---+-----+--------------+\n| 3 | bar | {3, bar}     |\n| 1 | foo | {1, foo}     |\n*---+-----+--------------*/\n\n####  Explicit and implicit ` UNNEST `\n\nArray unnesting can be either explicit or implicit. To learn more, see the following sections.\n\n**Explicit unnesting**\n\nThe ` UNNEST ` keyword is required in explicit unnesting. For example:\n\n\nWITH Coordinates AS (SELECT [1,2] AS position) SELECT results FROM Coordinates, UNNEST(Coordinates.position) AS results;\n\nIn explicit unnesting, ` array_expression ` must return an array value but doesn't need to resolve to an array.\n\n**Implicit unnesting**\n\nThe ` UNNEST ` keyword is not used in implicit unnesting.\n\nFor example:\n\n\nWITH Coordinates AS (SELECT [1,2] AS position) SELECT results FROM Coordinates, Coordinates.position AS results;\n\n**Tables and implicit unnesting**\n\nWhen you use ` array_path ` with implicit ` UNNEST ` , ` array_path ` must be prepended with the table. For example:\n\n\nWITH Coordinates AS (SELECT [1,2] AS position) SELECT results FROM Coordinates, Coordinates.position AS results;\n\n####  ` UNNEST ` and ` NULL ` values\n\n` UNNEST ` treats ` NULL ` values as follows:\n\n* ` NULL ` and empty arrays produce zero rows.\n* An array containing ` NULL ` values produces rows containing ` NULL ` values."
            },
            "PIVOT": {
                "name": "PIVOT",
                "summary": "Rotates rows into columns, using aggregation.",
                "description": "FROM from_item[, ...] pivot_operator\n\npivot_operator:\nPIVOT( aggregate_function_call [as_alias][, ...]\nFOR input_column IN ( pivot_column [as_alias][, ...] ) ) [AS alias]\n\nas_alias:\n[AS] alias\n\nThe ` PIVOT ` operator rotates rows into columns, using aggregation. ` PIVOT `\nis part of the ` FROM ` clause.\n\n* ` PIVOT ` can be used to modify any table expression.\n* Combining ` PIVOT ` with ` FOR SYSTEM_TIME AS OF ` is not allowed, although users may use ` PIVOT ` against a subquery input which itself uses ` FOR SYSTEM_TIME AS OF ` .\n* A ` WITH OFFSET ` clause immediately preceding the ` PIVOT ` operator is not allowed.\n\nConceptual example:\n\n\n-- Before PIVOT is used to rotate sales and quarter into Q1, Q2, Q3, Q4 columns:\n/*---------+-------+---------+------*\n| product | sales | quarter | year |\n+---------+-------+---------+------|\n| Kale    | 51    | Q1      | 2020 |\n| Kale    | 23    | Q2      | 2020 |\n| Kale    | 45    | Q3      | 2020 |\n| Kale    | 3     | Q4      | 2020 |\n| Kale    | 70    | Q1      | 2021 |\n| Kale    | 85    | Q2      | 2021 |\n| Apple   | 77    | Q1      | 2020 |\n| Apple   | 0     | Q2      | 2020 |\n| Apple   | 1     | Q1      | 2021 |\n*---------+-------+---------+------*/\n\n-- After PIVOT is used to rotate sales and quarter into Q1, Q2, Q3, Q4 columns:\n/*---------+------+----+------+------+------*\n| product | year | Q1 | Q2   | Q3   | Q4   |\n+---------+------+----+------+------+------+\n| Apple   | 2020 | 77 | 0    | NULL | NULL |\n| Apple   | 2021 | 1  | NULL | NULL | NULL |\n| Kale    | 2020 | 51 | 23   | 45   | 3    |\n| Kale    | 2021 | 70 | 85   | NULL | NULL |\n*---------+------+----+------+------+------*/\n\n**Definitions**\n\nTop-level definitions:\n\n* ` from_item ` : The table or subquery on which to perform a pivot operation. The ` from_item ` must  follow these rules  .\n* ` pivot_operator ` : The pivot operation to perform on a ` from_item ` .\n* ` alias ` : An alias to use for an item in the query.\n\n` pivot_operator ` definitions:\n\n* ` aggregate_function_call ` : An aggregate function call that aggregates all input rows such that ` input_column ` matches a particular value in ` pivot_column ` . Each aggregation corresponding to a different ` pivot_column ` value produces a different column in the output.  Follow these rules  when creating an aggregate function call.\n* ` input_column ` : Takes a column and retrieves the row values for the column,  following these rules  .\n* ` pivot_column ` : A pivot column to create for each aggregate function call. If an alias is not provided, a default alias is created. A pivot column value type must match the value type in ` input_column ` so that the values can be compared. It is possible to have a value in ` pivot_column ` that doesn't match a value in ` input_column ` . Must be a constant and  follow these rules  .\n\n**Rules**\n\nRules for a ` from_item ` passed to ` PIVOT ` :\n\n* The ` from_item ` may consist of any table or subquery result.\n* The ` from_item ` may not produce a value table.\n* The ` from_item ` may not be a subquery using ` SELECT AS STRUCT ` .\n\nRules for ` aggregate_function_call ` :\n\n* Must be an aggregate function. For example, ` SUM ` .\n* You may reference columns in a table passed to ` PIVOT ` , as well as correlated columns, but may not access columns defined by the ` PIVOT ` clause itself.\n* A table passed to ` PIVOT ` may be accessed through its alias if one is provided.\n* You can only use an aggregate function that takes one argument.\n* Except for ` COUNT ` , you can only use aggregate functions that ignore ` NULL ` inputs.\n* If you are using ` COUNT ` , you can use ` * ` as an argument.\n\nRules for ` input_column ` :\n\n* May access columns from the input table, as well as correlated columns, not columns defined by the ` PIVOT ` clause, itself.\n* Evaluated against each row in the input table; aggregate and window function calls are prohibited.\n* Non-determinism is okay.\n* The type must be groupable.\n* The input table may be accessed through its alias if one is provided.\n\nRules for ` pivot_column ` :\n\n* A ` pivot_column ` must be a constant.\n* Named constants, such as variables, are not supported.\n* Query parameters are not supported.\n* If a name is desired for a named constant or query parameter, specify it explicitly with an alias.\n* Corner cases exist where a distinct ` pivot_column ` s can end up with the same default column names. For example, an input column might contain both a ` NULL ` value and the string literal ` \"NULL\" ` . When this happens, multiple pivot columns are created with the same name. To avoid this situation, use aliases for pivot column names.\n* If a ` pivot_column ` doesn't specify an alias, a column name is constructed as follows:\n\nFrom  |  To  |  Example\n---|---|---\nNULL  |  NULL  |  Input: NULL Output: \"NULL\"\n\n` INT64 `\n` NUMERIC `\n` BIGNUMERIC `\n|  The number in string format with the following rules:\n\n* Positive numbers are preceded with ` _ ` .\n* Negative numbers are preceded with ` minus_ ` .\n* A decimal point is replaced with ` _point_ ` .\n\n|  Input: 1 Output: _1\n\n* * *\n\nInput: -1 Output: minus_1\n\n* * *\n\nInput: 1.0 Output: _1_point_0\n\nBOOL  |  ` TRUE ` or ` FALSE ` .  |  Input: TRUE Output: TRUE\n\n* * *\n\nInput: FALSE Output: FALSE\n\nSTRING  |  The string value.  |  Input: \"PlayerName\"\nOutput: PlayerName\n\nDATE  |  The date in ` _YYYY_MM_DD ` format.  |  Input: DATE '2013-11-25'\nOutput: _2013_11_25\n\nENUM  |  The name of the enumeration constant.  |  Input: COLOR.RED Output: RED\n\nSTRUCT  |  A string formed by computing the ` pivot_column ` name for each field and joining the results together with an underscore. The following rules apply:\n\n* If the field is named: ` <field_name>_<pivot_column_name_for_field_name> ` .\n* If the field is unnamed: ` <pivot_column_name_for_field_name> ` .\n\n` <pivot_column_name_for_field_name> ` is determined by applying the rules in this table, recursively. If no rule is available for any ` STRUCT ` field, the entire pivot column is unnamed.\n\nDue to implicit type coercion from the ` IN ` list values to the type of `\n<value-expression> ` , field names must be present in ` input_column ` to have an effect on the names of the pivot columns.\n\n|  Input: STRUCT(\"one\", \"two\") Output: one_two\n\n* * *\n\nInput: STRUCT(\"one\" AS a, \"two\" AS b) Output: one_a_two_b\n\nAll other data types  |  Not supported. You must provide an alias.  |\n\n**Examples**\n\nThe following examples reference a table called ` Produce ` that looks like this:\n\n\nWITH Produce AS ( SELECT 'Kale' as product, 51 as sales, 'Q1' as quarter, 2020 as year UNION ALL SELECT 'Kale', 23, 'Q2', 2020 UNION ALL SELECT 'Kale', 45, 'Q3', 2020 UNION ALL SELECT 'Kale', 3, 'Q4', 2020 UNION ALL SELECT 'Kale', 70, 'Q1', 2021 UNION ALL SELECT 'Kale', 85, 'Q2', 2021 UNION ALL SELECT 'Apple', 77, 'Q1', 2020 UNION ALL SELECT 'Apple', 0, 'Q2', 2020 UNION ALL SELECT 'Apple', 1, 'Q1', 2021) SELECT * FROM Produce\n\n/*---------+-------+---------+------*\n| product | sales | quarter | year |\n+---------+-------+---------+------|\n| Kale    | 51    | Q1      | 2020 |\n| Kale    | 23    | Q2      | 2020 |\n| Kale    | 45    | Q3      | 2020 |\n| Kale    | 3     | Q4      | 2020 |\n| Kale    | 70    | Q1      | 2021 |\n| Kale    | 85    | Q2      | 2021 |\n| Apple   | 77    | Q1      | 2020 |\n| Apple   | 0     | Q2      | 2020 |\n| Apple   | 1     | Q1      | 2021 |\n*---------+-------+---------+------*/\n\nWith the ` PIVOT ` operator, the rows in the ` quarter ` column are rotated into these new columns: ` Q1 ` , ` Q2 ` , ` Q3 ` , ` Q4 ` . The aggregate function ` SUM ` is implicitly grouped by all unaggregated columns other than the ` pivot_column ` : ` product ` and ` year ` .\n\n\nSELECT * FROM Produce PIVOT(SUM(sales) FOR quarter IN ('Q1', 'Q2', 'Q3', 'Q4'))\n\n/*---------+------+----+------+------+------*\n| product | year | Q1 | Q2   | Q3   | Q4   |\n+---------+------+----+------+------+------+\n| Apple   | 2020 | 77 | 0    | NULL | NULL |\n| Apple   | 2021 | 1  | NULL | NULL | NULL |\n| Kale    | 2020 | 51 | 23   | 45   | 3    |\n| Kale    | 2021 | 70 | 85   | NULL | NULL |\n*---------+------+----+------+------+------*/\n\nIf you don't include ` year ` , then ` SUM ` is grouped only by ` product ` .\n\n\nSELECT * FROM (SELECT product, sales, quarter FROM Produce) PIVOT(SUM(sales) FOR quarter IN ('Q1', 'Q2', 'Q3', 'Q4'))\n\n/*---------+-----+-----+------+------*\n| product | Q1  | Q2  | Q3   | Q4   |\n+---------+-----+-----+------+------+\n| Apple   | 78  | 0   | NULL | NULL |\n| Kale    | 121 | 108 | 45   | 3    |\n*---------+-----+-----+------+------*/\n\nYou can select a subset of values in the ` pivot_column ` :\n\n\nSELECT * FROM (SELECT product, sales, quarter FROM Produce) PIVOT(SUM(sales) FOR quarter IN ('Q1', 'Q2', 'Q3'))\n\n/*---------+-----+-----+------*\n| product | Q1  | Q2  | Q3   |\n+---------+-----+-----+------+\n| Apple   | 78  | 0   | NULL |\n| Kale    | 121 | 108 | 45   |\n*---------+-----+-----+------*/\n\n\nSELECT * FROM (SELECT sales, quarter FROM Produce) PIVOT(SUM(sales) FOR quarter IN ('Q1', 'Q2', 'Q3'))\n\n/*-----+-----+----*\n| Q1  | Q2  | Q3 |\n+-----+-----+----+\n| 199 | 108 | 45 |\n*-----+-----+----*/\n\nYou can include multiple aggregation functions in the ` PIVOT ` . In this case, you must specify an alias for each aggregation. These aliases are used to construct the column names in the resulting table.\n\n\nSELECT * FROM (SELECT product, sales, quarter FROM Produce) PIVOT(SUM(sales) total_sales, COUNT(*) num_records FOR quarter IN ('Q1', 'Q2'))\n\n/*--------+----------------+----------------+----------------+----------------*\n|product | total_sales_Q1 | num_records_Q1 | total_sales_Q2 | num_records_Q2 |\n+--------+----------------+----------------+----------------+----------------+\n| Kale   | 121            | 2              | 108            | 2              |\n| Apple  | 78             | 2              | 0              | 1              |\n*--------+----------------+----------------+----------------+----------------*/"
            },
            "UNPIVOT": {
                "name": "UNPIVOT",
                "summary": "Rotates columns into rows.",
                "description": "FROM from_item[, ...] unpivot_operator\n\nunpivot_operator:\nUNPIVOT [ { INCLUDE NULLS | EXCLUDE NULLS } ] (\n{ single_column_unpivot | multi_column_unpivot }\n) [unpivot_alias]\n\nsingle_column_unpivot:\nvalues_column FOR name_column IN (columns_to_unpivot)\n\nmulti_column_unpivot:\nvalues_column_set FOR name_column IN (column_sets_to_unpivot)\n\nvalues_column_set:\n(values_column[, ...])\n\ncolumns_to_unpivot:\nunpivot_column [row_value_alias][, ...]\n\ncolumn_sets_to_unpivot:\n(unpivot_column [row_value_alias][, ...])\n\nunpivot_alias and row_value_alias:\n[AS] alias\n\nThe ` UNPIVOT ` operator rotates columns into rows. ` UNPIVOT ` is part of the\n` FROM ` clause.\n\n* ` UNPIVOT ` can be used to modify any table expression.\n* Combining ` UNPIVOT ` with ` FOR SYSTEM_TIME AS OF ` is not allowed, although users may use ` UNPIVOT ` against a subquery input which itself uses ` FOR SYSTEM_TIME AS OF ` .\n* A ` WITH OFFSET ` clause immediately preceding the ` UNPIVOT ` operator is not allowed.\n* ` PIVOT ` aggregations cannot be reversed with ` UNPIVOT ` .\n\nConceptual example:\n\n\n-- Before UNPIVOT is used to rotate Q1, Q2, Q3, Q4 into sales and quarter columns:\n/*---------+----+----+----+----*\n| product | Q1 | Q2 | Q3 | Q4 |\n+---------+----+----+----+----+\n| Kale    | 51 | 23 | 45 | 3  |\n| Apple   | 77 | 0  | 25 | 2  |\n*---------+----+----+----+----*/\n\n-- After UNPIVOT is used to rotate Q1, Q2, Q3, Q4 into sales and quarter columns:\n/*---------+-------+---------*\n| product | sales | quarter |\n+---------+-------+---------+\n| Kale    | 51    | Q1      |\n| Kale    | 23    | Q2      |\n| Kale    | 45    | Q3      |\n| Kale    | 3     | Q4      |\n| Apple   | 77    | Q1      |\n| Apple   | 0     | Q2      |\n| Apple   | 25    | Q3      |\n| Apple   | 2     | Q4      |\n*---------+-------+---------*/\n\n**Definitions**\n\nTop-level definitions:\n\n* ` from_item ` : The table or subquery on which to perform a pivot operation. The ` from_item ` must  follow these rules  .\n* ` unpivot_operator ` : The pivot operation to perform on a ` from_item ` .\n\n` unpivot_operator ` definitions:\n\n* ` INCLUDE NULLS ` : Add rows with ` NULL ` values to the result.\n* ` EXCLUDE NULLS ` : don't add rows with ` NULL ` values to the result. By default, ` UNPIVOT ` excludes rows with ` NULL ` values.\n* ` single_column_unpivot ` : Rotates columns into one ` values_column ` and one ` name_column ` .\n* ` multi_column_unpivot ` : Rotates columns into multiple ` values_column ` s and one ` name_column ` .\n* ` unpivot_alias ` : An alias for the results of the ` UNPIVOT ` operation. This alias can be referenced elsewhere in the query.\n\n` single_column_unpivot ` definitions:\n\n* ` values_column ` : A column to contain the row values from ` columns_to_unpivot ` .  Follow these rules  when creating a values column.\n* ` name_column ` : A column to contain the column names from ` columns_to_unpivot ` .  Follow these rules  when creating a name column.\n* ` columns_to_unpivot ` : The columns from the ` from_item ` to populate ` values_column ` and ` name_column ` .  Follow these rules  when creating an unpivot column.\n* ` row_value_alias ` : An optional alias for a column that is displayed for the column in ` name_column ` . If not specified, the string value of the column name is used.  Follow these rules  when creating a row value alias.\n\n` multi_column_unpivot ` definitions:\n\n* ` values_column_set ` : A set of columns to contain the row values from ` columns_to_unpivot ` .  Follow these rules  when creating a values column.\n* ` name_column ` : A set of columns to contain the column names from ` columns_to_unpivot ` .  Follow these rules  when creating a name column.\n* ` column_sets_to_unpivot ` : The columns from the ` from_item ` to unpivot.  Follow these rules  when creating an unpivot column.\n* ` row_value_alias ` : An optional alias for a column set that is displayed for the column set in ` name_column ` . If not specified, a string value for the column set is used and each column in the string is separated with an underscore ( ` _ ` ). For example, ` (col1, col2) ` outputs ` col1_col2 ` .  Follow these rules  when creating a row value alias.\n\n**Rules**\n\nRules for a ` from_item ` passed to ` UNPIVOT ` :\n\n* The ` from_item ` may consist of any table or subquery result.\n* The ` from_item ` may not produce a value table.\n* Duplicate columns in a ` from_item ` cannot be referenced in the ` UNPIVOT ` clause.\n\nRules for ` unpivot_operator ` :\n\n* Expressions are not permitted.\n* Qualified names are not permitted. For example, ` mytable.mycolumn ` is not allowed.\n* In the case where the ` UNPIVOT ` result has duplicate column names:\n* ` SELECT * ` is allowed.\n* ` SELECT values_column ` causes ambiguity.\n\nRules for ` values_column ` :\n\n* It cannot be a name used for a ` name_column ` or an ` unpivot_column ` .\n* It can be the same name as a column from the ` from_item ` .\n\nRules for ` name_column ` :\n\n* It cannot be a name used for a ` values_column ` or an ` unpivot_column ` .\n* It can be the same name as a column from the ` from_item ` .\n\nRules for ` unpivot_column ` :\n\n* Must be a column name from the ` from_item ` .\n* It cannot reference duplicate ` from_item ` column names.\n* All columns in a column set must have equivalent data types.\n* Data types cannot be coerced to a common supertype.\n* If the data types are exact matches (for example, a struct with different field names), the data type of the first input is the data type of the output.\n* You cannot have the same name in the same column set. For example, ` (emp1, emp1) ` results in an error.\n* You can have a the same name in different column sets. For example, ` (emp1, emp2), (emp1, emp3) ` is valid.\n\nRules for ` row_value_alias ` :\n\n* This can be a string or an ` INT64 ` literal.\n* The data type for all ` row_value_alias ` clauses must be the same.\n* If the value is an ` INT64 ` , the ` row_value_alias ` for each ` unpivot_column ` must be specified.\n\n**Examples**\n\nThe following examples reference a table called ` Produce ` that looks like this:\n\n\nWITH Produce AS ( SELECT 'Kale' as product, 51 as Q1, 23 as Q2, 45 as Q3, 3 as Q4 UNION ALL SELECT 'Apple', 77, 0, 25, 2) SELECT * FROM Produce\n\n/*---------+----+----+----+----*\n| product | Q1 | Q2 | Q3 | Q4 |\n+---------+----+----+----+----+\n| Kale    | 51 | 23 | 45 | 3  |\n| Apple   | 77 | 0  | 25 | 2  |\n*---------+----+----+----+----*/\n\nWith the ` UNPIVOT ` operator, the columns ` Q1 ` , ` Q2 ` , ` Q3 ` , and ` Q4\n` are rotated. The values of these columns now populate a new column called `\nSales ` and the names of these columns now populate a new column called `\nQuarter ` . This is a single-column unpivot operation.\n\n\nSELECT * FROM Produce UNPIVOT(sales FOR quarter IN (Q1, Q2, Q3, Q4))\n\n/*---------+-------+---------*\n| product | sales | quarter |\n+---------+-------+---------+\n| Kale    | 51    | Q1      |\n| Kale    | 23    | Q2      |\n| Kale    | 45    | Q3      |\n| Kale    | 3     | Q4      |\n| Apple   | 77    | Q1      |\n| Apple   | 0     | Q2      |\n| Apple   | 25    | Q3      |\n| Apple   | 2     | Q4      |\n*---------+-------+---------*/\n\nIn this example, we ` UNPIVOT ` four quarters into two semesters. This is a multi-column unpivot operation.\n\n\nSELECT * FROM Produce UNPIVOT( (first_half_sales, second_half_sales) FOR semesters IN ((Q1, Q2) AS 'semester_1', (Q3, Q4) AS 'semester_2'))\n\n/*---------+------------------+-------------------+------------*\n| product | first_half_sales | second_half_sales | semesters  |\n+---------+------------------+-------------------+------------+\n| Kale    | 51               | 23                | semester_1 |\n| Kale    | 45               | 3                 | semester_2 |\n| Apple   | 77               | 0                 | semester_1 |\n| Apple   | 25               | 2                 | semester_2 |\n*---------+------------------+-------------------+------------*/"
            },
            "TABLESAMPLE": {
                "name": "TABLESAMPLE",
                "summary": "Select a random sample of a dataset.",
                "description": "TABLESAMPLE SYSTEM ( percent PERCENT )\n\n**Description**\n\nYou can use the ` TABLESAMPLE ` operator to select a random sample of a dataset. This operator is useful when you're working with tables that have large amounts of data and you don't need precise answers.\n\nSampling returns a variety of records while avoiding the costs associated with scanning and processing an entire table. Each execution of the query might return different results because each execution processes an independently computed sample. GoogleSQL doesn't cache the results of queries that include a\n` TABLESAMPLE ` clause.\n\nReplace ` percent ` with the percentage of the dataset that you want to include in the results. The value must be between ` 0 ` and ` 100 ` . The value can be a literal value or a query parameter. It cannot be a variable.\n\nFor more information, see [ Table sampling ](/bigquery/docs/table-sampling) .\n\n**Example**\n\nThe following query selects approximately 10% of a table's data:\n\n\nSELECT * FROM dataset.my_table TABLESAMPLE SYSTEM (10 PERCENT)"
            },
            "ARRAY_SUBSCRIPT": {
                "name": "ARRAY_SUBSCRIPT",
                "summary": "Gets a value from an array at a specific position.",
                "description": "array_expression[array_subscript_specifier]\n\narray_subscript_specifier:\n{ index | position_keyword(index) }\n\nposition_keyword:\n{ OFFSET | SAFE_OFFSET | ORDINAL | SAFE_ORDINAL }\n\n**Note:** The brackets ( ` [] ` ) around ` array_subscript_specifier ` are part of the syntax; they do not represent an optional part.\n\n**Description**\n\nGets a value from an array at a specific position.\n\nInput values:\n\n* ` array_expression ` : The input array.\n* ` position_keyword(index) ` : Determines where the index for the array should start and how out-of-range indexes are handled. The index is an integer that represents a specific position in the array.\n* ` OFFSET(index) ` : The index starts at zero. Produces an error if the index is out of range. To produce ` NULL ` instead of an error, use ` SAFE_OFFSET(index) ` . This position keyword produces the same result as ` index ` by itself.\n* ` SAFE_OFFSET(index) ` : The index starts at zero. Returns ` NULL ` if the index is out of range.\n* ` ORDINAL(index) ` : The index starts at one. Produces an error if the index is out of range. To produce ` NULL ` instead of an error, use ` SAFE_ORDINAL(index) ` .\n* ` SAFE_ORDINAL(index) ` : The index starts at one. Returns ` NULL ` if the index is out of range.\n* ` index ` : An integer that represents a specific position in the array. If used by itself without a position keyword, the index starts at zero and produces an error if the index is out of range. To produce ` NULL ` instead of an error, use the ` SAFE_OFFSET(index) ` or ` SAFE_ORDINAL(index) ` position keyword.\n\n**Return type**\n\n` T ` where ` array_expression ` is ` ARRAY<T> ` .\n\n**Examples**\n\nIn following query, the array subscript operator is used to return values at specific position in ` item_array ` . This query also shows what happens when you reference an index ( ` 6 ` ) in an array that is out of range. If the `\nSAFE ` prefix is included, ` NULL ` is returned, otherwise an error is produced.\n\n\nWITH Items AS (SELECT [\"coffee\", \"tea\", \"milk\"] AS item_array) SELECT item_array,\nitem_array[0] AS item_index,\nitem_array[OFFSET(0)] AS item_offset,\nitem_array[ORDINAL(1)] AS item_ordinal,\nitem_array[SAFE_OFFSET(6)] AS item_safe_offset FROM Items\n\n/*---------------------+------------+-------------+--------------+------------------*\n| item_array          | item_index | item_offset | item_ordinal | item_safe_offset |\n+---------------------+------------+-------------+--------------+------------------+\n| [coffee, tea, milk] | coffee     | coffee      | coffee       | NULL             |\n*----------------------------------+-------------+--------------+------------------*/\n\nWhen you reference an index that is out of range in an array, and a positional keyword that begins with ` SAFE ` is not included, an error is produced. For example:\n\n\nWITH Items AS (SELECT [\"coffee\", \"tea\", \"milk\"] AS item_array) SELECT item_array[6] AS item_offset FROM Items\n\n-- Error. Array index 6 is out of bounds.\n\n\nWITH Items AS (SELECT [\"coffee\", \"tea\", \"milk\"] AS item_array) SELECT item_array[OFFSET(6)] AS item_offset FROM Items\n\n-- Error. Array index 6 is out of bounds."
            },
            "STRUCT_SUBSCRIPT": {
                "name": "STRUCT_SUBSCRIPT",
                "summary": "Gets the value of a field at a selected position in a struct.",
                "description": "struct_expression[struct_subscript_specifier]\n\nstruct_subscript_specifier:\n{ index | position_keyword(index) }\n\nposition_keyword:\n{ OFFSET | ORDINAL }\n\n**Note:** The brackets ( ` [] ` ) around ` struct_subscript_specifier ` are part of the syntax; they do not represent an optional part.\n\n**Description**\n\nGets the value of a field at a selected position in a struct.\n\n**Input types**\n\n* ` struct_expression ` : The input struct.\n* ` position_keyword(index) ` : Determines where the index for the struct should start and how out-of-range indexes are handled. The index is an integer literal or constant that represents a specific position in the struct.\n* ` OFFSET(index) ` : The index starts at zero. Produces an error if the index is out of range. Produces the same result as ` index ` by itself.\n* ` ORDINAL(index) ` : The index starts at one. Produces an error if the index is out of range.\n* ` index ` : An integer literal or constant that represents a specific position in the struct. If used by itself without a position keyword, the index starts at zero and produces an error if the index is out of range.\n\n**Note:** The struct subscript operator doesn't support ` SAFE ` positional keywords at this time.\n\n**Examples**\n\nIn following query, the struct subscript operator is used to return values at specific locations in ` item_struct ` using position keywords. This query also shows what happens when you reference an index ( ` 6 ` ) in an struct that is out of range.\n\n\nWITH Items AS (SELECT STRUCT<INT64, STRING, BOOL>(23, \"tea\", FALSE) AS item_struct) SELECT item_struct[0] AS field_index,\nitem_struct[OFFSET(0)] AS field_offset,\nitem_struct[ORDINAL(1)] AS field_ordinal FROM Items\n\n/*-------------+--------------+---------------*\n| field_index | field_offset | field_ordinal |\n+-------------+--------------+---------------+\n| 23          | 23           | 23            |\n*-------------+--------------+---------------*/\n\nWhen you reference an index that is out of range in a struct, an error is produced. For example:\n\n\nWITH Items AS (SELECT STRUCT<INT64, STRING, BOOL>(23, \"tea\", FALSE) AS item_struct) SELECT item_struct[6] AS field_offset FROM Items\n\n-- Error. Field ordinal 6 is out of bounds in STRUCT\n\n\nWITH Items AS (SELECT STRUCT<INT64, STRING, BOOL>(23, \"tea\", FALSE) AS item_struct) SELECT item_struct[OFFSET(6)] AS field_offset FROM Items\n\n-- Error. Field ordinal 6 is out of bounds in STRUCT"
            }
        }
    }
]